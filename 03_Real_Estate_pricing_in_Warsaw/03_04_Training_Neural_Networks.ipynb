{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import libraries</a></span></li><li><span><a href=\"#Custom-functions\" data-toc-modified-id=\"Custom-functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Custom functions</a></span></li><li><span><a href=\"#Importing-data\" data-toc-modified-id=\"Importing-data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Importing data</a></span></li><li><span><a href=\"#Spliting-into-train-and-test-sets\" data-toc-modified-id=\"Spliting-into-train-and-test-sets-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Spliting into train and test sets</a></span></li><li><span><a href=\"#Data-transformation-and-standarization\" data-toc-modified-id=\"Data-transformation-and-standarization-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data transformation and standarization</a></span></li></ul></li><li><span><a href=\"#Training-initial-DNN-model\" data-toc-modified-id=\"Training-initial-DNN-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training initial DNN model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base-configuration\" data-toc-modified-id=\"Base-configuration-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Base configuration</a></span></li><li><span><a href=\"#Adding-Drop-out\" data-toc-modified-id=\"Adding-Drop-out-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Adding Drop-out</a></span></li><li><span><a href=\"#Adding-batch-normalization\" data-toc-modified-id=\"Adding-batch-normalization-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Adding batch normalization</a></span></li><li><span><a href=\"#Adding-Leaky-Relu\" data-toc-modified-id=\"Adding-Leaky-Relu-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Adding Leaky Relu</a></span></li></ul></li><li><span><a href=\"#Expanding-network-architecture-and-learning-rate-tuning\" data-toc-modified-id=\"Expanding-network-architecture-and-learning-rate-tuning-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Expanding network architecture and learning rate tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Adding-hidden-layer-with-1024-neurons\" data-toc-modified-id=\"Adding-hidden-layer-with-1024-neurons-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Adding hidden layer with 1024 neurons</a></span></li><li><span><a href=\"#Adjusting-learning-rate-decay\" data-toc-modified-id=\"Adjusting-learning-rate-decay-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Adjusting learning rate decay</a></span></li><li><span><a href=\"#Callbacks-stop\" data-toc-modified-id=\"Callbacks-stop-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Callbacks stop</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cufflinks wrapper on plotly\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "\n",
    "# Set global theme\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_summary(model, X_test, y_test ):\n",
    "    \n",
    "    y_hat=model.predict(X_test)\n",
    "    \n",
    "    df_summary=pd.DataFrame(y_hat, columns=[\"y_hat\"])\n",
    "    df_summary[\"y_true\"]=y_test\n",
    "    df_summary[\"abs_error\"]=np.abs(df_summary.y_true-df_summary.y_hat)\n",
    "    df_summary[\"error\"]=df_summary.y_hat-df_summary.y_true\n",
    "    df_summary[\"relative_error\"]= df_summary[\"error\"]/df_summary.y_true\n",
    "    df_summary[\"relative_abs_error\"]= df_summary[\"abs_error\"]/df_summary.y_true\n",
    "    \n",
    "    share_within_5pct=(df_summary.query(\"relative_abs_error<0.05\").shape[0]/df_summary.shape[0])*100\n",
    "    \n",
    "    print(\"{:.2f}% : Share of forecasts within 5% absolute error\\n\".format(share_within_5pct))\n",
    "    print(\"{:.0f}   : Mean absolute error \\n\".format(df_summary.abs_error.mean()))\n",
    "    print(\"{:.2f}% : Mean absolute percentage error\\n\".format(df_summary.relative_abs_error.mean()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_plot_loss(history, starting_epoch,previous_val_loss):\n",
    "\n",
    "        trace0=go.Scatter(\n",
    "                y=history.history['loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"blue\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Training Loss\"\n",
    "            )\n",
    "\n",
    "\n",
    "        trace1=go.Scatter(\n",
    "                y=history.history['val_loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"red\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Validation Loss\"\n",
    "            )\n",
    "        \n",
    "        trace2=go.Scatter(\n",
    "                y=list(np.ones([len(history.epoch[starting_epoch:])])*np.asarray(previous_val_loss).min()),\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"grey\",\n",
    "                size=5,\n",
    "\n",
    "                ),\n",
    "                name=\"Lowest error from previous models\"\n",
    "            )\n",
    "\n",
    "        data=[trace0, trace1,trace2]\n",
    "        figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning curve\",\n",
    "                yaxis=dict(title=\"Loss\",range=(900,1500)),\n",
    "                xaxis=dict(title=\"Epoch\",range=(starting_epoch,history.epoch[-1])),\n",
    "                legend=dict(\n",
    "                    x=0.57,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None,\n",
    "\n",
    "\n",
    "\n",
    "            )))\n",
    "        iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (24935, 46)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"https://raw.githubusercontent.com/Jan-Majewski/Project_Portfolio/master/03_Real_Estate_pricing_in_Warsaw/top_features_data.csv\")\n",
    "print(\"Data dimensions: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting X and y from data, removing features not used in regression\n",
    "y=df.unit_price\n",
    "X=df.drop(columns=[\"unit_price\",\"lat_mod\",\"lon_mod\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['build_year', 'building_floors_num', 'rooms_num',\n",
       "       'Equipment_types_dishwasher', 'Equipment_types_fridge',\n",
       "       'Equipment_types_furniture', 'Equipment_types_tv',\n",
       "       'Equipment_types_washing_machine', 'Extras_types_air_conditioning',\n",
       "       'Extras_types_balcony', 'Extras_types_garden', 'Extras_types_lift',\n",
       "       'floor_num', 'east_bank', 'distance_driving', 'distance_transit',\n",
       "       'time_driving', 'time_transit', 'restaurant_price_level',\n",
       "       'restaurant_mean_rating', 'restaurant_mean_popularity',\n",
       "       'restaurant_count', 'restaurant_ratings_count', 'district_Bemowo',\n",
       "       'district_Bialoleka', 'district_Downtown', 'district_Subburbs',\n",
       "       'district_Targowek', 'district_Wawer', 'district_Wola',\n",
       "       'district_Zoliborz', 'market_primary', 'Building_material_brick',\n",
       "       'Building_ownership_full_ownership', 'Building_type_apartment',\n",
       "       'Building_type_block', 'Building_type_tenement',\n",
       "       'Construction_status_ready_to_use', 'Construction_status_to_completion',\n",
       "       'Heating_urban', 'Windows_type_aluminium', 'Windows_type_plastic',\n",
       "       'Windows_type_wooden', 'unit_price', 'lat_mod', 'lon_mod'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation and standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping y\n",
    "y_train=np.asarray(y_train).reshape(-1,1)\n",
    "y_test=np.asarray(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarizing X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training initial DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Initial_model\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Initial_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 2s 123us/sample - loss: 11118.3712 - val_loss: 10586.6312\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 7066.7532 - val_loss: 3763.7942\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 2464.6213 - val_loss: 1684.2397\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1628.4329 - val_loss: 1496.9290\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1463.1656 - val_loss: 1407.6186\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1387.9316 - val_loss: 1362.7425\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1347.6677 - val_loss: 1336.8397\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1319.9563 - val_loss: 1317.9443\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1298.9936 - val_loss: 1307.2187\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1286.4863 - val_loss: 1288.8486\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1266.7749 - val_loss: 1278.5656\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1257.1073 - val_loss: 1272.4424\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1245.9567 - val_loss: 1266.4797\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1238.1848 - val_loss: 1258.4730\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1230.3967 - val_loss: 1257.1221\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1222.6299 - val_loss: 1250.6509\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1215.9925 - val_loss: 1256.2491\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1211.2178 - val_loss: 1244.7884\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1205.1730 - val_loss: 1249.8148\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1202.4692 - val_loss: 1240.1362\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1197.0589 - val_loss: 1249.9176\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1199.0472 - val_loss: 1237.4305\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1193.2368 - val_loss: 1230.0728\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1185.2611 - val_loss: 1228.3953\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1180.1749 - val_loss: 1226.3071\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1176.6279 - val_loss: 1231.7320\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1174.2598 - val_loss: 1232.7204\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1171.1418 - val_loss: 1225.2275\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1165.8637 - val_loss: 1221.7506\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1161.4940 - val_loss: 1218.5342\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1160.2686 - val_loss: 1216.6966\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1157.7748 - val_loss: 1224.3637\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1157.5267 - val_loss: 1215.3038\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1151.7757 - val_loss: 1212.6736\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1150.0639 - val_loss: 1216.1089\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1149.5172 - val_loss: 1213.8360\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1144.9425 - val_loss: 1247.4697\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1160.4416 - val_loss: 1214.9550\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1143.0170 - val_loss: 1218.7990\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1137.4226 - val_loss: 1210.1933\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1134.6074 - val_loss: 1208.3317\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1131.5178 - val_loss: 1205.3584\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1130.5451 - val_loss: 1209.4332\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1131.3951 - val_loss: 1206.9375\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1129.0698 - val_loss: 1209.5329\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1124.8683 - val_loss: 1202.1867\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1120.1970 - val_loss: 1203.4296\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1116.5470 - val_loss: 1207.0689\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1114.6619 - val_loss: 1199.0832\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1112.3834 - val_loss: 1196.2431\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1111.15 - 0s 9us/sample - loss: 1111.4966 - val_loss: 1204.4175\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1114.1352 - val_loss: 1198.2407\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1103.8736 - val_loss: 1208.2518\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1108.4508 - val_loss: 1198.3145\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1099.9906 - val_loss: 1195.8515\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1096.3562 - val_loss: 1189.9547\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1093.8826 - val_loss: 1199.2403\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1091.7666 - val_loss: 1199.8163\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1089.4592 - val_loss: 1192.6154\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1086.7882 - val_loss: 1189.5412\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1082.8976 - val_loss: 1187.4291\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1088.7853 - val_loss: 1209.5408\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1094.9329 - val_loss: 1205.2325\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1077.9766 - val_loss: 1188.3222\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1067.9941 - val_loss: 1185.1855\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1071.8931 - val_loss: 1186.4818\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1062.1808 - val_loss: 1182.6778\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1063.3670 - val_loss: 1183.8029\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1058.3843 - val_loss: 1178.7426\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1058.3901 - val_loss: 1180.2725\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1057.7540 - val_loss: 1192.5393\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1044.2532 - val_loss: 1175.5885\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1039.1478 - val_loss: 1179.1760\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1038.9988 - val_loss: 1184.7684\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1045.3983 - val_loss: 1178.2782\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1030.0149 - val_loss: 1169.4896\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1024.2890 - val_loss: 1168.7235\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1019.1747 - val_loss: 1175.9734\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1020.1266 - val_loss: 1181.1238\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1020.9978 - val_loss: 1174.5035\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1023.7750 - val_loss: 1189.0411\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1035.7889 - val_loss: 1198.0716\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1015.4579 - val_loss: 1162.6249\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1004.4827 - val_loss: 1180.3889\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1002.9421 - val_loss: 1186.5759\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 995.6185 - val_loss: 1169.1167\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 995.5452 - val_loss: 1181.7682\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 991.4018 - val_loss: 1164.9680\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 986.5027 - val_loss: 1168.4908\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 976.1636 - val_loss: 1165.4763\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 976.1165 - val_loss: 1164.1544\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 972.7191 - val_loss: 1171.5086\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 972.7369 - val_loss: 1161.5413\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 975.2895 - val_loss: 1181.8683\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 976.6082 - val_loss: 1171.5005\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 957.7147 - val_loss: 1171.8049\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 961.9167 - val_loss: 1183.5040\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 963.0659 - val_loss: 1194.4317\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 951.0739 - val_loss: 1171.2314\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 946.9458 - val_loss: 1166.9764\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 940.6916 - val_loss: 1172.5611\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 937.0681 - val_loss: 1167.1437\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 932.4212 - val_loss: 1163.7142\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 925.2418 - val_loss: 1168.4919\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 923.1738 - val_loss: 1154.6400\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 922.5143 - val_loss: 1172.4903\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 919.4620 - val_loss: 1173.3097\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 914.4976 - val_loss: 1169.5990\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 910.4559 - val_loss: 1171.4554\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 902.7651 - val_loss: 1166.4193\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 902.5165 - val_loss: 1158.8106\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 896.5742 - val_loss: 1170.0957\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 907.0282 - val_loss: 1177.6343\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 895.8937 - val_loss: 1178.3552\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 912.7180 - val_loss: 1182.6652\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 903.3951 - val_loss: 1165.0796\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 881.9373 - val_loss: 1174.0791\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 882.7621 - val_loss: 1164.9211\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 879.5807 - val_loss: 1167.3307\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 869.7369 - val_loss: 1171.6832\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 875.6047 - val_loss: 1165.9992\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 887.7560 - val_loss: 1184.8343\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 860.4580 - val_loss: 1156.2056\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 855.4378 - val_loss: 1164.0958\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 853.6319 - val_loss: 1185.1231\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 860.8129 - val_loss: 1186.9673\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 855.7892 - val_loss: 1167.4030\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 847.8641 - val_loss: 1171.1986\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 839.6653 - val_loss: 1194.4099\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 854.8130 - val_loss: 1182.5661\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 837.3309 - val_loss: 1170.6119\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 842.6890 - val_loss: 1184.9929\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 841.4554 - val_loss: 1182.9723\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 834.3512 - val_loss: 1194.8387\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 838.2387 - val_loss: 1171.3050\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 828.5647 - val_loss: 1170.0866\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 847.1152 - val_loss: 1188.7898\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 827.1187 - val_loss: 1177.2366\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 816.8004 - val_loss: 1190.4443\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 832.4737 - val_loss: 1191.7888\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 824.0803 - val_loss: 1165.0092\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 805.3443 - val_loss: 1190.6960\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 813.2479 - val_loss: 1183.5098\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 8us/sample - loss: 818.9702 - val_loss: 1183.2370\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 796.2257 - val_loss: 1188.0276\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 800.2548 - val_loss: 1191.3843\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 787.9946 - val_loss: 1178.7354\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 798.3653 - val_loss: 1197.1904\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 800.5607 - val_loss: 1183.5763\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 786.6381 - val_loss: 1193.3012\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 784.9981 - val_loss: 1181.5430\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 785.8560 - val_loss: 1229.9443\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 785.1487 - val_loss: 1182.9630\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 778.6983 - val_loss: 1201.4116\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 771.8635 - val_loss: 1180.6477\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 776.8969 - val_loss: 1199.3137\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 767.3703 - val_loss: 1205.9128\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 763.3212 - val_loss: 1173.1608\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 771.1987 - val_loss: 1187.2147\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 763.3953 - val_loss: 1186.7753\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 780.9536 - val_loss: 1172.9977\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 762.4064 - val_loss: 1177.9431\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 749.9434 - val_loss: 1194.7679\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 760.1842 - val_loss: 1203.3513\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 760.7688 - val_loss: 1186.7201\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 758.0560 - val_loss: 1183.8025\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 754.0179 - val_loss: 1185.2227\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 749.5564 - val_loss: 1186.2488\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 762.3182 - val_loss: 1194.9912\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 749.5843 - val_loss: 1184.2156\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 743.3266 - val_loss: 1181.6455\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 724.6662 - val_loss: 1186.1722\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 734.2880 - val_loss: 1194.3460\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 729.4237 - val_loss: 1195.3889\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 730.2701 - val_loss: 1209.0135\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 731.1872 - val_loss: 1183.8256\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 721.9512 - val_loss: 1184.3981\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 712.1459 - val_loss: 1184.3223\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 709.6259 - val_loss: 1189.6841\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 723.3634 - val_loss: 1202.7952\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 741.0888 - val_loss: 1188.1603\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 725.2045 - val_loss: 1194.3416\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 719.0449 - val_loss: 1198.6581\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 719.1576 - val_loss: 1187.3943\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 712.1229 - val_loss: 1196.5239\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 696.9586 - val_loss: 1190.0383\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 712.3008 - val_loss: 1193.6124\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 704.1335 - val_loss: 1198.7932\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 716.9709 - val_loss: 1226.2058\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 726.5105 - val_loss: 1197.6048\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 717.0354 - val_loss: 1202.9837\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 729.9376 - val_loss: 1191.7669\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 701.2859 - val_loss: 1198.4494\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 689.0028 - val_loss: 1221.0031\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 701.3740 - val_loss: 1204.5699\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 695.1664 - val_loss: 1195.0949\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 682.3309 - val_loss: 1190.8560\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 674.5869 - val_loss: 1211.2600\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 714.4700 - val_loss: 1208.6984\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 702.0186 - val_loss: 1189.3056\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1266.7748657006264,
          1257.1073133653358,
          1245.9567115711789,
          1238.1848369481354,
          1230.3966597332753,
          1222.6299152699362,
          1215.9924710007738,
          1211.2178181337886,
          1205.1729801194042,
          1202.4692170101248,
          1197.0588828561683,
          1199.0471619766079,
          1193.2368179238676,
          1185.2610657337984,
          1180.174899049051,
          1176.6279471890587,
          1174.2598492163609,
          1171.1418407586286,
          1165.8636724526739,
          1161.4940481727099,
          1160.2686322818233,
          1157.7747555020005,
          1157.5267020180204,
          1151.7756794374359,
          1150.0638808117903,
          1149.5172304192072,
          1144.9424905388776,
          1160.4416114221194,
          1143.017043605366,
          1137.422588035533,
          1134.6074158534848,
          1131.5178410889798,
          1130.5451175742478,
          1131.3950678502388,
          1129.0698212814254,
          1124.8682513867109,
          1120.1969535832227,
          1116.5470050378053,
          1114.6619096124534,
          1112.383391838311,
          1111.496564309392,
          1114.1352495002632,
          1103.8736012460522,
          1108.450811000202,
          1099.9906136080763,
          1096.3562019796393,
          1093.8825677229547,
          1091.7666346318788,
          1089.4592250887463,
          1086.788162148833,
          1082.897573926584,
          1088.7853388781534,
          1094.9328983139364,
          1077.9766389438712,
          1067.9941446271048,
          1071.8930695393963,
          1062.180771421712,
          1063.3669860729694,
          1058.3843113310427,
          1058.390075989565,
          1057.75397566877,
          1044.253232819393,
          1039.1478310403925,
          1038.9987586132147,
          1045.3983244619606,
          1030.014855053804,
          1024.288982482384,
          1019.1747179752317,
          1020.1266475453366,
          1020.9977813041447,
          1023.7750151859678,
          1035.7888660664212,
          1015.4578859257703,
          1004.4826740075955,
          1002.9420790319479,
          995.6184786621975,
          995.5452146161074,
          991.40177527979,
          986.5027358607966,
          976.1635909615,
          976.116491451994,
          972.7190914639782,
          972.7369266293535,
          975.2894939929755,
          976.6081854814706,
          957.7147354116797,
          961.916671162405,
          963.0659258016154,
          951.0738576304251,
          946.9457665247025,
          940.6915870545645,
          937.068065361052,
          932.4211781442297,
          925.2418166015821,
          923.1737799283998,
          922.5143310351053,
          919.462004189408,
          914.4976465421424,
          910.4559458841034,
          902.7651053750493,
          902.516545227063,
          896.5741577882768,
          907.0281707414864,
          895.893722702464,
          912.7179850105391,
          903.3950869306094,
          881.9372707540582,
          882.7620551715711,
          879.5806762989045,
          869.7369076346548,
          875.6047349696124,
          887.7560020433009,
          860.4580324370708,
          855.4378333251415,
          853.6319426235752,
          860.8129386771818,
          855.7891855249429,
          847.8641467204379,
          839.6652841944721,
          854.8130350214268,
          837.330894468685,
          842.6890083773475,
          841.455440395027,
          834.3512044156604,
          838.2387431565234,
          828.5646951238258,
          847.1151817479353,
          827.1186531147977,
          816.800350711655,
          832.4736725030979,
          824.0803275160927,
          805.3443451998634,
          813.2479201539236,
          818.9702262625991,
          796.2257260968362,
          800.2547629329421,
          787.9946156760507,
          798.3652600153955,
          800.5607468088906,
          786.6380772993181,
          784.9980752691373,
          785.8559635423194,
          785.1487088475936,
          778.6982820739196,
          771.8634861651035,
          776.8968772666354,
          767.3703472779226,
          763.3212186646217,
          771.1987268827663,
          763.3952934490026,
          780.9536088752632,
          762.4063946509949,
          749.9433981354451,
          760.1841930254012,
          760.7687804453689,
          758.0560165292064,
          754.0179075214699,
          749.556371382489,
          762.3182083720408,
          749.5842901260073,
          743.3265843243214,
          724.6662141514227,
          734.2879671471999,
          729.4237275828286,
          730.270140071706,
          731.1871592091586,
          721.9511841260911,
          712.1458674634127,
          709.6258760570452,
          723.3633553562505,
          741.0887640139754,
          725.2045006714724,
          719.0448810461888,
          719.157578062337,
          712.1229446805836,
          696.958633695166,
          712.3007907228716,
          704.1335035790321,
          716.9708968415537,
          726.5104735814093,
          717.0353907586483,
          729.9375978741016,
          701.2858607428142,
          689.0027714758568,
          701.3740310133495,
          695.1664434707975,
          682.3309119335272,
          674.5869371694531,
          714.4700314200711,
          702.0186142662328
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1278.5656311145306,
          1272.4424232007125,
          1266.4796627236674,
          1258.4730257899053,
          1257.122088205702,
          1250.6509238228377,
          1256.2491267823684,
          1244.7884160438234,
          1249.814783745355,
          1240.1362269684382,
          1249.917611865068,
          1237.4304965860456,
          1230.072759780899,
          1228.3953041579982,
          1226.3070574068752,
          1231.7320427643122,
          1232.7204085603037,
          1225.227460611457,
          1221.7506433964636,
          1218.5341736170292,
          1216.6965719023758,
          1224.3636883142265,
          1215.3038354800606,
          1212.6736309815676,
          1216.108912909894,
          1213.835996564701,
          1247.4696967241782,
          1214.9549751962525,
          1218.7989989793775,
          1210.193258306558,
          1208.3316975944094,
          1205.3584075676838,
          1209.4331930010214,
          1206.937536618646,
          1209.5329129907682,
          1202.186715220315,
          1203.4296065032756,
          1207.06887964861,
          1199.0831714214771,
          1196.2430873248964,
          1204.4175196330773,
          1198.2406919122532,
          1208.2517728467062,
          1198.3144508240957,
          1195.8515461733712,
          1189.954732426379,
          1199.2402760605307,
          1199.8162778154842,
          1192.6154416766797,
          1189.541167141991,
          1187.4290924225252,
          1209.5408236953187,
          1205.2324603294737,
          1188.3222436684991,
          1185.1855314050908,
          1186.4817502802207,
          1182.6778079794576,
          1183.8029087786447,
          1178.7425776011771,
          1180.2725353007663,
          1192.5392725431825,
          1175.5885383540174,
          1179.1760491095206,
          1184.7683733811425,
          1178.2782046063908,
          1169.4896027278346,
          1168.72348320477,
          1175.973391242034,
          1181.123763900399,
          1174.5035089034222,
          1189.0411361042618,
          1198.0715572891863,
          1162.624901330373,
          1180.3889127356126,
          1186.5759372072466,
          1169.1167315537978,
          1181.7681775350286,
          1164.9679755499062,
          1168.4907781472061,
          1165.4763394346785,
          1164.154372927228,
          1171.508581197833,
          1161.5412585906952,
          1181.8682773208388,
          1171.50047633613,
          1171.8049490197952,
          1183.5039609821474,
          1194.4316672567425,
          1171.2313577802286,
          1166.9763813160312,
          1172.5610679808517,
          1167.1437202791712,
          1163.7142214533176,
          1168.4919301658922,
          1154.64001344903,
          1172.4903466052754,
          1173.3097095172056,
          1169.5990092795957,
          1171.4554053429542,
          1166.4193057956547,
          1158.8106153636172,
          1170.0956721117484,
          1177.6343385624891,
          1178.3552497969329,
          1182.6651690880665,
          1165.079610551888,
          1174.0791347297895,
          1164.9210882620987,
          1167.330662260941,
          1171.6831764051378,
          1165.9992297111162,
          1184.8343247130038,
          1156.2055919854513,
          1164.0958491834629,
          1185.1231136991335,
          1186.9672990351085,
          1167.4029503564736,
          1171.1985729302437,
          1194.4098570834951,
          1182.5660887986498,
          1170.6119389905944,
          1184.9929021021062,
          1182.9722881053237,
          1194.8387107015349,
          1171.3050334189204,
          1170.0865717173244,
          1188.7898265225915,
          1177.236565876944,
          1190.4442811563972,
          1191.788774519806,
          1165.0091880246061,
          1190.695954378846,
          1183.5098170526571,
          1183.2370173437264,
          1188.0275800332818,
          1191.3842972930793,
          1178.7354319472818,
          1197.1904095423492,
          1183.576273775684,
          1193.301202927416,
          1181.543017778842,
          1229.9443289123988,
          1182.9630196469807,
          1201.411645277722,
          1180.6477261779064,
          1199.3137161747497,
          1205.9128133782601,
          1173.1608151653284,
          1187.2146520895735,
          1186.7752933506022,
          1172.9976604210008,
          1177.9430593762534,
          1194.7678609403981,
          1203.351328297323,
          1186.7200762754646,
          1183.8025362279818,
          1185.2226570577643,
          1186.2488196606098,
          1194.991161223282,
          1184.215570542003,
          1181.6455110680347,
          1186.172244148262,
          1194.3460337208774,
          1195.3888817713164,
          1209.0134577440408,
          1183.8255749127418,
          1184.3981303782411,
          1184.3223057684354,
          1189.684137130563,
          1202.7952258880903,
          1188.1603096272966,
          1194.3415946646828,
          1198.6581175264673,
          1187.3942813326366,
          1196.5238630204155,
          1190.0383213151067,
          1193.6124343948563,
          1198.7931975431043,
          1226.2057645532682,
          1197.6047571586516,
          1202.9837490561397,
          1191.766861099601,
          1198.4494080850445,
          1221.0030557721561,
          1204.5699493732532,
          1195.094942871975,
          1190.855969882045,
          1211.2599520471977,
          1208.698438107047,
          1189.305595892093
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"2f8c1096-bab9-48a2-8f2f-addd7e2567d9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"2f8c1096-bab9-48a2-8f2f-addd7e2567d9\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '2f8c1096-bab9-48a2-8f2f-addd7e2567d9',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1266.7748657006264, 1257.1073133653358, 1245.9567115711789, 1238.1848369481354, 1230.3966597332753, 1222.6299152699362, 1215.9924710007738, 1211.2178181337886, 1205.1729801194042, 1202.4692170101248, 1197.0588828561683, 1199.0471619766079, 1193.2368179238676, 1185.2610657337984, 1180.174899049051, 1176.6279471890587, 1174.2598492163609, 1171.1418407586286, 1165.8636724526739, 1161.4940481727099, 1160.2686322818233, 1157.7747555020005, 1157.5267020180204, 1151.7756794374359, 1150.0638808117903, 1149.5172304192072, 1144.9424905388776, 1160.4416114221194, 1143.017043605366, 1137.422588035533, 1134.6074158534848, 1131.5178410889798, 1130.5451175742478, 1131.3950678502388, 1129.0698212814254, 1124.8682513867109, 1120.1969535832227, 1116.5470050378053, 1114.6619096124534, 1112.383391838311, 1111.496564309392, 1114.1352495002632, 1103.8736012460522, 1108.450811000202, 1099.9906136080763, 1096.3562019796393, 1093.8825677229547, 1091.7666346318788, 1089.4592250887463, 1086.788162148833, 1082.897573926584, 1088.7853388781534, 1094.9328983139364, 1077.9766389438712, 1067.9941446271048, 1071.8930695393963, 1062.180771421712, 1063.3669860729694, 1058.3843113310427, 1058.390075989565, 1057.75397566877, 1044.253232819393, 1039.1478310403925, 1038.9987586132147, 1045.3983244619606, 1030.014855053804, 1024.288982482384, 1019.1747179752317, 1020.1266475453366, 1020.9977813041447, 1023.7750151859678, 1035.7888660664212, 1015.4578859257703, 1004.4826740075955, 1002.9420790319479, 995.6184786621975, 995.5452146161074, 991.40177527979, 986.5027358607966, 976.1635909615, 976.116491451994, 972.7190914639782, 972.7369266293535, 975.2894939929755, 976.6081854814706, 957.7147354116797, 961.916671162405, 963.0659258016154, 951.0738576304251, 946.9457665247025, 940.6915870545645, 937.068065361052, 932.4211781442297, 925.2418166015821, 923.1737799283998, 922.5143310351053, 919.462004189408, 914.4976465421424, 910.4559458841034, 902.7651053750493, 902.516545227063, 896.5741577882768, 907.0281707414864, 895.893722702464, 912.7179850105391, 903.3950869306094, 881.9372707540582, 882.7620551715711, 879.5806762989045, 869.7369076346548, 875.6047349696124, 887.7560020433009, 860.4580324370708, 855.4378333251415, 853.6319426235752, 860.8129386771818, 855.7891855249429, 847.8641467204379, 839.6652841944721, 854.8130350214268, 837.330894468685, 842.6890083773475, 841.455440395027, 834.3512044156604, 838.2387431565234, 828.5646951238258, 847.1151817479353, 827.1186531147977, 816.800350711655, 832.4736725030979, 824.0803275160927, 805.3443451998634, 813.2479201539236, 818.9702262625991, 796.2257260968362, 800.2547629329421, 787.9946156760507, 798.3652600153955, 800.5607468088906, 786.6380772993181, 784.9980752691373, 785.8559635423194, 785.1487088475936, 778.6982820739196, 771.8634861651035, 776.8968772666354, 767.3703472779226, 763.3212186646217, 771.1987268827663, 763.3952934490026, 780.9536088752632, 762.4063946509949, 749.9433981354451, 760.1841930254012, 760.7687804453689, 758.0560165292064, 754.0179075214699, 749.556371382489, 762.3182083720408, 749.5842901260073, 743.3265843243214, 724.6662141514227, 734.2879671471999, 729.4237275828286, 730.270140071706, 731.1871592091586, 721.9511841260911, 712.1458674634127, 709.6258760570452, 723.3633553562505, 741.0887640139754, 725.2045006714724, 719.0448810461888, 719.157578062337, 712.1229446805836, 696.958633695166, 712.3007907228716, 704.1335035790321, 716.9708968415537, 726.5104735814093, 717.0353907586483, 729.9375978741016, 701.2858607428142, 689.0027714758568, 701.3740310133495, 695.1664434707975, 682.3309119335272, 674.5869371694531, 714.4700314200711, 702.0186142662328]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1278.5656311145306, 1272.4424232007125, 1266.4796627236674, 1258.4730257899053, 1257.122088205702, 1250.6509238228377, 1256.2491267823684, 1244.7884160438234, 1249.814783745355, 1240.1362269684382, 1249.917611865068, 1237.4304965860456, 1230.072759780899, 1228.3953041579982, 1226.3070574068752, 1231.7320427643122, 1232.7204085603037, 1225.227460611457, 1221.7506433964636, 1218.5341736170292, 1216.6965719023758, 1224.3636883142265, 1215.3038354800606, 1212.6736309815676, 1216.108912909894, 1213.835996564701, 1247.4696967241782, 1214.9549751962525, 1218.7989989793775, 1210.193258306558, 1208.3316975944094, 1205.3584075676838, 1209.4331930010214, 1206.937536618646, 1209.5329129907682, 1202.186715220315, 1203.4296065032756, 1207.06887964861, 1199.0831714214771, 1196.2430873248964, 1204.4175196330773, 1198.2406919122532, 1208.2517728467062, 1198.3144508240957, 1195.8515461733712, 1189.954732426379, 1199.2402760605307, 1199.8162778154842, 1192.6154416766797, 1189.541167141991, 1187.4290924225252, 1209.5408236953187, 1205.2324603294737, 1188.3222436684991, 1185.1855314050908, 1186.4817502802207, 1182.6778079794576, 1183.8029087786447, 1178.7425776011771, 1180.2725353007663, 1192.5392725431825, 1175.5885383540174, 1179.1760491095206, 1184.7683733811425, 1178.2782046063908, 1169.4896027278346, 1168.72348320477, 1175.973391242034, 1181.123763900399, 1174.5035089034222, 1189.0411361042618, 1198.0715572891863, 1162.624901330373, 1180.3889127356126, 1186.5759372072466, 1169.1167315537978, 1181.7681775350286, 1164.9679755499062, 1168.4907781472061, 1165.4763394346785, 1164.154372927228, 1171.508581197833, 1161.5412585906952, 1181.8682773208388, 1171.50047633613, 1171.8049490197952, 1183.5039609821474, 1194.4316672567425, 1171.2313577802286, 1166.9763813160312, 1172.5610679808517, 1167.1437202791712, 1163.7142214533176, 1168.4919301658922, 1154.64001344903, 1172.4903466052754, 1173.3097095172056, 1169.5990092795957, 1171.4554053429542, 1166.4193057956547, 1158.8106153636172, 1170.0956721117484, 1177.6343385624891, 1178.3552497969329, 1182.6651690880665, 1165.079610551888, 1174.0791347297895, 1164.9210882620987, 1167.330662260941, 1171.6831764051378, 1165.9992297111162, 1184.8343247130038, 1156.2055919854513, 1164.0958491834629, 1185.1231136991335, 1186.9672990351085, 1167.4029503564736, 1171.1985729302437, 1194.4098570834951, 1182.5660887986498, 1170.6119389905944, 1184.9929021021062, 1182.9722881053237, 1194.8387107015349, 1171.3050334189204, 1170.0865717173244, 1188.7898265225915, 1177.236565876944, 1190.4442811563972, 1191.788774519806, 1165.0091880246061, 1190.695954378846, 1183.5098170526571, 1183.2370173437264, 1188.0275800332818, 1191.3842972930793, 1178.7354319472818, 1197.1904095423492, 1183.576273775684, 1193.301202927416, 1181.543017778842, 1229.9443289123988, 1182.9630196469807, 1201.411645277722, 1180.6477261779064, 1199.3137161747497, 1205.9128133782601, 1173.1608151653284, 1187.2146520895735, 1186.7752933506022, 1172.9976604210008, 1177.9430593762534, 1194.7678609403981, 1203.351328297323, 1186.7200762754646, 1183.8025362279818, 1185.2226570577643, 1186.2488196606098, 1194.991161223282, 1184.215570542003, 1181.6455110680347, 1186.172244148262, 1194.3460337208774, 1195.3888817713164, 1209.0134577440408, 1183.8255749127418, 1184.3981303782411, 1184.3223057684354, 1189.684137130563, 1202.7952258880903, 1188.1603096272966, 1194.3415946646828, 1198.6581175264673, 1187.3942813326366, 1196.5238630204155, 1190.0383213151067, 1193.6124343948563, 1198.7931975431043, 1226.2057645532682, 1197.6047571586516, 1202.9837490561397, 1191.766861099601, 1198.4494080850445, 1221.0030557721561, 1204.5699493732532, 1195.094942871975, 1190.855969882045, 1211.2599520471977, 1208.698438107047, 1189.305595892093]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2f8c1096-bab9-48a2-8f2f-addd7e2567d9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.70% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1189   : Mean absolute error \n",
      "\n",
      "10.19% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss=[history.history[\"val_loss\"][-1]]\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Dropout\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Dropout\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 1s 63us/sample - loss: 11104.9790 - val_loss: 10453.7489\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 6566.8867 - val_loss: 2844.8541\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 2426.2825 - val_loss: 1743.4334\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1758.0013 - val_loss: 1499.9646\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1648.7797 - val_loss: 1414.9995\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1603.1394 - val_loss: 1372.3611\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1572.5695 - val_loss: 1345.1777\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1534.9278 - val_loss: 1323.9781\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1533.2601 - val_loss: 1315.9334\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1520.0375 - val_loss: 1303.2768\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1498.2686 - val_loss: 1295.0109\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1486.9806 - val_loss: 1276.7369\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1484.9899 - val_loss: 1286.5147\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1479.4079 - val_loss: 1271.5623\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1480.0787 - val_loss: 1266.2495\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1463.1334 - val_loss: 1272.2022\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1467.7238 - val_loss: 1260.4349\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1456.3472 - val_loss: 1265.0093\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1455.9374 - val_loss: 1255.0354\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1446.3600 - val_loss: 1249.8433\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1449.37 - 0s 9us/sample - loss: 1450.1909 - val_loss: 1250.4015\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1440.6258 - val_loss: 1253.1815\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1431.8069 - val_loss: 1243.4917\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1434.9901 - val_loss: 1240.8898\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1425.2364 - val_loss: 1253.0599\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1428.5057 - val_loss: 1246.9472\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1416.2872 - val_loss: 1239.2219\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1430.1015 - val_loss: 1237.2325\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1421.6869 - val_loss: 1235.1409\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1416.6408 - val_loss: 1234.9527\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1416.4612 - val_loss: 1233.6598\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1412.3986 - val_loss: 1227.1812\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1425.3353 - val_loss: 1238.6139\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1419.9283 - val_loss: 1239.8970\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1399.1499 - val_loss: 1221.8391\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1407.0488 - val_loss: 1228.7370\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1409.3394 - val_loss: 1224.6134\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1396.8381 - val_loss: 1236.0973\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1411.6219 - val_loss: 1229.3938\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1398.2490 - val_loss: 1229.9638\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1398.1916 - val_loss: 1217.3788\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1384.2995 - val_loss: 1233.2745\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1390.5407 - val_loss: 1228.1004\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1395.2120 - val_loss: 1220.9346\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1395.3530 - val_loss: 1219.3039\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1389.7140 - val_loss: 1233.8624\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1395.0913 - val_loss: 1213.7301\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1381.6248 - val_loss: 1227.7592\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1384.5482 - val_loss: 1219.8518\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1387.0188 - val_loss: 1219.0003\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1380.4804 - val_loss: 1216.9251\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1377.9632 - val_loss: 1214.9142\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1385.9705 - val_loss: 1227.5703\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1382.0176 - val_loss: 1210.1509\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1376.5625 - val_loss: 1220.5135\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1369.3636 - val_loss: 1210.7686\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1380.8688 - val_loss: 1206.8882\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1367.3038 - val_loss: 1206.0918\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1369.0041 - val_loss: 1205.1093\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1380.2853 - val_loss: 1213.8772\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1364.4757 - val_loss: 1206.5962\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1373.1516 - val_loss: 1204.4159\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1371.6352 - val_loss: 1236.2166\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1373.7662 - val_loss: 1209.1358\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1361.3859 - val_loss: 1211.8164\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1363.5736 - val_loss: 1226.3814\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1377.4212 - val_loss: 1208.1215\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1364.3565 - val_loss: 1206.8731\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1371.0126 - val_loss: 1202.2846\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1356.3163 - val_loss: 1202.1810\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1357.4523 - val_loss: 1200.5623\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1344.1066 - val_loss: 1204.2787\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1361.8238 - val_loss: 1212.8770\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1361.7295 - val_loss: 1193.3304\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1357.0677 - val_loss: 1196.8888\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1368.5989 - val_loss: 1209.1847\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1350.5886 - val_loss: 1218.7005\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1355.7820 - val_loss: 1198.4501\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1345.8968 - val_loss: 1198.2013\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1362.0107 - val_loss: 1206.4731\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1355.0454 - val_loss: 1200.8786\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1349.2393 - val_loss: 1195.3978\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1349.8475 - val_loss: 1192.6721\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1339.4379 - val_loss: 1200.3769\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1355.2246 - val_loss: 1214.4751\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1348.6905 - val_loss: 1189.1059\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1351.9411 - val_loss: 1193.1847\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1337.4778 - val_loss: 1196.5896\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1335.9313 - val_loss: 1181.8593\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1332.7091 - val_loss: 1188.3481\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1343.2103 - val_loss: 1186.0661\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1351.5774 - val_loss: 1202.4736\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1346.9153 - val_loss: 1188.4453\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1347.1468 - val_loss: 1184.9129\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1345.2564 - val_loss: 1185.4115\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1334.5641 - val_loss: 1187.8442\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1337.2609 - val_loss: 1215.9389\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1331.9186 - val_loss: 1185.1947\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1333.6374 - val_loss: 1187.8050\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1339.4222 - val_loss: 1194.6724\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1319.8527 - val_loss: 1184.6441\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1329.4333 - val_loss: 1190.5655\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1332.1765 - val_loss: 1180.7449\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1333.8281 - val_loss: 1183.0484\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1331.7469 - val_loss: 1191.7318\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1336.6518 - val_loss: 1179.2892\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1327.7739 - val_loss: 1186.0785\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1324.8376 - val_loss: 1177.4107\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1324.4250 - val_loss: 1182.6995\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1314.3974 - val_loss: 1169.6520\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1329.2771 - val_loss: 1174.1582\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1326.5135 - val_loss: 1179.7389\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1325.2845 - val_loss: 1180.0469\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1323.4041 - val_loss: 1187.0839\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1319.6405 - val_loss: 1179.2829\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1317.5366 - val_loss: 1177.1511\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1318.8555 - val_loss: 1197.3942\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1321.6113 - val_loss: 1195.1070\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1318.9068 - val_loss: 1173.9793\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1316.7837 - val_loss: 1169.7354\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1313.6740 - val_loss: 1176.8291\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1324.3102 - val_loss: 1193.5186\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1322.6203 - val_loss: 1173.0625\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1321.2354 - val_loss: 1168.6972\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1311.6350 - val_loss: 1185.4222\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1308.6336 - val_loss: 1178.6296\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1308.9142 - val_loss: 1169.2232\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1302.0993 - val_loss: 1175.0599\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1313.1137 - val_loss: 1159.1533\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1314.4898 - val_loss: 1184.2666\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1299.7322 - val_loss: 1185.9308\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1309.4084 - val_loss: 1168.6298\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1311.3658 - val_loss: 1165.2541\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1300.9956 - val_loss: 1160.3946\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1310.7514 - val_loss: 1167.0118\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1309.6726 - val_loss: 1164.4704\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1310.0857 - val_loss: 1170.1611\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1305.4869 - val_loss: 1207.5441\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1317.1751 - val_loss: 1168.2666\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1301.0592 - val_loss: 1165.5448\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1297.8416 - val_loss: 1164.1859\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1303.5398 - val_loss: 1181.5926\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1311.8990 - val_loss: 1157.2143\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1291.3246 - val_loss: 1161.1286\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1305.9377 - val_loss: 1172.8775\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1327.5305 - val_loss: 1217.6493\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1303.5474 - val_loss: 1172.1562\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1304.6293 - val_loss: 1177.5680\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1293.3838 - val_loss: 1181.7596\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1310.2340 - val_loss: 1159.4899\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1289.5568 - val_loss: 1164.3255\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1297.2136 - val_loss: 1160.7035\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1284.8328 - val_loss: 1157.7115\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1301.8269 - val_loss: 1161.5477\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1287.7307 - val_loss: 1154.4041\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1305.6720 - val_loss: 1176.2787\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1292.7845 - val_loss: 1226.5046\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1306.8557 - val_loss: 1164.8283\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1301.0184 - val_loss: 1155.5092\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1291.0776 - val_loss: 1156.5951\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1296.9724 - val_loss: 1155.3046\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1292.8188 - val_loss: 1154.7435\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1283.0792 - val_loss: 1155.1941\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1287.3808 - val_loss: 1160.6311\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1282.3430 - val_loss: 1160.0494\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1280.8984 - val_loss: 1156.3235\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1291.0921 - val_loss: 1200.0392\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1293.5578 - val_loss: 1156.9738\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1281.6632 - val_loss: 1154.5868\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1275.7743 - val_loss: 1169.8536\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1284.9450 - val_loss: 1156.6098\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1300.5993 - val_loss: 1152.5275\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1275.3553 - val_loss: 1152.7773\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1282.1892 - val_loss: 1147.2432\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1280.6323 - val_loss: 1156.8527\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1284.5635 - val_loss: 1158.1765\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1274.7754 - val_loss: 1186.2504\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1277.3026 - val_loss: 1155.9479\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1281.0501 - val_loss: 1153.3047\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1280.4864 - val_loss: 1164.0184\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1280.6493 - val_loss: 1152.4166\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1276.4159 - val_loss: 1155.7938\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1271.1711 - val_loss: 1149.9234\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1266.7864 - val_loss: 1149.7889\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1285.1446 - val_loss: 1145.8681\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1292.8837 - val_loss: 1177.9689\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1265.5180 - val_loss: 1150.7878\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1275.7752 - val_loss: 1161.3753\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1276.9346 - val_loss: 1156.5123\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1269.1920 - val_loss: 1181.0014\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1276.9665 - val_loss: 1148.8200\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1268.5219 - val_loss: 1151.1974\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1270.9956 - val_loss: 1149.8363\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1267.1006 - val_loss: 1154.6584\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1276.8413 - val_loss: 1145.1779\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1268.2109 - val_loss: 1152.2165\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1265.4771 - val_loss: 1157.1813\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1271.3710 - val_loss: 1162.7901\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1273.4812 - val_loss: 1150.6599\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1263.9590 - val_loss: 1152.2637\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1498.2686125283158,
          1486.9805913832997,
          1484.9898518619113,
          1479.4078778652622,
          1480.0787413975556,
          1463.133440132822,
          1467.7237892739874,
          1456.3471634648524,
          1455.937425857033,
          1446.3600421643148,
          1450.190920294132,
          1440.6257895773151,
          1431.8068712049767,
          1434.9901128187194,
          1425.236410467998,
          1428.5056987767805,
          1416.287227602313,
          1430.1014660333665,
          1421.6868583414534,
          1416.6408093905482,
          1416.4611601492004,
          1412.398580645616,
          1425.3353021322234,
          1419.9282875956,
          1399.1499462812296,
          1407.048767469248,
          1409.3393625428066,
          1396.8380827260253,
          1411.6219043291856,
          1398.2490045896675,
          1398.1915595293858,
          1384.299484347781,
          1390.5406677968185,
          1395.2120004443193,
          1395.3530052893382,
          1389.7139734452153,
          1395.0913382117724,
          1381.6247855018753,
          1384.5481715350536,
          1387.018792953476,
          1380.4804434400535,
          1377.9631958864531,
          1385.97048292357,
          1382.0175819679996,
          1376.5625117982536,
          1369.363611968266,
          1380.8687912547232,
          1367.3038157510307,
          1369.0040704219643,
          1380.285266179371,
          1364.4757241141422,
          1373.1515505499847,
          1371.6351816970216,
          1373.7662012051646,
          1361.3859057475217,
          1363.573612682036,
          1377.421238677593,
          1364.3564903514764,
          1371.0125940482365,
          1356.316251746729,
          1357.4522948876063,
          1344.1066247340743,
          1361.8237783716177,
          1361.729528806146,
          1357.0676790172026,
          1368.598872816393,
          1350.5885915930246,
          1355.7820249641254,
          1345.8968494844407,
          1362.010705911542,
          1355.0453750552217,
          1349.2392585223533,
          1349.847452130422,
          1339.4379239293648,
          1355.224622960126,
          1348.6905194717203,
          1351.9410705872378,
          1337.4778435896221,
          1335.9312707913623,
          1332.7090678439724,
          1343.210346241047,
          1351.5774041560792,
          1346.9153180104208,
          1347.1467549025904,
          1345.2563738229162,
          1334.5640659856251,
          1337.2609054537893,
          1331.9185582220805,
          1333.6373997197402,
          1339.4222056693104,
          1319.852747245377,
          1329.4333082420544,
          1332.1765046640796,
          1333.828092273309,
          1331.7469498087605,
          1336.6518364495735,
          1327.7738646604219,
          1324.8375725372296,
          1324.4250461355775,
          1314.397426644236,
          1329.2770729286772,
          1326.513453998952,
          1325.2844867358256,
          1323.4041305000424,
          1319.640459065641,
          1317.5365616863612,
          1318.8554816987057,
          1321.6113199004913,
          1318.9068099519689,
          1316.7836577494063,
          1313.6739780019848,
          1324.3101984838313,
          1322.6203159954161,
          1321.2353973847628,
          1311.6349575546813,
          1308.633577721999,
          1308.9142067960681,
          1302.0992942245541,
          1313.113696049181,
          1314.4897943393057,
          1299.7322025302897,
          1309.408447265625,
          1311.3657517112854,
          1300.9956347685622,
          1310.7514398764913,
          1309.6725939836151,
          1310.0857048149408,
          1305.4869000955218,
          1317.1750690711867,
          1301.0591537509242,
          1297.841590013919,
          1303.5398338414252,
          1311.8989528291136,
          1291.3246408680147,
          1305.9376951607383,
          1327.530502255082,
          1303.5474431764776,
          1304.6293387220837,
          1293.3837678158525,
          1310.2339982881272,
          1289.556771849974,
          1297.2136400099243,
          1284.8328107914563,
          1301.8269384922282,
          1287.7306880947103,
          1305.6720066900505,
          1292.7845274177705,
          1306.8557094392686,
          1301.0183929143507,
          1291.0775539351532,
          1296.9724138717697,
          1292.8188047713118,
          1283.0791835138548,
          1287.3808388137281,
          1282.3430048007633,
          1280.8983687910834,
          1291.0920805226401,
          1293.5578213560334,
          1281.6632267087787,
          1275.774324253802,
          1284.9449939471533,
          1300.599349177001,
          1275.355269256708,
          1282.189172953194,
          1280.632283536805,
          1284.5635073799301,
          1274.7754264603595,
          1277.302591792371,
          1281.0501403747398,
          1280.4863853049178,
          1280.6492566219044,
          1276.4158975690311,
          1271.1711172437008,
          1266.7863633190436,
          1285.144571099703,
          1292.8836858155423,
          1265.5180187236815,
          1275.77524282862,
          1276.9345847788234,
          1269.1920473455593,
          1276.9664810391785,
          1268.5218668146556,
          1270.9955976603624,
          1267.1005671386229,
          1276.841264362538,
          1268.2109208062054,
          1265.477145975043,
          1271.37097048028,
          1273.481235709916,
          1263.958950081736
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1295.0108962501724,
          1276.73687473662,
          1286.5146784716435,
          1271.5622834947035,
          1266.2495089527695,
          1272.2021600888875,
          1260.43494012166,
          1265.0093498467108,
          1255.0353834275757,
          1249.843269918781,
          1250.4015167707523,
          1253.181500367753,
          1243.4916603236775,
          1240.8898243832402,
          1253.0599374291126,
          1246.9471820336391,
          1239.2218700261305,
          1237.232488992866,
          1235.1408990279022,
          1234.9527229047476,
          1233.6598345884847,
          1227.1812176992212,
          1238.6139058231088,
          1239.8970389173962,
          1221.839139575396,
          1228.736966944133,
          1224.6134460583846,
          1236.0972888151773,
          1229.3937504112255,
          1229.9638302751216,
          1217.3788180568306,
          1233.2744631402975,
          1228.100432756025,
          1220.9346493020525,
          1219.303932387293,
          1233.8624071022157,
          1213.7301025880179,
          1227.7592220486154,
          1219.8517640249413,
          1219.0003334352912,
          1216.925121649296,
          1214.9142367812562,
          1227.570311594325,
          1210.1509348622826,
          1220.5135067239275,
          1210.7685560827292,
          1206.8882050851746,
          1206.0917868880965,
          1205.1092913596835,
          1213.8771513209736,
          1206.5961717016978,
          1204.415922756589,
          1236.2166090820508,
          1209.1358306988795,
          1211.8163566826483,
          1226.3814092666323,
          1208.121486005215,
          1206.8730538021903,
          1202.2846129428701,
          1202.1809610749667,
          1200.5623460352385,
          1204.2786649096245,
          1212.8769741513481,
          1193.3303657869837,
          1196.8887539242655,
          1209.1847253787582,
          1218.7004746765417,
          1198.4500732666652,
          1198.2013457156577,
          1206.4731487903705,
          1200.878629456117,
          1195.3978175286995,
          1192.6721309388786,
          1200.3768712470783,
          1214.4750813051435,
          1189.105929028756,
          1193.1846649433057,
          1196.589551559641,
          1181.8593096445288,
          1188.348077193085,
          1186.0661490627585,
          1202.4736157760176,
          1188.4452890993146,
          1184.9128536685616,
          1185.4114642895747,
          1187.8442017360371,
          1215.9388953515468,
          1185.1946650402374,
          1187.8049690180799,
          1194.672425674919,
          1184.6440520989338,
          1190.565499351047,
          1180.744916298979,
          1183.0484472519174,
          1191.7318024911644,
          1179.2891565188631,
          1186.0784942706505,
          1177.4107229333376,
          1182.6994976489655,
          1169.6519736471075,
          1174.158154169591,
          1179.7388662426606,
          1180.0468523581233,
          1187.0839245025347,
          1179.282905304769,
          1177.1511170008819,
          1197.3942430984623,
          1195.1069934417376,
          1173.9792884840429,
          1169.7354133442263,
          1176.829084991094,
          1193.5186285122568,
          1173.062483110384,
          1168.697156919514,
          1185.422181436383,
          1178.6296334581239,
          1169.2232018090785,
          1175.0599216899486,
          1159.1532724585877,
          1184.2665656537074,
          1185.9308195448791,
          1168.6297796389754,
          1165.2541293642769,
          1160.3946310945569,
          1167.0118153634999,
          1164.4704219740856,
          1170.161091959211,
          1207.5440872831862,
          1168.2665576739757,
          1165.5447885449435,
          1164.185892598099,
          1181.5925921066648,
          1157.2143297426826,
          1161.1285694367857,
          1172.877500397518,
          1217.649287316946,
          1172.156150253354,
          1177.5679933598842,
          1181.7596321480771,
          1159.4898992017918,
          1164.3255293842114,
          1160.703500439032,
          1157.7115030084078,
          1161.5477022729801,
          1154.4041410254551,
          1176.2786661335097,
          1226.5046040114453,
          1164.8283285321131,
          1155.5091881959502,
          1156.595104326914,
          1155.3045516487398,
          1154.7434918434415,
          1155.19408491864,
          1160.6311333049914,
          1160.049355839832,
          1156.3235219431851,
          1200.0391943369166,
          1156.9737546380354,
          1154.5867701335974,
          1169.8536040139911,
          1156.6097605238463,
          1152.5274529939952,
          1152.7772555813083,
          1147.2431893234912,
          1156.85272066259,
          1158.1765351632996,
          1186.2503856217572,
          1155.9478808476258,
          1153.3046617984103,
          1164.018396218841,
          1152.4165755280135,
          1155.7937985686613,
          1149.9234247079517,
          1149.7888975937046,
          1145.8680781457572,
          1177.9688539323333,
          1150.7878032934457,
          1161.3752585335153,
          1156.5123472395417,
          1181.0013584881237,
          1148.8200081197442,
          1151.1973799358802,
          1149.8362809956277,
          1154.658414465501,
          1145.1779342354575,
          1152.216462264779,
          1157.1813200649815,
          1162.7901293117966,
          1150.6598637414309,
          1152.263653981798
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093,
          1189.305595892093
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"31a41a55-3798-4f6e-9560-d4a719c2646c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"31a41a55-3798-4f6e-9560-d4a719c2646c\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '31a41a55-3798-4f6e-9560-d4a719c2646c',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1498.2686125283158, 1486.9805913832997, 1484.9898518619113, 1479.4078778652622, 1480.0787413975556, 1463.133440132822, 1467.7237892739874, 1456.3471634648524, 1455.937425857033, 1446.3600421643148, 1450.190920294132, 1440.6257895773151, 1431.8068712049767, 1434.9901128187194, 1425.236410467998, 1428.5056987767805, 1416.287227602313, 1430.1014660333665, 1421.6868583414534, 1416.6408093905482, 1416.4611601492004, 1412.398580645616, 1425.3353021322234, 1419.9282875956, 1399.1499462812296, 1407.048767469248, 1409.3393625428066, 1396.8380827260253, 1411.6219043291856, 1398.2490045896675, 1398.1915595293858, 1384.299484347781, 1390.5406677968185, 1395.2120004443193, 1395.3530052893382, 1389.7139734452153, 1395.0913382117724, 1381.6247855018753, 1384.5481715350536, 1387.018792953476, 1380.4804434400535, 1377.9631958864531, 1385.97048292357, 1382.0175819679996, 1376.5625117982536, 1369.363611968266, 1380.8687912547232, 1367.3038157510307, 1369.0040704219643, 1380.285266179371, 1364.4757241141422, 1373.1515505499847, 1371.6351816970216, 1373.7662012051646, 1361.3859057475217, 1363.573612682036, 1377.421238677593, 1364.3564903514764, 1371.0125940482365, 1356.316251746729, 1357.4522948876063, 1344.1066247340743, 1361.8237783716177, 1361.729528806146, 1357.0676790172026, 1368.598872816393, 1350.5885915930246, 1355.7820249641254, 1345.8968494844407, 1362.010705911542, 1355.0453750552217, 1349.2392585223533, 1349.847452130422, 1339.4379239293648, 1355.224622960126, 1348.6905194717203, 1351.9410705872378, 1337.4778435896221, 1335.9312707913623, 1332.7090678439724, 1343.210346241047, 1351.5774041560792, 1346.9153180104208, 1347.1467549025904, 1345.2563738229162, 1334.5640659856251, 1337.2609054537893, 1331.9185582220805, 1333.6373997197402, 1339.4222056693104, 1319.852747245377, 1329.4333082420544, 1332.1765046640796, 1333.828092273309, 1331.7469498087605, 1336.6518364495735, 1327.7738646604219, 1324.8375725372296, 1324.4250461355775, 1314.397426644236, 1329.2770729286772, 1326.513453998952, 1325.2844867358256, 1323.4041305000424, 1319.640459065641, 1317.5365616863612, 1318.8554816987057, 1321.6113199004913, 1318.9068099519689, 1316.7836577494063, 1313.6739780019848, 1324.3101984838313, 1322.6203159954161, 1321.2353973847628, 1311.6349575546813, 1308.633577721999, 1308.9142067960681, 1302.0992942245541, 1313.113696049181, 1314.4897943393057, 1299.7322025302897, 1309.408447265625, 1311.3657517112854, 1300.9956347685622, 1310.7514398764913, 1309.6725939836151, 1310.0857048149408, 1305.4869000955218, 1317.1750690711867, 1301.0591537509242, 1297.841590013919, 1303.5398338414252, 1311.8989528291136, 1291.3246408680147, 1305.9376951607383, 1327.530502255082, 1303.5474431764776, 1304.6293387220837, 1293.3837678158525, 1310.2339982881272, 1289.556771849974, 1297.2136400099243, 1284.8328107914563, 1301.8269384922282, 1287.7306880947103, 1305.6720066900505, 1292.7845274177705, 1306.8557094392686, 1301.0183929143507, 1291.0775539351532, 1296.9724138717697, 1292.8188047713118, 1283.0791835138548, 1287.3808388137281, 1282.3430048007633, 1280.8983687910834, 1291.0920805226401, 1293.5578213560334, 1281.6632267087787, 1275.774324253802, 1284.9449939471533, 1300.599349177001, 1275.355269256708, 1282.189172953194, 1280.632283536805, 1284.5635073799301, 1274.7754264603595, 1277.302591792371, 1281.0501403747398, 1280.4863853049178, 1280.6492566219044, 1276.4158975690311, 1271.1711172437008, 1266.7863633190436, 1285.144571099703, 1292.8836858155423, 1265.5180187236815, 1275.77524282862, 1276.9345847788234, 1269.1920473455593, 1276.9664810391785, 1268.5218668146556, 1270.9955976603624, 1267.1005671386229, 1276.841264362538, 1268.2109208062054, 1265.477145975043, 1271.37097048028, 1273.481235709916, 1263.958950081736]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1295.0108962501724, 1276.73687473662, 1286.5146784716435, 1271.5622834947035, 1266.2495089527695, 1272.2021600888875, 1260.43494012166, 1265.0093498467108, 1255.0353834275757, 1249.843269918781, 1250.4015167707523, 1253.181500367753, 1243.4916603236775, 1240.8898243832402, 1253.0599374291126, 1246.9471820336391, 1239.2218700261305, 1237.232488992866, 1235.1408990279022, 1234.9527229047476, 1233.6598345884847, 1227.1812176992212, 1238.6139058231088, 1239.8970389173962, 1221.839139575396, 1228.736966944133, 1224.6134460583846, 1236.0972888151773, 1229.3937504112255, 1229.9638302751216, 1217.3788180568306, 1233.2744631402975, 1228.100432756025, 1220.9346493020525, 1219.303932387293, 1233.8624071022157, 1213.7301025880179, 1227.7592220486154, 1219.8517640249413, 1219.0003334352912, 1216.925121649296, 1214.9142367812562, 1227.570311594325, 1210.1509348622826, 1220.5135067239275, 1210.7685560827292, 1206.8882050851746, 1206.0917868880965, 1205.1092913596835, 1213.8771513209736, 1206.5961717016978, 1204.415922756589, 1236.2166090820508, 1209.1358306988795, 1211.8163566826483, 1226.3814092666323, 1208.121486005215, 1206.8730538021903, 1202.2846129428701, 1202.1809610749667, 1200.5623460352385, 1204.2786649096245, 1212.8769741513481, 1193.3303657869837, 1196.8887539242655, 1209.1847253787582, 1218.7004746765417, 1198.4500732666652, 1198.2013457156577, 1206.4731487903705, 1200.878629456117, 1195.3978175286995, 1192.6721309388786, 1200.3768712470783, 1214.4750813051435, 1189.105929028756, 1193.1846649433057, 1196.589551559641, 1181.8593096445288, 1188.348077193085, 1186.0661490627585, 1202.4736157760176, 1188.4452890993146, 1184.9128536685616, 1185.4114642895747, 1187.8442017360371, 1215.9388953515468, 1185.1946650402374, 1187.8049690180799, 1194.672425674919, 1184.6440520989338, 1190.565499351047, 1180.744916298979, 1183.0484472519174, 1191.7318024911644, 1179.2891565188631, 1186.0784942706505, 1177.4107229333376, 1182.6994976489655, 1169.6519736471075, 1174.158154169591, 1179.7388662426606, 1180.0468523581233, 1187.0839245025347, 1179.282905304769, 1177.1511170008819, 1197.3942430984623, 1195.1069934417376, 1173.9792884840429, 1169.7354133442263, 1176.829084991094, 1193.5186285122568, 1173.062483110384, 1168.697156919514, 1185.422181436383, 1178.6296334581239, 1169.2232018090785, 1175.0599216899486, 1159.1532724585877, 1184.2665656537074, 1185.9308195448791, 1168.6297796389754, 1165.2541293642769, 1160.3946310945569, 1167.0118153634999, 1164.4704219740856, 1170.161091959211, 1207.5440872831862, 1168.2665576739757, 1165.5447885449435, 1164.185892598099, 1181.5925921066648, 1157.2143297426826, 1161.1285694367857, 1172.877500397518, 1217.649287316946, 1172.156150253354, 1177.5679933598842, 1181.7596321480771, 1159.4898992017918, 1164.3255293842114, 1160.703500439032, 1157.7115030084078, 1161.5477022729801, 1154.4041410254551, 1176.2786661335097, 1226.5046040114453, 1164.8283285321131, 1155.5091881959502, 1156.595104326914, 1155.3045516487398, 1154.7434918434415, 1155.19408491864, 1160.6311333049914, 1160.049355839832, 1156.3235219431851, 1200.0391943369166, 1156.9737546380354, 1154.5867701335974, 1169.8536040139911, 1156.6097605238463, 1152.5274529939952, 1152.7772555813083, 1147.2431893234912, 1156.85272066259, 1158.1765351632996, 1186.2503856217572, 1155.9478808476258, 1153.3046617984103, 1164.018396218841, 1152.4165755280135, 1155.7937985686613, 1149.9234247079517, 1149.7888975937046, 1145.8680781457572, 1177.9688539323333, 1150.7878032934457, 1161.3752585335153, 1156.5123472395417, 1181.0013584881237, 1148.8200081197442, 1151.1973799358802, 1149.8362809956277, 1154.658414465501, 1145.1779342354575, 1152.216462264779, 1157.1813200649815, 1162.7901293117966, 1150.6598637414309, 1152.263653981798]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093, 1189.305595892093]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('31a41a55-3798-4f6e-9560-d4a719c2646c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.54% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1152   : Mean absolute error \n",
      "\n",
      "9.82% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Batchnorm\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Batchnorm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 3s 145us/sample - loss: 11193.2647 - val_loss: 11118.9992\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11176.2126 - val_loss: 11081.1866\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11140.8903 - val_loss: 11036.2253\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 11086.9974 - val_loss: 10963.5740\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 11010.0876 - val_loss: 10843.1675\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10903.1470 - val_loss: 10675.2024\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 10759.9926 - val_loss: 10465.1814\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10573.1849 - val_loss: 10204.1006\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10335.5598 - val_loss: 9889.6445\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10037.0181 - val_loss: 9517.1134\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 9670.9168 - val_loss: 9070.6666\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 9225.3658 - val_loss: 8560.1470\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 8692.8814 - val_loss: 7953.4732\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 8063.2196 - val_loss: 7254.4370\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 7331.0448 - val_loss: 6435.0588\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 6491.2140 - val_loss: 5625.5027\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 5585.9774 - val_loss: 4677.2923\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 4746.4746 - val_loss: 4717.0548\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 4233.0078 - val_loss: 3861.1672\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 3932.7563 - val_loss: 3321.2411\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 3602.2882 - val_loss: 3005.2882\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 3252.5435 - val_loss: 2426.8739\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 2876.55 - 0s 24us/sample - loss: 2845.7960 - val_loss: 1971.5735\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 2421.5980 - val_loss: 1603.9622\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1994.0238 - val_loss: 1562.1499\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1637.4536 - val_loss: 1838.4096\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1467.2673 - val_loss: 2075.7206\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1410.2100 - val_loss: 1954.2117\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1396.8758 - val_loss: 1929.4825\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1378.5364 - val_loss: 1672.0525\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1382.3299 - val_loss: 1556.8032\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1348.6268 - val_loss: 1466.4685\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1359.3389 - val_loss: 1415.3034\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1349.8793 - val_loss: 1360.1662\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1343.1640 - val_loss: 1272.2651\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1327.5515 - val_loss: 1279.6427\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1334.0136 - val_loss: 1305.2157\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1297.0916 - val_loss: 1217.8001\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1323.5633 - val_loss: 1191.5123\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1286.2231 - val_loss: 1204.1306\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1275.0306 - val_loss: 1180.8375\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1277.4404 - val_loss: 1189.0957\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1257.5648 - val_loss: 1171.9154\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1291.8856 - val_loss: 1176.6572\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1295.5301 - val_loss: 1163.1427\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1254.6497 - val_loss: 1160.8444\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1258.4959 - val_loss: 1155.0543\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1271.4604 - val_loss: 1165.4884\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1242.6009 - val_loss: 1154.7624\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1248.6274 - val_loss: 1157.7970\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1290.7233 - val_loss: 1160.1586\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1234.6194 - val_loss: 1147.9042\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1252.9308 - val_loss: 1140.7020\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1224.5361 - val_loss: 1134.0498\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1228.9950 - val_loss: 1132.9409\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1247.5544 - val_loss: 1140.4254\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1229.5037 - val_loss: 1124.2810\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1192.8499 - val_loss: 1131.5851\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1207.0235 - val_loss: 1138.6018\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1220.8453 - val_loss: 1139.5551\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1194.8919 - val_loss: 1133.4249\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1215.5128 - val_loss: 1141.9666\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1186.5354 - val_loss: 1125.5519\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1204.5855 - val_loss: 1124.7725\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1197.7187 - val_loss: 1112.9362\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1189.7976 - val_loss: 1121.2668\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1174.4498 - val_loss: 1128.0468\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1167.0262 - val_loss: 1126.8588\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1153.6972 - val_loss: 1114.0022\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1172.8287 - val_loss: 1120.7516\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1183.5383 - val_loss: 1132.3109\n",
      "Epoch 72/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1193.0288 - val_loss: 1118.6670\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1172.1359 - val_loss: 1118.3066\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1195.8703 - val_loss: 1115.3266\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1172.0994 - val_loss: 1118.5195\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1153.7113 - val_loss: 1116.5877\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1158.3088 - val_loss: 1117.9578\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1173.4218 - val_loss: 1121.2880\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1160.5391 - val_loss: 1125.5864\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1140.7779 - val_loss: 1110.3520\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1138.4854 - val_loss: 1116.9711\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1154.1573 - val_loss: 1114.4593\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1168.7985 - val_loss: 1125.6049\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1174.1883 - val_loss: 1112.9222\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.9944 - val_loss: 1132.4425\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.6502 - val_loss: 1133.8511\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1146.2982 - val_loss: 1120.6855\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1131.8326 - val_loss: 1107.6459\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1121.3986 - val_loss: 1110.9026\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1157.2089 - val_loss: 1119.5916\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1124.3968 - val_loss: 1116.2197\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1105.5712 - val_loss: 1117.8576\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1117.7205 - val_loss: 1114.0784\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1140.4847 - val_loss: 1106.4871\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1119.6368 - val_loss: 1111.6061\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1105.3281 - val_loss: 1101.3761\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1115.8759 - val_loss: 1142.7233\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1098.0702 - val_loss: 1119.6030\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1086.2334 - val_loss: 1124.4008\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1128.7836 - val_loss: 1108.7213\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1128.9422 - val_loss: 1111.8863\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1108.3779 - val_loss: 1096.9900\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1097.9500 - val_loss: 1114.9008\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1093.3787 - val_loss: 1098.8386\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1116.6114 - val_loss: 1101.7040\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1088.6732 - val_loss: 1115.0550\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1098.3480 - val_loss: 1110.6360\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1095.3243 - val_loss: 1103.3233\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1079.0205 - val_loss: 1109.8486\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.4890 - val_loss: 1100.5451\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1074.0212 - val_loss: 1119.9460\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1067.7108 - val_loss: 1113.2627\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1093.6405 - val_loss: 1141.4658\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1066.2109 - val_loss: 1106.2099\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1081.8215 - val_loss: 1118.3560\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1078.2988 - val_loss: 1121.4437\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1063.2207 - val_loss: 1122.9860\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1057.1188 - val_loss: 1114.3174\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1054.1422 - val_loss: 1108.3190\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1073.6962 - val_loss: 1103.5105\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1070.5807 - val_loss: 1128.0763\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1087.5598 - val_loss: 1097.2219\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1043.4388 - val_loss: 1115.3279\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1083.6905 - val_loss: 1099.1089\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1044.1276 - val_loss: 1103.7768\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1040.1897 - val_loss: 1111.2665\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1043.9445 - val_loss: 1112.1945\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1053.8918 - val_loss: 1105.5886\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1071.5553 - val_loss: 1107.4800\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1037.2635 - val_loss: 1093.4831\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1037.6593 - val_loss: 1102.3031\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1039.3392 - val_loss: 1101.1561\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1062.4456 - val_loss: 1090.3819\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1057.4648 - val_loss: 1101.8239\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1039.4343 - val_loss: 1096.9509\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1032.7054 - val_loss: 1105.2940\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1016.4292 - val_loss: 1105.2314\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1024.7182 - val_loss: 1112.8374\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1022.8647 - val_loss: 1106.9557\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1050.0108 - val_loss: 1094.0831\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1041.1654 - val_loss: 1094.8337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1032.9513 - val_loss: 1103.5276\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1011.9039 - val_loss: 1112.1450\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1010.8992 - val_loss: 1102.1539\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1020.2334 - val_loss: 1100.7314\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1006.5899 - val_loss: 1095.9733\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1018.1540 - val_loss: 1094.0600\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1012.9936 - val_loss: 1121.5801\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1085.6748 - val_loss: 1103.3289\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1038.3393 - val_loss: 1090.7106\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1019.9932 - val_loss: 1093.0645\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1001.1250 - val_loss: 1103.7525\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1011.5483 - val_loss: 1086.1103\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1036.6678 - val_loss: 1090.3884\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1020.3916 - val_loss: 1099.4522\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1022.1331 - val_loss: 1099.8173\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 992.7807 - val_loss: 1086.6232\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1025.3808 - val_loss: 1096.4918\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1016.5331 - val_loss: 1096.3301\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1012.2567 - val_loss: 1100.2756\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1027.8201 - val_loss: 1102.2511\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 996.3717 - val_loss: 1114.5079\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 991.4358 - val_loss: 1102.2929\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 993.6690 - val_loss: 1092.7047\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 994.4592 - val_loss: 1089.5950\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1012.4964 - val_loss: 1116.2687\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 996.1973 - val_loss: 1099.9498\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1005.1224 - val_loss: 1094.8330\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1015.2761 - val_loss: 1105.2084\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1000.3305 - val_loss: 1116.3443\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 975.1200 - val_loss: 1102.0386\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1003.6176 - val_loss: 1099.0346\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 981.6689 - val_loss: 1099.0341\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 979.8929 - val_loss: 1105.0485\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 987.5056 - val_loss: 1096.1787\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 978.7244 - val_loss: 1089.1238\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 994.1271 - val_loss: 1094.2393\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 993.0840 - val_loss: 1092.3232\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 969.7014 - val_loss: 1093.7163\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 979.9664 - val_loss: 1121.2068\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 983.5719 - val_loss: 1111.6157\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 971.0952 - val_loss: 1097.9557\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 978.6671 - val_loss: 1092.2316\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 957.3753 - val_loss: 1106.0347\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 969.6598 - val_loss: 1088.3501\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 952.3710 - val_loss: 1111.1964\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 977.1275 - val_loss: 1095.2141\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 970.1585 - val_loss: 1101.1431\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 972.0557 - val_loss: 1114.1087\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 968.1702 - val_loss: 1103.1020\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 957.6343 - val_loss: 1089.8955\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 958.9449 - val_loss: 1097.0990\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 956.5867 - val_loss: 1086.8435\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 959.5763 - val_loss: 1108.4275\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 956.6764 - val_loss: 1101.6709\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 967.6569 - val_loss: 1095.9151\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 959.0988 - val_loss: 1086.2684\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 949.5536 - val_loss: 1107.3311\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 954.6930 - val_loss: 1086.7402\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 949.0192 - val_loss: 1112.9760\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.263653981798"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_val_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9670.91682945972,
          9225.365781892295,
          8692.88135597867,
          8063.2195689260825,
          7331.044840315289,
          6491.214032069318,
          5585.977350388353,
          4746.474561692432,
          4233.007781853914,
          3932.756330864545,
          3602.2881573144864,
          3252.5435080427865,
          2845.7959529738255,
          2421.5979788171903,
          1994.0238231951903,
          1637.4535896357875,
          1467.2673102410015,
          1410.2099654658753,
          1396.8757829928127,
          1378.5364121031087,
          1382.3299062915141,
          1348.6268293657258,
          1359.3389318331176,
          1349.8793044532974,
          1343.164037899907,
          1327.551466283039,
          1334.0136426975762,
          1297.0916281501827,
          1323.5633182651848,
          1286.2231099442536,
          1275.0305727998264,
          1277.4404222462779,
          1257.5647809059415,
          1291.8855973284446,
          1295.530083221258,
          1254.6497144039347,
          1258.4959240460744,
          1271.4604310562931,
          1242.6009434343746,
          1248.6274426301352,
          1290.7232976882472,
          1234.6193962211908,
          1252.9307864510226,
          1224.5360512283107,
          1228.9949926449394,
          1247.554435477554,
          1229.5037367908515,
          1192.849943985221,
          1207.0235386418751,
          1220.8453124853133,
          1194.8919210210217,
          1215.5127564333281,
          1186.5353589988267,
          1204.585546737925,
          1197.7187311032121,
          1189.7975732999548,
          1174.4497948817316,
          1167.0261615503043,
          1153.6972028152102,
          1172.8286616981495,
          1183.5382645268323,
          1193.0288316762253,
          1172.1358703772387,
          1195.8703210378233,
          1172.09944153138,
          1153.7112963921038,
          1158.3088082970803,
          1173.4217993883708,
          1160.5391109413772,
          1140.7779437719712,
          1138.4853796873824,
          1154.1572644539867,
          1168.798509596631,
          1174.1882815241504,
          1150.9944465963067,
          1143.6501574503866,
          1146.2981593206791,
          1131.8325840300015,
          1121.3986476655712,
          1157.2088870516468,
          1124.3968429733713,
          1105.5712445130778,
          1117.7204660829093,
          1140.4846793313004,
          1119.6368481391462,
          1105.3280550182428,
          1115.8759471892545,
          1098.0702347588103,
          1086.2333560421157,
          1128.783601034565,
          1128.9422272811655,
          1108.3779257710673,
          1097.9499736668854,
          1093.3787435711756,
          1116.6113656248433,
          1088.6731677899647,
          1098.3479822195914,
          1095.324334309243,
          1079.020453374085,
          1061.4889643737781,
          1074.0211670949939,
          1067.7107833149391,
          1093.640474462117,
          1066.2108909189283,
          1081.8214948220843,
          1078.2988091058235,
          1063.2207040551527,
          1057.1187652838787,
          1054.1421943145929,
          1073.6962380509638,
          1070.5807308229914,
          1087.559785789733,
          1043.4387813343992,
          1083.690482216654,
          1044.127575788848,
          1040.189747126709,
          1043.9444658162001,
          1053.8917901387358,
          1071.5552708526543,
          1037.2634995274823,
          1037.6592893231386,
          1039.3392475710284,
          1062.445559112873,
          1057.464808379717,
          1039.4343273223271,
          1032.7054059059415,
          1016.4291570191874,
          1024.718215578277,
          1022.8646560525139,
          1050.0107726132867,
          1041.1653790333382,
          1032.9513094152217,
          1011.9038922218275,
          1010.899155431074,
          1020.233405144391,
          1006.5898866716549,
          1018.1539653979253,
          1012.9935779560059,
          1085.6747955573162,
          1038.3393227053425,
          1019.9931797037532,
          1001.1250151027436,
          1011.5482787841161,
          1036.6677515593276,
          1020.3916290754399,
          1022.1331063303652,
          992.7807217221808,
          1025.3808260118888,
          1016.5331383325162,
          1012.2566530400917,
          1027.820122455102,
          996.3717286527002,
          991.4357960947486,
          993.6690173381456,
          994.4592012229845,
          1012.4963605814571,
          996.1973038714133,
          1005.1224311995368,
          1015.2761342576637,
          1000.3304557180701,
          975.1199951661429,
          1003.6176342425854,
          981.668930821699,
          979.8929433078739,
          987.5056406422323,
          978.7243656627348,
          994.1270988285362,
          993.0839680116545,
          969.7014012800469,
          979.966390691971,
          983.5718674217027,
          971.0951618984751,
          978.6671399349245,
          957.3753195686716,
          969.6597938575844,
          952.3710215407716,
          977.1274951348114,
          970.1584861362198,
          972.0556641848885,
          968.1701991094815,
          957.6342613965255,
          958.9448524904988,
          956.5867480669467,
          959.576334994423,
          956.6763865346777,
          967.6569099923473,
          959.0987800214151,
          949.5536263303143,
          954.6930300519632,
          949.0192090622141
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9070.66659604033,
          8560.147023471965,
          7953.4732318677,
          7254.437026992838,
          6435.058792900604,
          5625.5027295577875,
          4677.292321559492,
          4717.054823302305,
          3861.167238873806,
          3321.241074645641,
          3005.288214592315,
          2426.8738622762935,
          1971.5734596474272,
          1603.9621874050265,
          1562.1498963467125,
          1838.4096160515387,
          2075.72060736822,
          1954.211669750531,
          1929.4825496975732,
          1672.0525324584153,
          1556.8031573986898,
          1466.468458641883,
          1415.303358923631,
          1360.1662005785354,
          1272.2651353724762,
          1279.642746874295,
          1305.215678562113,
          1217.8001224325826,
          1191.512256745468,
          1204.1305898509954,
          1180.837496044403,
          1189.0957201614824,
          1171.9154104137554,
          1176.6571614273282,
          1163.142671972519,
          1160.8443614530593,
          1155.054292821492,
          1165.488409170484,
          1154.7623654019983,
          1157.7970378991236,
          1160.1585552612573,
          1147.9042075558561,
          1140.701989454809,
          1134.0498457610881,
          1132.9409320679079,
          1140.4254046360381,
          1124.281030557379,
          1131.5850941451681,
          1138.601779299029,
          1139.555136763789,
          1133.4248871773643,
          1141.9666445502446,
          1125.5518784141502,
          1124.7725178236853,
          1112.9361988386602,
          1121.2668201484014,
          1128.046762010916,
          1126.858800899918,
          1114.0021519329161,
          1120.7515787629868,
          1132.3108874088255,
          1118.6669681014387,
          1118.3065953412465,
          1115.3266191071395,
          1118.5194517953712,
          1116.5876581357625,
          1117.9577544193028,
          1121.2879775012689,
          1125.5863668144598,
          1110.351981607257,
          1116.9710970202214,
          1114.4593145302788,
          1125.6049145502916,
          1112.9221709844032,
          1132.4425105861176,
          1133.8511025884095,
          1120.685454251366,
          1107.6458575254646,
          1110.9026410806769,
          1119.5916339269208,
          1116.2196726625946,
          1117.8575859245757,
          1114.078424558148,
          1106.4870762615612,
          1111.6061288157805,
          1101.3760813025976,
          1142.7233158017486,
          1119.6029722199594,
          1124.4007957554877,
          1108.7213313697646,
          1111.8862978313928,
          1096.9899935633428,
          1114.9007761488463,
          1098.8385911769037,
          1101.7040196845783,
          1115.0550370416206,
          1110.635963710726,
          1103.3233299400708,
          1109.8486294590546,
          1100.5451413420988,
          1119.9459879622566,
          1113.2626645440255,
          1141.465841681536,
          1106.2099226788478,
          1118.3559837364257,
          1121.4436524955117,
          1122.985984237729,
          1114.3173898865566,
          1108.319001180413,
          1103.5104624807702,
          1128.076292329784,
          1097.2219463231354,
          1115.3278627703514,
          1099.1089467625593,
          1103.7768103318629,
          1111.2664704843921,
          1112.1944980533372,
          1105.5886380027525,
          1107.4800006383784,
          1093.4830985121864,
          1102.3031164964455,
          1101.1560584130066,
          1090.3819260399305,
          1101.823865169558,
          1096.9509447708574,
          1105.2940156124673,
          1105.2314359864945,
          1112.8374113025898,
          1106.9556748180034,
          1094.083070353035,
          1094.833727481491,
          1103.5276178758131,
          1112.1450255772431,
          1102.1538680011388,
          1100.7314216915152,
          1095.9733432167777,
          1094.0600207756966,
          1121.5801033370358,
          1103.3288964882913,
          1090.7106275143499,
          1093.064495789639,
          1103.752491193901,
          1086.1102915676463,
          1090.3884169397072,
          1099.4521815803118,
          1099.8173448965356,
          1086.6232018188696,
          1096.491809809019,
          1096.3300915877376,
          1100.275551345608,
          1102.251143916566,
          1114.5079314126885,
          1102.2929288758194,
          1092.7046847193328,
          1089.5949883760277,
          1116.2687495789835,
          1099.9498121874688,
          1094.8329734702806,
          1105.2084378123354,
          1116.3443265478525,
          1102.0386355109222,
          1099.0346490719621,
          1099.034070247683,
          1105.0485452361688,
          1096.178735292816,
          1089.1237898712434,
          1094.239338833702,
          1092.3231545573178,
          1093.716332116824,
          1121.2067933511896,
          1111.61572414939,
          1097.9556973374915,
          1092.2316194958455,
          1106.0346722278705,
          1088.3500798120033,
          1111.1963835219617,
          1095.2141339700017,
          1101.1431341850139,
          1114.108678535873,
          1103.101995363727,
          1089.8955477601137,
          1097.0990498146743,
          1086.843511415814,
          1108.427460567397,
          1101.6709444800622,
          1095.9151123242696,
          1086.268376905932,
          1107.3310541734681,
          1086.7401833145084,
          1112.9759580230866
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798,
          1152.263653981798
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"0ef45fa5-2d07-4b16-baf6-61f406917b72\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"0ef45fa5-2d07-4b16-baf6-61f406917b72\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '0ef45fa5-2d07-4b16-baf6-61f406917b72',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9670.91682945972, 9225.365781892295, 8692.88135597867, 8063.2195689260825, 7331.044840315289, 6491.214032069318, 5585.977350388353, 4746.474561692432, 4233.007781853914, 3932.756330864545, 3602.2881573144864, 3252.5435080427865, 2845.7959529738255, 2421.5979788171903, 1994.0238231951903, 1637.4535896357875, 1467.2673102410015, 1410.2099654658753, 1396.8757829928127, 1378.5364121031087, 1382.3299062915141, 1348.6268293657258, 1359.3389318331176, 1349.8793044532974, 1343.164037899907, 1327.551466283039, 1334.0136426975762, 1297.0916281501827, 1323.5633182651848, 1286.2231099442536, 1275.0305727998264, 1277.4404222462779, 1257.5647809059415, 1291.8855973284446, 1295.530083221258, 1254.6497144039347, 1258.4959240460744, 1271.4604310562931, 1242.6009434343746, 1248.6274426301352, 1290.7232976882472, 1234.6193962211908, 1252.9307864510226, 1224.5360512283107, 1228.9949926449394, 1247.554435477554, 1229.5037367908515, 1192.849943985221, 1207.0235386418751, 1220.8453124853133, 1194.8919210210217, 1215.5127564333281, 1186.5353589988267, 1204.585546737925, 1197.7187311032121, 1189.7975732999548, 1174.4497948817316, 1167.0261615503043, 1153.6972028152102, 1172.8286616981495, 1183.5382645268323, 1193.0288316762253, 1172.1358703772387, 1195.8703210378233, 1172.09944153138, 1153.7112963921038, 1158.3088082970803, 1173.4217993883708, 1160.5391109413772, 1140.7779437719712, 1138.4853796873824, 1154.1572644539867, 1168.798509596631, 1174.1882815241504, 1150.9944465963067, 1143.6501574503866, 1146.2981593206791, 1131.8325840300015, 1121.3986476655712, 1157.2088870516468, 1124.3968429733713, 1105.5712445130778, 1117.7204660829093, 1140.4846793313004, 1119.6368481391462, 1105.3280550182428, 1115.8759471892545, 1098.0702347588103, 1086.2333560421157, 1128.783601034565, 1128.9422272811655, 1108.3779257710673, 1097.9499736668854, 1093.3787435711756, 1116.6113656248433, 1088.6731677899647, 1098.3479822195914, 1095.324334309243, 1079.020453374085, 1061.4889643737781, 1074.0211670949939, 1067.7107833149391, 1093.640474462117, 1066.2108909189283, 1081.8214948220843, 1078.2988091058235, 1063.2207040551527, 1057.1187652838787, 1054.1421943145929, 1073.6962380509638, 1070.5807308229914, 1087.559785789733, 1043.4387813343992, 1083.690482216654, 1044.127575788848, 1040.189747126709, 1043.9444658162001, 1053.8917901387358, 1071.5552708526543, 1037.2634995274823, 1037.6592893231386, 1039.3392475710284, 1062.445559112873, 1057.464808379717, 1039.4343273223271, 1032.7054059059415, 1016.4291570191874, 1024.718215578277, 1022.8646560525139, 1050.0107726132867, 1041.1653790333382, 1032.9513094152217, 1011.9038922218275, 1010.899155431074, 1020.233405144391, 1006.5898866716549, 1018.1539653979253, 1012.9935779560059, 1085.6747955573162, 1038.3393227053425, 1019.9931797037532, 1001.1250151027436, 1011.5482787841161, 1036.6677515593276, 1020.3916290754399, 1022.1331063303652, 992.7807217221808, 1025.3808260118888, 1016.5331383325162, 1012.2566530400917, 1027.820122455102, 996.3717286527002, 991.4357960947486, 993.6690173381456, 994.4592012229845, 1012.4963605814571, 996.1973038714133, 1005.1224311995368, 1015.2761342576637, 1000.3304557180701, 975.1199951661429, 1003.6176342425854, 981.668930821699, 979.8929433078739, 987.5056406422323, 978.7243656627348, 994.1270988285362, 993.0839680116545, 969.7014012800469, 979.966390691971, 983.5718674217027, 971.0951618984751, 978.6671399349245, 957.3753195686716, 969.6597938575844, 952.3710215407716, 977.1274951348114, 970.1584861362198, 972.0556641848885, 968.1701991094815, 957.6342613965255, 958.9448524904988, 956.5867480669467, 959.576334994423, 956.6763865346777, 967.6569099923473, 959.0987800214151, 949.5536263303143, 954.6930300519632, 949.0192090622141]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9070.66659604033, 8560.147023471965, 7953.4732318677, 7254.437026992838, 6435.058792900604, 5625.5027295577875, 4677.292321559492, 4717.054823302305, 3861.167238873806, 3321.241074645641, 3005.288214592315, 2426.8738622762935, 1971.5734596474272, 1603.9621874050265, 1562.1498963467125, 1838.4096160515387, 2075.72060736822, 1954.211669750531, 1929.4825496975732, 1672.0525324584153, 1556.8031573986898, 1466.468458641883, 1415.303358923631, 1360.1662005785354, 1272.2651353724762, 1279.642746874295, 1305.215678562113, 1217.8001224325826, 1191.512256745468, 1204.1305898509954, 1180.837496044403, 1189.0957201614824, 1171.9154104137554, 1176.6571614273282, 1163.142671972519, 1160.8443614530593, 1155.054292821492, 1165.488409170484, 1154.7623654019983, 1157.7970378991236, 1160.1585552612573, 1147.9042075558561, 1140.701989454809, 1134.0498457610881, 1132.9409320679079, 1140.4254046360381, 1124.281030557379, 1131.5850941451681, 1138.601779299029, 1139.555136763789, 1133.4248871773643, 1141.9666445502446, 1125.5518784141502, 1124.7725178236853, 1112.9361988386602, 1121.2668201484014, 1128.046762010916, 1126.858800899918, 1114.0021519329161, 1120.7515787629868, 1132.3108874088255, 1118.6669681014387, 1118.3065953412465, 1115.3266191071395, 1118.5194517953712, 1116.5876581357625, 1117.9577544193028, 1121.2879775012689, 1125.5863668144598, 1110.351981607257, 1116.9710970202214, 1114.4593145302788, 1125.6049145502916, 1112.9221709844032, 1132.4425105861176, 1133.8511025884095, 1120.685454251366, 1107.6458575254646, 1110.9026410806769, 1119.5916339269208, 1116.2196726625946, 1117.8575859245757, 1114.078424558148, 1106.4870762615612, 1111.6061288157805, 1101.3760813025976, 1142.7233158017486, 1119.6029722199594, 1124.4007957554877, 1108.7213313697646, 1111.8862978313928, 1096.9899935633428, 1114.9007761488463, 1098.8385911769037, 1101.7040196845783, 1115.0550370416206, 1110.635963710726, 1103.3233299400708, 1109.8486294590546, 1100.5451413420988, 1119.9459879622566, 1113.2626645440255, 1141.465841681536, 1106.2099226788478, 1118.3559837364257, 1121.4436524955117, 1122.985984237729, 1114.3173898865566, 1108.319001180413, 1103.5104624807702, 1128.076292329784, 1097.2219463231354, 1115.3278627703514, 1099.1089467625593, 1103.7768103318629, 1111.2664704843921, 1112.1944980533372, 1105.5886380027525, 1107.4800006383784, 1093.4830985121864, 1102.3031164964455, 1101.1560584130066, 1090.3819260399305, 1101.823865169558, 1096.9509447708574, 1105.2940156124673, 1105.2314359864945, 1112.8374113025898, 1106.9556748180034, 1094.083070353035, 1094.833727481491, 1103.5276178758131, 1112.1450255772431, 1102.1538680011388, 1100.7314216915152, 1095.9733432167777, 1094.0600207756966, 1121.5801033370358, 1103.3288964882913, 1090.7106275143499, 1093.064495789639, 1103.752491193901, 1086.1102915676463, 1090.3884169397072, 1099.4521815803118, 1099.8173448965356, 1086.6232018188696, 1096.491809809019, 1096.3300915877376, 1100.275551345608, 1102.251143916566, 1114.5079314126885, 1102.2929288758194, 1092.7046847193328, 1089.5949883760277, 1116.2687495789835, 1099.9498121874688, 1094.8329734702806, 1105.2084378123354, 1116.3443265478525, 1102.0386355109222, 1099.0346490719621, 1099.034070247683, 1105.0485452361688, 1096.178735292816, 1089.1237898712434, 1094.239338833702, 1092.3231545573178, 1093.716332116824, 1121.2067933511896, 1111.61572414939, 1097.9556973374915, 1092.2316194958455, 1106.0346722278705, 1088.3500798120033, 1111.1963835219617, 1095.2141339700017, 1101.1431341850139, 1114.108678535873, 1103.101995363727, 1089.8955477601137, 1097.0990498146743, 1086.843511415814, 1108.427460567397, 1101.6709444800622, 1095.9151123242696, 1086.268376905932, 1107.3310541734681, 1086.7401833145084, 1112.9759580230866]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798, 1152.263653981798]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0ef45fa5-2d07-4b16-baf6-61f406917b72');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.61% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1113   : Mean absolute error \n",
      "\n",
      "9.68% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"LeakyRELU\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeakyRELU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 2s 110us/sample - loss: 11193.0127 - val_loss: 11115.5698\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11179.0485 - val_loss: 11076.8315\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11154.1230 - val_loss: 11051.6681\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11116.2471 - val_loss: 11000.9666\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11061.6192 - val_loss: 10915.5305\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10985.3352 - val_loss: 10820.2911\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10882.3317 - val_loss: 10665.9907\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10747.6648 - val_loss: 10502.1500\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10576.5834 - val_loss: 10265.9487\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10363.0859 - val_loss: 10003.9948\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10102.1534 - val_loss: 9658.3315\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 9784.3850 - val_loss: 9266.2112\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 9405.5885 - val_loss: 8739.2755\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 8959.3076 - val_loss: 8248.1958\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 8441.7251 - val_loss: 7503.9709\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 7840.8184 - val_loss: 6831.4969\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 7174.1609 - val_loss: 6154.5413\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 6451.6672 - val_loss: 5388.7557\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 5749.6202 - val_loss: 4891.2464\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 5044.8129 - val_loss: 4459.3078\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 4361.6450 - val_loss: 4348.1145\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 3777.2383 - val_loss: 4449.0062\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 3312.6322 - val_loss: 3639.3390\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 2877.2616 - val_loss: 2688.2093\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 2416.4813 - val_loss: 2038.4727\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1986.3079 - val_loss: 1721.5724\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1668.9848 - val_loss: 1589.1743\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1495.8985 - val_loss: 1591.8229\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1447.8081 - val_loss: 1613.3195\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1427.1262 - val_loss: 1534.8237\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1435.0664 - val_loss: 1439.4011\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1396.9153 - val_loss: 1445.5575\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1403.8875 - val_loss: 1334.9803\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1398.8153 - val_loss: 1303.4498\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1389.2562 - val_loss: 1357.4229\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1390.3592 - val_loss: 1286.6119\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1380.5924 - val_loss: 1274.9813\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1362.3642 - val_loss: 1300.2203\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1359.1466 - val_loss: 1263.5168\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1349.8973 - val_loss: 1241.8430\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1351.6418 - val_loss: 1257.8238\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1351.1027 - val_loss: 1215.2626\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1336.1296 - val_loss: 1224.3670\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1346.4563 - val_loss: 1223.0458\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1345.4095 - val_loss: 1224.5910\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1333.0325 - val_loss: 1221.8127\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1336.7545 - val_loss: 1217.9570\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1338.8927 - val_loss: 1200.8851\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1327.6497 - val_loss: 1203.5744\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1333.3092 - val_loss: 1200.2538\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1338.5384 - val_loss: 1198.0193\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1313.5887 - val_loss: 1195.6059\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1313.6517 - val_loss: 1201.7802\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1317.9463 - val_loss: 1199.4067\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1312.9291 - val_loss: 1196.9002\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1317.5876 - val_loss: 1198.4332\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1318.0759 - val_loss: 1190.4016\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1298.8202 - val_loss: 1193.6159\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1302.7096 - val_loss: 1188.1328\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1318.6071 - val_loss: 1193.7038\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1294.6820 - val_loss: 1178.5906\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1309.5238 - val_loss: 1217.0844\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1286.5864 - val_loss: 1191.1304\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1309.6752 - val_loss: 1184.7909\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1309.6989 - val_loss: 1181.1098\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1309.4697 - val_loss: 1175.2324\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1296.7758 - val_loss: 1179.8429\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1288.7875 - val_loss: 1187.8007\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1286.0419 - val_loss: 1177.8099\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1288.1129 - val_loss: 1183.4331\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1296.2924 - val_loss: 1174.9230\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1300.4500 - val_loss: 1192.4060\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1285.7784 - val_loss: 1166.4650\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1282.1263 - val_loss: 1171.2508\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1281.6993 - val_loss: 1163.7345\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1280.6535 - val_loss: 1165.4358\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1277.4028 - val_loss: 1170.8390\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1282.9564 - val_loss: 1163.2927\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1270.4187 - val_loss: 1175.0886\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1272.1770 - val_loss: 1187.2075\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1274.0681 - val_loss: 1164.3209\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1282.4924 - val_loss: 1177.9821\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1277.8961 - val_loss: 1209.2468\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1273.8782 - val_loss: 1169.2719\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1276.6225 - val_loss: 1167.1469\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1270.9371 - val_loss: 1176.3539\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1275.7856 - val_loss: 1168.2104\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1262.6230 - val_loss: 1183.2047\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1270.9103 - val_loss: 1170.3526\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1277.8836 - val_loss: 1181.9092\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1271.7702 - val_loss: 1182.3740\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1258.5394 - val_loss: 1166.2503\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1255.9743 - val_loss: 1174.5637\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1273.5573 - val_loss: 1162.3259\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1267.1626 - val_loss: 1156.3370\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1263.8737 - val_loss: 1177.3253\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1256.9434 - val_loss: 1203.2763\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1265.6392 - val_loss: 1161.3102\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1246.4298 - val_loss: 1150.2476\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1265.8834 - val_loss: 1157.0463\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1264.8889 - val_loss: 1147.3323\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1260.9873 - val_loss: 1152.6014\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1252.5780 - val_loss: 1161.1217\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1261.8223 - val_loss: 1169.3143\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1262.4419 - val_loss: 1157.1866\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1251.2513 - val_loss: 1149.8954\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1249.5574 - val_loss: 1200.0160\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1252.2357 - val_loss: 1149.3849\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1243.0821 - val_loss: 1166.4025\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1255.3765 - val_loss: 1151.0001\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1234.5428 - val_loss: 1152.3398\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1243.0873 - val_loss: 1149.7961\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1243.8562 - val_loss: 1149.8358\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1240.5155 - val_loss: 1154.6885\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1244.5361 - val_loss: 1160.4118\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1246.4961 - val_loss: 1171.9840\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1232.5927 - val_loss: 1144.9969\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1245.2029 - val_loss: 1153.6019\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1233.7679 - val_loss: 1179.1971\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1242.1906 - val_loss: 1148.8968\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1243.6348 - val_loss: 1147.9523\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1229.7207 - val_loss: 1183.1639\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1227.5234 - val_loss: 1149.8023\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1233.9349 - val_loss: 1155.8532\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1246.3245 - val_loss: 1155.0094\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1232.5814 - val_loss: 1141.5095\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1223.1269 - val_loss: 1142.0337\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1232.3703 - val_loss: 1143.8088\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1231.2764 - val_loss: 1163.9454\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1222.5514 - val_loss: 1147.8177\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1222.3703 - val_loss: 1159.9945\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1230.1045 - val_loss: 1150.4587\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1230.9123 - val_loss: 1145.0273\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1217.1281 - val_loss: 1147.0390\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1227.1538 - val_loss: 1147.0770\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1226.3074 - val_loss: 1140.3538\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1221.6572 - val_loss: 1152.5358\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1221.3390 - val_loss: 1144.5184\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1221.7547 - val_loss: 1146.5846\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1228.3887 - val_loss: 1146.8922\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1224.9608 - val_loss: 1145.3150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1219.2089 - val_loss: 1149.0181\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1220.1687 - val_loss: 1149.9302\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1223.7032 - val_loss: 1140.0915\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1214.2329 - val_loss: 1157.8368\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1218.4128 - val_loss: 1159.5815\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1229.2537 - val_loss: 1172.2902\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1224.7704 - val_loss: 1138.8615\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1229.3350 - val_loss: 1134.8769\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1218.8611 - val_loss: 1131.8578\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1215.5766 - val_loss: 1137.0423\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1209.3784 - val_loss: 1138.5028\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1214.1593 - val_loss: 1132.2272\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1219.7569 - val_loss: 1141.3089\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1210.9877 - val_loss: 1135.4911\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1218.8067 - val_loss: 1172.7900\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1212.1526 - val_loss: 1139.1659\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1208.7392 - val_loss: 1140.9986\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1209.4024 - val_loss: 1144.0049\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1204.8862 - val_loss: 1140.3690\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1206.7398 - val_loss: 1156.2106\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1209.4336 - val_loss: 1136.6168\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1210.5894 - val_loss: 1147.3033\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1209.8626 - val_loss: 1135.7416\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1203.6980 - val_loss: 1133.6278\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1210.4523 - val_loss: 1135.8588\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1206.3752 - val_loss: 1164.1901\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1205.9623 - val_loss: 1135.8584\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1212.9014 - val_loss: 1137.4836\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1194.5646 - val_loss: 1132.1966\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1197.3548 - val_loss: 1134.0263\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1204.4939 - val_loss: 1134.4201\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1204.8709 - val_loss: 1146.7698\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1204.8141 - val_loss: 1133.6895\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1197.6428 - val_loss: 1131.3683\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1204.9118 - val_loss: 1125.8917\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1199.8141 - val_loss: 1133.3712\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1206.2216 - val_loss: 1143.6774\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1201.7957 - val_loss: 1134.4569\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1202.8887 - val_loss: 1149.3621\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1202.4178 - val_loss: 1147.6822\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1194.1507 - val_loss: 1145.6002\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1199.9839 - val_loss: 1133.6050\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1199.8868 - val_loss: 1123.5974\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1188.8757 - val_loss: 1152.8999\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1200.1408 - val_loss: 1135.9637\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1195.8701 - val_loss: 1125.1497\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1192.9706 - val_loss: 1137.0593\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1188.7512 - val_loss: 1168.7895\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1197.6435 - val_loss: 1125.5105\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1195.1261 - val_loss: 1127.0837\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1196.9265 - val_loss: 1139.7257\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1192.6179 - val_loss: 1143.9353\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1187.5441 - val_loss: 1128.2752\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1190.0803 - val_loss: 1128.9747\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1190.7101 - val_loss: 1126.5218\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1181.8297 - val_loss: 1122.4192\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1179.2753 - val_loss: 1123.9250\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1188.5558 - val_loss: 1139.2966\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1176.0420 - val_loss: 1127.9034\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1178.3537 - val_loss: 1138.0135\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1183.8243 - val_loss: 1133.4170\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1180.8182 - val_loss: 1128.1007\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1194.1915 - val_loss: 1136.8267\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1175.3219 - val_loss: 1125.6981\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1177.7530 - val_loss: 1118.8234\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1178.5410 - val_loss: 1121.6231\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1188.0979 - val_loss: 1124.4310\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1173.2316 - val_loss: 1124.5119\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1177.2863 - val_loss: 1122.9514\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1184.6905 - val_loss: 1126.2697\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1184.2732 - val_loss: 1126.5663\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1183.5091 - val_loss: 1135.6423\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1184.5412 - val_loss: 1140.0388\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1176.1644 - val_loss: 1122.0773\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1166.1989 - val_loss: 1125.6962\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1178.6928 - val_loss: 1127.5971\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1178.2132 - val_loss: 1130.0447\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1174.8739 - val_loss: 1120.4430\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1175.0411 - val_loss: 1125.2808\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1176.5485 - val_loss: 1120.6429\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1165.9273 - val_loss: 1130.8433\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1162.8211 - val_loss: 1138.3870\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1168.9235 - val_loss: 1122.4943\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1171.0662 - val_loss: 1130.9666\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1173.3699 - val_loss: 1128.6682\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1185.0422 - val_loss: 1142.0518\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1168.0669 - val_loss: 1138.6728\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1178.6424 - val_loss: 1133.6037\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1174.9513 - val_loss: 1121.3295\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1174.0123 - val_loss: 1127.9269\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1164.4598 - val_loss: 1130.7697\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1169.3845 - val_loss: 1164.3945\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1172.5097 - val_loss: 1117.8690\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1161.8059 - val_loss: 1133.9293\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1156.5657 - val_loss: 1116.3800\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1167.4824 - val_loss: 1128.0791\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1162.7465 - val_loss: 1122.3441\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1163.8068 - val_loss: 1117.6926\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1159.7668 - val_loss: 1115.4451\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1177.9973 - val_loss: 1137.3974\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1167.3557 - val_loss: 1125.8234\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1169.2597 - val_loss: 1126.6925\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1173.5506 - val_loss: 1115.2065\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1164.1470 - val_loss: 1151.7796\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1172.5866 - val_loss: 1117.9942\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1162.9575 - val_loss: 1122.1495\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1157.3320 - val_loss: 1115.0214\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1154.7370 - val_loss: 1134.0633\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1165.2892 - val_loss: 1144.9868\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1162.3057 - val_loss: 1124.6134\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1162.4144 - val_loss: 1109.2704\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1154.0225 - val_loss: 1122.4681\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.3747 - val_loss: 1126.1001\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1162.9709 - val_loss: 1119.1880\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1159.2868 - val_loss: 1131.3158\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1169.1070 - val_loss: 1127.5327\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1165.0298 - val_loss: 1122.2962\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1167.2245 - val_loss: 1124.8761\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1166.4691 - val_loss: 1113.6134\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.9759 - val_loss: 1116.8609\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1152.8933 - val_loss: 1108.0553\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1146.8901 - val_loss: 1128.2648\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1155.4414 - val_loss: 1113.0894\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1151.6257 - val_loss: 1124.5051\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1160.5771 - val_loss: 1117.5104\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1152.2453 - val_loss: 1113.9848\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.2169 - val_loss: 1136.3806\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1165.2832 - val_loss: 1114.3720\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1157.1394 - val_loss: 1111.5863\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.0948 - val_loss: 1110.4843\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1156.5939 - val_loss: 1111.0853\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1155.2870 - val_loss: 1116.0686\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1155.9952 - val_loss: 1132.4102\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1168.7166 - val_loss: 1126.1961\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1151.1318 - val_loss: 1118.4088\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1157.2324 - val_loss: 1112.5199\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1148.3297 - val_loss: 1121.7647\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1151.3684 - val_loss: 1115.2924\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1157.1111 - val_loss: 1124.1129\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1147.7727 - val_loss: 1113.8745\n",
      "Epoch 282/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1157.9877 - val_loss: 1121.0649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1151.9712 - val_loss: 1115.6588\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1152.5480 - val_loss: 1124.4890\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1145.2065 - val_loss: 1113.9451\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1147.5380 - val_loss: 1117.9644\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1141.9522 - val_loss: 1116.2165\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.5279 - val_loss: 1112.7297\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1144.3831 - val_loss: 1111.6944\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1134.1284 - val_loss: 1105.6419\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1156.3850 - val_loss: 1111.7582\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.7767 - val_loss: 1148.4879\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.4976 - val_loss: 1110.6545\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1142.1258 - val_loss: 1106.0163\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1139.2616 - val_loss: 1108.2444\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1143.2820 - val_loss: 1107.8832\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1150.2599 - val_loss: 1133.8266\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1141.5568 - val_loss: 1123.1941\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1137.3040 - val_loss: 1124.0910\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1144.6874 - val_loss: 1113.1894\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10102.153441526092,
          9784.385044866653,
          9405.588500413576,
          8959.307629720084,
          8441.725110972122,
          7840.818430751987,
          7174.1608960641415,
          6451.667201422918,
          5749.620216273248,
          5044.812902707195,
          4361.6450197270715,
          3777.238265094715,
          3312.632162812766,
          2877.2615719816836,
          2416.481277297536,
          1986.307850802712,
          1668.9847916105311,
          1495.898487801683,
          1447.808101405451,
          1427.1262290744999,
          1435.0664479355307,
          1396.9153358301896,
          1403.887508498659,
          1398.8152953538574,
          1389.256230775211,
          1390.3591539908057,
          1380.5924330505409,
          1362.3642049406583,
          1359.14658610434,
          1349.8972883207277,
          1351.6418151549497,
          1351.1026733961426,
          1336.1296487322115,
          1346.4563216413455,
          1345.409452418084,
          1333.0325248722852,
          1336.7545479085466,
          1338.8926786304548,
          1327.649679498728,
          1333.3092413566287,
          1338.5384462493107,
          1313.588736549991,
          1313.651654825006,
          1317.9462522480324,
          1312.929064517942,
          1317.5876082502004,
          1318.0758558923121,
          1298.820248221548,
          1302.7096022901733,
          1318.6071114732288,
          1294.6820041923454,
          1309.5237586230057,
          1286.5864284737975,
          1309.675242348857,
          1309.6989150551199,
          1309.4697130752847,
          1296.7758159985494,
          1288.7874514753985,
          1286.041858318934,
          1288.1128980515355,
          1296.2924424303972,
          1300.4499637044598,
          1285.7783631729606,
          1282.1263360665464,
          1281.6992797729174,
          1280.6535367443635,
          1277.4028122532648,
          1282.9563697155572,
          1270.4187056023395,
          1272.1770070444875,
          1274.0680605756609,
          1282.4923663830648,
          1277.896128885297,
          1273.8782057715293,
          1276.6225478969932,
          1270.937073769731,
          1275.785565419309,
          1262.6229892300057,
          1270.9103492567003,
          1277.8836077806202,
          1271.7701909975704,
          1258.5393882003366,
          1255.9742839928736,
          1273.5573053170665,
          1267.1626410307424,
          1263.873658842091,
          1256.9434354027503,
          1265.6391865432154,
          1246.429806706421,
          1265.8834252993329,
          1264.8888712459034,
          1260.9872794265088,
          1252.5779718919582,
          1261.822255270931,
          1262.4418615842596,
          1251.2513347937058,
          1249.557437912792,
          1252.2356650974173,
          1243.082116163157,
          1255.3765251078585,
          1234.5427715576122,
          1243.0873387017182,
          1243.8561672702542,
          1240.5154780603398,
          1244.5361120798843,
          1246.496114458138,
          1232.592689356585,
          1245.2028522449384,
          1233.7679099202849,
          1242.1905697978807,
          1243.6347704471077,
          1229.7207383973723,
          1227.5234458713749,
          1233.9348989501611,
          1246.324524770262,
          1232.5814031521018,
          1223.1268662781042,
          1232.370255021454,
          1231.2763605050866,
          1222.551440703838,
          1222.3703192264732,
          1230.1044766196799,
          1230.9122675744045,
          1217.1281302578109,
          1227.1538203185705,
          1226.307384894084,
          1221.6571999307575,
          1221.3389664201143,
          1221.754704859588,
          1228.388737475248,
          1224.9607600366421,
          1219.2088981890024,
          1220.168738206642,
          1223.7031918975665,
          1214.2328870737747,
          1218.4127789381298,
          1229.2536747643483,
          1224.770391592359,
          1229.335046438122,
          1218.861056569346,
          1215.576550471656,
          1209.378411433203,
          1214.1592952222654,
          1219.7569401635658,
          1210.9877476849965,
          1218.8066924198229,
          1212.1525593496215,
          1208.7391513834407,
          1209.402387907779,
          1204.8862102746439,
          1206.7398310313847,
          1209.4335941171655,
          1210.5894183519729,
          1209.8625914438087,
          1203.6980318750627,
          1210.4523421785314,
          1206.375212907074,
          1205.9623170389498,
          1212.9013563193992,
          1194.5645900043237,
          1197.3548479954131,
          1204.4939166540034,
          1204.870934938653,
          1204.814129456313,
          1197.6427949729843,
          1204.9117766495624,
          1199.8140557049892,
          1206.2215928895598,
          1201.7956961782274,
          1202.8887162041228,
          1202.4177788030129,
          1194.1507139509993,
          1199.9839278657714,
          1199.886790322808,
          1188.8756832706442,
          1200.1407504482356,
          1195.8701094280677,
          1192.9705538844355,
          1188.7512490972622,
          1197.6434792716923,
          1195.126091044724,
          1196.926542996165,
          1192.6179180126142,
          1187.544143728773,
          1190.080342728986,
          1190.710075101514,
          1181.8297345559392,
          1179.2753498940997,
          1188.5557525494019,
          1176.0420352927376,
          1178.353706159452,
          1183.8243418238983,
          1180.8182121905627,
          1194.1914769905661,
          1175.3218887907387,
          1177.7530395923934,
          1178.5409807687488,
          1188.097905873631,
          1173.2316076486366,
          1177.2862722864604,
          1184.690468949738,
          1184.2731658219575,
          1183.5091155950784,
          1184.5411686840864,
          1176.1643951030492,
          1166.1988892556194,
          1178.6928242679012,
          1178.2131573585464,
          1174.873927093255,
          1175.041099779348,
          1176.548493686123,
          1165.9272682705503,
          1162.8210576258036,
          1168.923492609104,
          1171.0662128516565,
          1173.3698662420732,
          1185.0422045560645,
          1168.0668603848521,
          1178.6423926819105,
          1174.9513494607463,
          1174.01226567983,
          1164.4598287089402,
          1169.3844603635469,
          1172.5096722425571,
          1161.8059221554165,
          1156.5657078521342,
          1167.482442681049,
          1162.7465085736587,
          1163.8068425660624,
          1159.7668200504907,
          1177.9972813103132,
          1167.3557166846692,
          1169.2596673959717,
          1173.5506090003933,
          1164.1469760831287,
          1172.5865526491925,
          1162.9574604175937,
          1157.3319777906934,
          1154.7369544115481,
          1165.2892261579325,
          1162.3056940966435,
          1162.4144437892035,
          1154.0224869817775,
          1153.3746553294425,
          1162.97086318471,
          1159.286791336185,
          1169.106950409933,
          1165.0297744839709,
          1167.2245460267006,
          1166.4691081822505,
          1150.9759462003553,
          1152.893335391745,
          1146.8901294733494,
          1155.4413989311663,
          1151.6257192039145,
          1160.5770891524996,
          1152.2452593050525,
          1158.2169367650813,
          1165.2832258647875,
          1157.1394338414643,
          1150.0947684928078,
          1156.5938901348584,
          1155.2869882103626,
          1155.9951808540095,
          1168.716631846316,
          1151.1318195619156,
          1157.2324303932412,
          1148.3296611473033,
          1151.3684351286,
          1157.1110633251924,
          1147.772683464312,
          1157.987694470467,
          1151.9711850420467,
          1152.5480445447224,
          1145.2064830228517,
          1147.5379921878916,
          1141.9522433620382,
          1150.5279032124147,
          1144.3831433896098,
          1134.1283848504158,
          1156.3850136086246,
          1158.7766919576836,
          1153.4976320268636,
          1142.1258426205009,
          1139.261600351343,
          1143.2819566958076,
          1150.2599141067558,
          1141.5567810780685,
          1137.3039714781871,
          1144.6874473484575
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9658.331511343556,
          9266.2111673946,
          8739.275469345297,
          8248.19583788945,
          7503.970898554993,
          6831.496901807982,
          6154.5413200297335,
          5388.755724355982,
          4891.246369760596,
          4459.307796364297,
          4348.114520703047,
          4449.006248570502,
          3639.338959958,
          2688.209250447648,
          2038.4727276269864,
          1721.572370585397,
          1589.1743002020096,
          1591.8228810189696,
          1613.319480502823,
          1534.8237312765143,
          1439.4011154587865,
          1445.5575270850695,
          1334.9803482217828,
          1303.4498300806706,
          1357.4228587099897,
          1286.6119230556087,
          1274.9812704927342,
          1300.2202563334592,
          1263.5168407586286,
          1241.8429837988926,
          1257.8238195675945,
          1215.2626268973158,
          1224.3670298146274,
          1223.0457523056039,
          1224.5909640211032,
          1221.8126634131554,
          1217.9569625410834,
          1200.8851218000787,
          1203.5744038014464,
          1200.2537647199124,
          1198.019271957676,
          1195.6058776255766,
          1201.7802483478529,
          1199.40673678811,
          1196.900172822385,
          1198.43324878571,
          1190.4015681249764,
          1193.6159293704648,
          1188.1328388135323,
          1193.7037748096222,
          1178.5905640554113,
          1217.08440174434,
          1191.1303947636902,
          1184.7908551246915,
          1181.109789554404,
          1175.2323895399522,
          1179.8429200099947,
          1187.8007138873572,
          1177.809906660638,
          1183.4331227255318,
          1174.9230024430708,
          1192.4059835846638,
          1166.4649515253332,
          1171.250811631727,
          1163.7344582976286,
          1165.4357788991613,
          1170.8389991957606,
          1163.2927027018295,
          1175.0885807494017,
          1187.2075465546359,
          1164.320894383992,
          1177.9820836176011,
          1209.246768355537,
          1169.271922452478,
          1167.1469073252565,
          1176.3538744681484,
          1168.2103659945546,
          1183.2047325242897,
          1170.3526374383946,
          1181.9091895275371,
          1182.373967677289,
          1166.2503411702457,
          1174.563698403936,
          1162.3258932452893,
          1156.3370062944907,
          1177.325251904953,
          1203.2762538802058,
          1161.3102068552064,
          1150.2475903168552,
          1157.0463195029733,
          1147.3322684634347,
          1152.6014185710974,
          1161.1217352372028,
          1169.3143450804123,
          1157.1866017906323,
          1149.8953812872453,
          1200.015962596501,
          1149.3848911221912,
          1166.4025208951477,
          1151.0000750975976,
          1152.3398132507803,
          1149.7960851044747,
          1149.835830385565,
          1154.6885328856981,
          1160.4117544972396,
          1171.9839865633069,
          1144.9968764980356,
          1153.6019432996493,
          1179.197087916865,
          1148.8967784990978,
          1147.9523110918467,
          1183.1638964677302,
          1149.802275834161,
          1155.8531785180533,
          1155.0093766497973,
          1141.5094682943231,
          1142.0337209997947,
          1143.8088225920596,
          1163.9453989552524,
          1147.817674415394,
          1159.9945409089023,
          1150.4586733368085,
          1145.0272601586391,
          1147.039026395386,
          1147.0769799574598,
          1140.3538209109308,
          1152.5358145778634,
          1144.518419986692,
          1146.5845988632946,
          1146.8922346048946,
          1145.3150189026626,
          1149.0180996959282,
          1149.930157888048,
          1140.0915292602563,
          1157.836769007623,
          1159.5815166796954,
          1172.290195377121,
          1138.8615212051334,
          1134.8768526440228,
          1131.8577968734335,
          1137.0422641103196,
          1138.502823821428,
          1132.2271509684947,
          1141.3088689038766,
          1135.491106148447,
          1172.7900202391452,
          1139.1658843510706,
          1140.998621391203,
          1144.004940139284,
          1140.369027538005,
          1156.210592388844,
          1136.6167620774952,
          1147.3032855884283,
          1135.7416455146663,
          1133.6278490089858,
          1135.8587825416396,
          1164.190111820029,
          1135.8584250692425,
          1137.483625761942,
          1132.1966327294717,
          1134.0262887119793,
          1134.4201454053978,
          1146.7697526753152,
          1133.6895027902624,
          1131.3682674318463,
          1125.891697735401,
          1133.371184048252,
          1143.677404244199,
          1134.4569096692417,
          1149.3621390223957,
          1147.682165622807,
          1145.6001948816925,
          1133.6050033064482,
          1123.5973856489763,
          1152.89985776985,
          1135.9636780972326,
          1125.149663245532,
          1137.0593057819472,
          1168.7895219709917,
          1125.5105491318443,
          1127.0836830544572,
          1139.7257138090285,
          1143.9353148508387,
          1128.2752129658204,
          1128.974676884509,
          1126.5218275034622,
          1122.4191545968738,
          1123.9250481427493,
          1139.2965555414783,
          1127.9034220907954,
          1138.0134830050317,
          1133.4169659718784,
          1128.1007280795302,
          1136.8266575616133,
          1125.6981134102964,
          1118.823398257344,
          1121.6230890011295,
          1124.4309632534826,
          1124.5119473472826,
          1122.9514231631147,
          1126.2696911873213,
          1126.566283053713,
          1135.642284368068,
          1140.0388046518606,
          1122.077279246353,
          1125.6961745068427,
          1127.5971039595527,
          1130.0447124927155,
          1120.4429770087775,
          1125.28077547522,
          1120.6428688466967,
          1130.8432606906865,
          1138.3870210741286,
          1122.4942676398089,
          1130.9666098898151,
          1128.6682374417626,
          1142.0518098031444,
          1138.6727776887876,
          1133.6037093660314,
          1121.3295341569722,
          1127.9268700917307,
          1130.7696948589771,
          1164.3944579637525,
          1117.8690350281827,
          1133.9292513073053,
          1116.37996929223,
          1128.079071405968,
          1122.3441005452066,
          1117.6926405137733,
          1115.4450842943606,
          1137.3974277212549,
          1125.823391305676,
          1126.6924960463612,
          1115.2065290654139,
          1151.7795525201461,
          1117.9942041935988,
          1122.1494947410142,
          1115.0214037943967,
          1134.063324702821,
          1144.9867751123625,
          1124.6133844969577,
          1109.270447744213,
          1122.4680659950245,
          1126.100065443591,
          1119.187997068746,
          1131.315796853068,
          1127.5326886281284,
          1122.2961947890687,
          1124.8761091827032,
          1113.613392745944,
          1116.8609416220454,
          1108.055278269399,
          1128.2647911914728,
          1113.0893751733022,
          1124.5051158157694,
          1117.5104086298204,
          1113.9848457796913,
          1136.3806411073087,
          1114.37201017078,
          1111.5862729179853,
          1110.4843061931726,
          1111.0853023280451,
          1116.068563347712,
          1132.4101738739473,
          1126.196110203913,
          1118.4088029756272,
          1112.5199375309398,
          1121.764671054899,
          1115.2924138404383,
          1124.1128620203544,
          1113.874548753517,
          1121.0649084837373,
          1115.6587851313807,
          1124.4889539217982,
          1113.9451309243877,
          1117.9643593117496,
          1116.2165208888814,
          1112.7297266104763,
          1111.6943907186028,
          1105.6418863605923,
          1111.758246024625,
          1148.4879002790067,
          1110.6545109814815,
          1106.0163094210964,
          1108.2444020468845,
          1107.8831556284622,
          1133.826577225787,
          1123.1941445218506,
          1124.0910097699332,
          1113.1894470300515
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"6ac1bccc-3857-4af5-9356-6cbcf80a0dab\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"6ac1bccc-3857-4af5-9356-6cbcf80a0dab\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '6ac1bccc-3857-4af5-9356-6cbcf80a0dab',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10102.153441526092, 9784.385044866653, 9405.588500413576, 8959.307629720084, 8441.725110972122, 7840.818430751987, 7174.1608960641415, 6451.667201422918, 5749.620216273248, 5044.812902707195, 4361.6450197270715, 3777.238265094715, 3312.632162812766, 2877.2615719816836, 2416.481277297536, 1986.307850802712, 1668.9847916105311, 1495.898487801683, 1447.808101405451, 1427.1262290744999, 1435.0664479355307, 1396.9153358301896, 1403.887508498659, 1398.8152953538574, 1389.256230775211, 1390.3591539908057, 1380.5924330505409, 1362.3642049406583, 1359.14658610434, 1349.8972883207277, 1351.6418151549497, 1351.1026733961426, 1336.1296487322115, 1346.4563216413455, 1345.409452418084, 1333.0325248722852, 1336.7545479085466, 1338.8926786304548, 1327.649679498728, 1333.3092413566287, 1338.5384462493107, 1313.588736549991, 1313.651654825006, 1317.9462522480324, 1312.929064517942, 1317.5876082502004, 1318.0758558923121, 1298.820248221548, 1302.7096022901733, 1318.6071114732288, 1294.6820041923454, 1309.5237586230057, 1286.5864284737975, 1309.675242348857, 1309.6989150551199, 1309.4697130752847, 1296.7758159985494, 1288.7874514753985, 1286.041858318934, 1288.1128980515355, 1296.2924424303972, 1300.4499637044598, 1285.7783631729606, 1282.1263360665464, 1281.6992797729174, 1280.6535367443635, 1277.4028122532648, 1282.9563697155572, 1270.4187056023395, 1272.1770070444875, 1274.0680605756609, 1282.4923663830648, 1277.896128885297, 1273.8782057715293, 1276.6225478969932, 1270.937073769731, 1275.785565419309, 1262.6229892300057, 1270.9103492567003, 1277.8836077806202, 1271.7701909975704, 1258.5393882003366, 1255.9742839928736, 1273.5573053170665, 1267.1626410307424, 1263.873658842091, 1256.9434354027503, 1265.6391865432154, 1246.429806706421, 1265.8834252993329, 1264.8888712459034, 1260.9872794265088, 1252.5779718919582, 1261.822255270931, 1262.4418615842596, 1251.2513347937058, 1249.557437912792, 1252.2356650974173, 1243.082116163157, 1255.3765251078585, 1234.5427715576122, 1243.0873387017182, 1243.8561672702542, 1240.5154780603398, 1244.5361120798843, 1246.496114458138, 1232.592689356585, 1245.2028522449384, 1233.7679099202849, 1242.1905697978807, 1243.6347704471077, 1229.7207383973723, 1227.5234458713749, 1233.9348989501611, 1246.324524770262, 1232.5814031521018, 1223.1268662781042, 1232.370255021454, 1231.2763605050866, 1222.551440703838, 1222.3703192264732, 1230.1044766196799, 1230.9122675744045, 1217.1281302578109, 1227.1538203185705, 1226.307384894084, 1221.6571999307575, 1221.3389664201143, 1221.754704859588, 1228.388737475248, 1224.9607600366421, 1219.2088981890024, 1220.168738206642, 1223.7031918975665, 1214.2328870737747, 1218.4127789381298, 1229.2536747643483, 1224.770391592359, 1229.335046438122, 1218.861056569346, 1215.576550471656, 1209.378411433203, 1214.1592952222654, 1219.7569401635658, 1210.9877476849965, 1218.8066924198229, 1212.1525593496215, 1208.7391513834407, 1209.402387907779, 1204.8862102746439, 1206.7398310313847, 1209.4335941171655, 1210.5894183519729, 1209.8625914438087, 1203.6980318750627, 1210.4523421785314, 1206.375212907074, 1205.9623170389498, 1212.9013563193992, 1194.5645900043237, 1197.3548479954131, 1204.4939166540034, 1204.870934938653, 1204.814129456313, 1197.6427949729843, 1204.9117766495624, 1199.8140557049892, 1206.2215928895598, 1201.7956961782274, 1202.8887162041228, 1202.4177788030129, 1194.1507139509993, 1199.9839278657714, 1199.886790322808, 1188.8756832706442, 1200.1407504482356, 1195.8701094280677, 1192.9705538844355, 1188.7512490972622, 1197.6434792716923, 1195.126091044724, 1196.926542996165, 1192.6179180126142, 1187.544143728773, 1190.080342728986, 1190.710075101514, 1181.8297345559392, 1179.2753498940997, 1188.5557525494019, 1176.0420352927376, 1178.353706159452, 1183.8243418238983, 1180.8182121905627, 1194.1914769905661, 1175.3218887907387, 1177.7530395923934, 1178.5409807687488, 1188.097905873631, 1173.2316076486366, 1177.2862722864604, 1184.690468949738, 1184.2731658219575, 1183.5091155950784, 1184.5411686840864, 1176.1643951030492, 1166.1988892556194, 1178.6928242679012, 1178.2131573585464, 1174.873927093255, 1175.041099779348, 1176.548493686123, 1165.9272682705503, 1162.8210576258036, 1168.923492609104, 1171.0662128516565, 1173.3698662420732, 1185.0422045560645, 1168.0668603848521, 1178.6423926819105, 1174.9513494607463, 1174.01226567983, 1164.4598287089402, 1169.3844603635469, 1172.5096722425571, 1161.8059221554165, 1156.5657078521342, 1167.482442681049, 1162.7465085736587, 1163.8068425660624, 1159.7668200504907, 1177.9972813103132, 1167.3557166846692, 1169.2596673959717, 1173.5506090003933, 1164.1469760831287, 1172.5865526491925, 1162.9574604175937, 1157.3319777906934, 1154.7369544115481, 1165.2892261579325, 1162.3056940966435, 1162.4144437892035, 1154.0224869817775, 1153.3746553294425, 1162.97086318471, 1159.286791336185, 1169.106950409933, 1165.0297744839709, 1167.2245460267006, 1166.4691081822505, 1150.9759462003553, 1152.893335391745, 1146.8901294733494, 1155.4413989311663, 1151.6257192039145, 1160.5770891524996, 1152.2452593050525, 1158.2169367650813, 1165.2832258647875, 1157.1394338414643, 1150.0947684928078, 1156.5938901348584, 1155.2869882103626, 1155.9951808540095, 1168.716631846316, 1151.1318195619156, 1157.2324303932412, 1148.3296611473033, 1151.3684351286, 1157.1110633251924, 1147.772683464312, 1157.987694470467, 1151.9711850420467, 1152.5480445447224, 1145.2064830228517, 1147.5379921878916, 1141.9522433620382, 1150.5279032124147, 1144.3831433896098, 1134.1283848504158, 1156.3850136086246, 1158.7766919576836, 1153.4976320268636, 1142.1258426205009, 1139.261600351343, 1143.2819566958076, 1150.2599141067558, 1141.5567810780685, 1137.3039714781871, 1144.6874473484575]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9658.331511343556, 9266.2111673946, 8739.275469345297, 8248.19583788945, 7503.970898554993, 6831.496901807982, 6154.5413200297335, 5388.755724355982, 4891.246369760596, 4459.307796364297, 4348.114520703047, 4449.006248570502, 3639.338959958, 2688.209250447648, 2038.4727276269864, 1721.572370585397, 1589.1743002020096, 1591.8228810189696, 1613.319480502823, 1534.8237312765143, 1439.4011154587865, 1445.5575270850695, 1334.9803482217828, 1303.4498300806706, 1357.4228587099897, 1286.6119230556087, 1274.9812704927342, 1300.2202563334592, 1263.5168407586286, 1241.8429837988926, 1257.8238195675945, 1215.2626268973158, 1224.3670298146274, 1223.0457523056039, 1224.5909640211032, 1221.8126634131554, 1217.9569625410834, 1200.8851218000787, 1203.5744038014464, 1200.2537647199124, 1198.019271957676, 1195.6058776255766, 1201.7802483478529, 1199.40673678811, 1196.900172822385, 1198.43324878571, 1190.4015681249764, 1193.6159293704648, 1188.1328388135323, 1193.7037748096222, 1178.5905640554113, 1217.08440174434, 1191.1303947636902, 1184.7908551246915, 1181.109789554404, 1175.2323895399522, 1179.8429200099947, 1187.8007138873572, 1177.809906660638, 1183.4331227255318, 1174.9230024430708, 1192.4059835846638, 1166.4649515253332, 1171.250811631727, 1163.7344582976286, 1165.4357788991613, 1170.8389991957606, 1163.2927027018295, 1175.0885807494017, 1187.2075465546359, 1164.320894383992, 1177.9820836176011, 1209.246768355537, 1169.271922452478, 1167.1469073252565, 1176.3538744681484, 1168.2103659945546, 1183.2047325242897, 1170.3526374383946, 1181.9091895275371, 1182.373967677289, 1166.2503411702457, 1174.563698403936, 1162.3258932452893, 1156.3370062944907, 1177.325251904953, 1203.2762538802058, 1161.3102068552064, 1150.2475903168552, 1157.0463195029733, 1147.3322684634347, 1152.6014185710974, 1161.1217352372028, 1169.3143450804123, 1157.1866017906323, 1149.8953812872453, 1200.015962596501, 1149.3848911221912, 1166.4025208951477, 1151.0000750975976, 1152.3398132507803, 1149.7960851044747, 1149.835830385565, 1154.6885328856981, 1160.4117544972396, 1171.9839865633069, 1144.9968764980356, 1153.6019432996493, 1179.197087916865, 1148.8967784990978, 1147.9523110918467, 1183.1638964677302, 1149.802275834161, 1155.8531785180533, 1155.0093766497973, 1141.5094682943231, 1142.0337209997947, 1143.8088225920596, 1163.9453989552524, 1147.817674415394, 1159.9945409089023, 1150.4586733368085, 1145.0272601586391, 1147.039026395386, 1147.0769799574598, 1140.3538209109308, 1152.5358145778634, 1144.518419986692, 1146.5845988632946, 1146.8922346048946, 1145.3150189026626, 1149.0180996959282, 1149.930157888048, 1140.0915292602563, 1157.836769007623, 1159.5815166796954, 1172.290195377121, 1138.8615212051334, 1134.8768526440228, 1131.8577968734335, 1137.0422641103196, 1138.502823821428, 1132.2271509684947, 1141.3088689038766, 1135.491106148447, 1172.7900202391452, 1139.1658843510706, 1140.998621391203, 1144.004940139284, 1140.369027538005, 1156.210592388844, 1136.6167620774952, 1147.3032855884283, 1135.7416455146663, 1133.6278490089858, 1135.8587825416396, 1164.190111820029, 1135.8584250692425, 1137.483625761942, 1132.1966327294717, 1134.0262887119793, 1134.4201454053978, 1146.7697526753152, 1133.6895027902624, 1131.3682674318463, 1125.891697735401, 1133.371184048252, 1143.677404244199, 1134.4569096692417, 1149.3621390223957, 1147.682165622807, 1145.6001948816925, 1133.6050033064482, 1123.5973856489763, 1152.89985776985, 1135.9636780972326, 1125.149663245532, 1137.0593057819472, 1168.7895219709917, 1125.5105491318443, 1127.0836830544572, 1139.7257138090285, 1143.9353148508387, 1128.2752129658204, 1128.974676884509, 1126.5218275034622, 1122.4191545968738, 1123.9250481427493, 1139.2965555414783, 1127.9034220907954, 1138.0134830050317, 1133.4169659718784, 1128.1007280795302, 1136.8266575616133, 1125.6981134102964, 1118.823398257344, 1121.6230890011295, 1124.4309632534826, 1124.5119473472826, 1122.9514231631147, 1126.2696911873213, 1126.566283053713, 1135.642284368068, 1140.0388046518606, 1122.077279246353, 1125.6961745068427, 1127.5971039595527, 1130.0447124927155, 1120.4429770087775, 1125.28077547522, 1120.6428688466967, 1130.8432606906865, 1138.3870210741286, 1122.4942676398089, 1130.9666098898151, 1128.6682374417626, 1142.0518098031444, 1138.6727776887876, 1133.6037093660314, 1121.3295341569722, 1127.9268700917307, 1130.7696948589771, 1164.3944579637525, 1117.8690350281827, 1133.9292513073053, 1116.37996929223, 1128.079071405968, 1122.3441005452066, 1117.6926405137733, 1115.4450842943606, 1137.3974277212549, 1125.823391305676, 1126.6924960463612, 1115.2065290654139, 1151.7795525201461, 1117.9942041935988, 1122.1494947410142, 1115.0214037943967, 1134.063324702821, 1144.9867751123625, 1124.6133844969577, 1109.270447744213, 1122.4680659950245, 1126.100065443591, 1119.187997068746, 1131.315796853068, 1127.5326886281284, 1122.2961947890687, 1124.8761091827032, 1113.613392745944, 1116.8609416220454, 1108.055278269399, 1128.2647911914728, 1113.0893751733022, 1124.5051158157694, 1117.5104086298204, 1113.9848457796913, 1136.3806411073087, 1114.37201017078, 1111.5862729179853, 1110.4843061931726, 1111.0853023280451, 1116.068563347712, 1132.4101738739473, 1126.196110203913, 1118.4088029756272, 1112.5199375309398, 1121.764671054899, 1115.2924138404383, 1124.1128620203544, 1113.874548753517, 1121.0649084837373, 1115.6587851313807, 1124.4889539217982, 1113.9451309243877, 1117.9643593117496, 1116.2165208888814, 1112.7297266104763, 1111.6943907186028, 1105.6418863605923, 1111.758246024625, 1148.4879002790067, 1110.6545109814815, 1106.0163094210964, 1108.2444020468845, 1107.8831556284622, 1133.826577225787, 1123.1941445218506, 1124.0910097699332, 1113.1894470300515]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6ac1bccc-3857-4af5-9356-6cbcf80a0dab');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.04% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1113   : Mean absolute error \n",
      "\n",
      "9.49% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding network architecture and learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding hidden layer with 1024 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Larger_network\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Larger_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              45056     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,072,897\n",
      "Trainable params: 1,067,777\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 3s 135us/sample - loss: 11191.7813 - val_loss: 11104.0071\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 11177.7168 - val_loss: 11047.0997\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 11149.8164 - val_loss: 10989.5884\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 11107.0936 - val_loss: 10923.0610\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 11046.8626 - val_loss: 10828.3631\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 10964.7572 - val_loss: 10684.6224\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 10855.6469 - val_loss: 10568.0095\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 10714.1471 - val_loss: 10397.5908\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 10534.8886 - val_loss: 10122.9053\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 10310.8124 - val_loss: 9902.7141\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 10040.1836 - val_loss: 9581.6194\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 9714.9072 - val_loss: 9136.7785\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 9333.0576 - val_loss: 8748.8694\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 8890.4241 - val_loss: 8359.2069\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 8379.5776 - val_loss: 7688.4259\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 7795.5548 - val_loss: 7268.5108\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 7140.0719 - val_loss: 6221.2520\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 6401.2566 - val_loss: 5334.0087\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 5583.1006 - val_loss: 4761.3861\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 4691.2218 - val_loss: 3809.7076\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 3757.0229 - val_loss: 2039.7416\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 2812.3104 - val_loss: 1855.6513\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 2053.2429 - val_loss: 2846.7825\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1629.8908 - val_loss: 3255.6716\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1499.8135 - val_loss: 3902.0574\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1461.0208 - val_loss: 2770.0738\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1443.4813 - val_loss: 2410.6838\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1415.5900 - val_loss: 2080.5883\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1403.7270 - val_loss: 1790.5891\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1414.9671 - val_loss: 1553.1482\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1397.3744 - val_loss: 1477.6773\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1369.4582 - val_loss: 1382.2308\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1376.6158 - val_loss: 1348.3411\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1359.5956 - val_loss: 1295.7081\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1366.3201 - val_loss: 1275.3115\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1351.6426 - val_loss: 1302.5815\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1348.5221 - val_loss: 1241.2812\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1343.4524 - val_loss: 1271.8138\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1335.4904 - val_loss: 1236.4490\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1329.5342 - val_loss: 1224.1246\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1326.1064 - val_loss: 1231.8366\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1328.9291 - val_loss: 1224.9224\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1319.8637 - val_loss: 1229.8053\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1320.6552 - val_loss: 1216.0824\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1322.6679 - val_loss: 1219.4858\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1314.1244 - val_loss: 1207.2325\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1294.9642 - val_loss: 1196.6081\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1309.0314 - val_loss: 1194.7369\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1290.8829 - val_loss: 1198.8374\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1294.6940 - val_loss: 1239.6075\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1292.8067 - val_loss: 1210.2184\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1292.1684 - val_loss: 1188.8216\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1290.7918 - val_loss: 1181.0191\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1282.8366 - val_loss: 1179.3446\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1274.5066 - val_loss: 1194.7125\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1280.9680 - val_loss: 1181.7769\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1275.2249 - val_loss: 1177.7937\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1271.8114 - val_loss: 1174.4462\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1268.1467 - val_loss: 1180.9764\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1271.4808 - val_loss: 1178.3790\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1262.3844 - val_loss: 1173.4900\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1266.2824 - val_loss: 1178.0919\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1257.9741 - val_loss: 1180.8704\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1259.3846 - val_loss: 1177.5391\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1259.3531 - val_loss: 1177.5397\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1256.8262 - val_loss: 1162.8140\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1250.7304 - val_loss: 1173.6703\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1246.7766 - val_loss: 1166.4592\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1254.6903 - val_loss: 1159.3884\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1241.5760 - val_loss: 1157.6832\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1247.1129 - val_loss: 1173.2485\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1243.1447 - val_loss: 1181.3267\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1232.7250 - val_loss: 1156.1025\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1239.1896 - val_loss: 1175.2472\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1242.7366 - val_loss: 1155.6676\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1232.1066 - val_loss: 1189.4137\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1227.1216 - val_loss: 1169.5916\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1232.6196 - val_loss: 1164.6877\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1228.2829 - val_loss: 1150.4729\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1232.4166 - val_loss: 1161.3373\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1231.1301 - val_loss: 1156.4642\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1220.4725 - val_loss: 1156.9851\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1225.4576 - val_loss: 1153.2238\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1217.0258 - val_loss: 1148.8428\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1217.0573 - val_loss: 1148.3522\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1208.3480 - val_loss: 1151.8042\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1213.9395 - val_loss: 1160.9843\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1209.8774 - val_loss: 1150.4903\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1210.7351 - val_loss: 1151.4798\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1219.3644 - val_loss: 1154.8104\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1216.2347 - val_loss: 1148.3262\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1209.9386 - val_loss: 1148.0450\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1211.6420 - val_loss: 1160.7284\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1205.8391 - val_loss: 1142.5684\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1195.9159 - val_loss: 1148.9628\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1201.0517 - val_loss: 1135.7509\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1193.7293 - val_loss: 1143.0891\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1205.4586 - val_loss: 1143.6164\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1205.1726 - val_loss: 1146.3202\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1198.1150 - val_loss: 1149.2749\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1198.5556 - val_loss: 1145.3001\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1197.7689 - val_loss: 1147.9182\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1190.7389 - val_loss: 1133.4283\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1200.3374 - val_loss: 1136.4548\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1190.7959 - val_loss: 1139.4579\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1182.4200 - val_loss: 1136.8803\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1187.4481 - val_loss: 1138.6200\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1192.5507 - val_loss: 1141.4828\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1179.8771 - val_loss: 1150.7468\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1186.6232 - val_loss: 1140.7666\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1188.1133 - val_loss: 1136.8352\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1189.9631 - val_loss: 1145.3122\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1183.1829 - val_loss: 1142.4595\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1176.2292 - val_loss: 1137.5366\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1184.2396 - val_loss: 1131.0163\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1175.0098 - val_loss: 1135.4009\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1177.5938 - val_loss: 1138.3074\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1176.0394 - val_loss: 1135.6093\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1179.2780 - val_loss: 1139.1658\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1178.7489 - val_loss: 1132.5431\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1171.6944 - val_loss: 1134.1759\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1174.9187 - val_loss: 1136.4285\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1178.5786 - val_loss: 1136.6675\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1167.9481 - val_loss: 1131.2009\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1169.6081 - val_loss: 1132.7063\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1168.9499 - val_loss: 1129.1233\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1161.4811 - val_loss: 1128.9166\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1165.5420 - val_loss: 1134.7936\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1165.2614 - val_loss: 1135.6371\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1162.5362 - val_loss: 1127.9403\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1167.7863 - val_loss: 1124.6839\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1164.9685 - val_loss: 1134.2503\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1160.6453 - val_loss: 1128.8197\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1152.4605 - val_loss: 1130.7160\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1162.2979 - val_loss: 1135.7670\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1160.7330 - val_loss: 1118.2405\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1152.7017 - val_loss: 1124.1185\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1153.2791 - val_loss: 1130.0860\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1163.3395 - val_loss: 1121.2911\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1154.0525 - val_loss: 1129.4714\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1157.4745 - val_loss: 1121.6477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1153.4533 - val_loss: 1135.5223\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1157.4082 - val_loss: 1123.8697\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1155.8154 - val_loss: 1120.3717\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1151.3753 - val_loss: 1137.0536\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1157.2134 - val_loss: 1123.3693\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1155.1334 - val_loss: 1122.7490\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1145.1992 - val_loss: 1119.7190\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.7928 - val_loss: 1116.8693\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1147.0809 - val_loss: 1141.1769\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1141.6937 - val_loss: 1116.2342\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1143.3863 - val_loss: 1125.2687\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1144.5127 - val_loss: 1114.2963\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1148.6055 - val_loss: 1129.2151\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1135.8822 - val_loss: 1125.1209\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1140.4118 - val_loss: 1129.5868\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1142.5293 - val_loss: 1116.9190\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1142.0537 - val_loss: 1130.4049\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1140.8963 - val_loss: 1127.8650\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1135.0483 - val_loss: 1117.8554\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1143.4147 - val_loss: 1129.2478\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1132.5963 - val_loss: 1119.7662\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1135.2903 - val_loss: 1123.1389\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1137.7989 - val_loss: 1119.8524\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1136.9805 - val_loss: 1116.2235\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1139.7213 - val_loss: 1117.0051\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1129.9562 - val_loss: 1118.7129\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1133.3355 - val_loss: 1116.6451\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1133.5364 - val_loss: 1111.8941\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1130.0472 - val_loss: 1116.7112\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1134.1571 - val_loss: 1110.9121\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1134.8316 - val_loss: 1106.8990\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1134.5239 - val_loss: 1113.1964\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1126.8617 - val_loss: 1114.9231\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1124.9137 - val_loss: 1122.4244\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1132.8232 - val_loss: 1105.8986\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1127.3814 - val_loss: 1116.2803\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1129.4829 - val_loss: 1113.3077\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1127.0785 - val_loss: 1117.5043\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1129.9412 - val_loss: 1121.4487\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1130.2320 - val_loss: 1118.5926\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1125.8525 - val_loss: 1109.8061\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1122.9858 - val_loss: 1115.8491\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1111.4072 - val_loss: 1102.9208\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1122.1518 - val_loss: 1118.3836\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1121.5885 - val_loss: 1110.4960\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1120.9847 - val_loss: 1121.6849\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1122.1546 - val_loss: 1107.0690\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1118.8702 - val_loss: 1112.1270\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1118.4561 - val_loss: 1119.8396\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1115.5318 - val_loss: 1115.2766\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1108.8030 - val_loss: 1106.4538\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1103.0237 - val_loss: 1105.8358\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1122.0221 - val_loss: 1116.2424\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1112.4466 - val_loss: 1122.0045\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1110.2699 - val_loss: 1106.8720\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1122.0378 - val_loss: 1108.0902\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1108.3363 - val_loss: 1107.1151\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1109.7231 - val_loss: 1102.4950\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1107.6734 - val_loss: 1126.1184\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1104.8287 - val_loss: 1114.7518\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1103.0916 - val_loss: 1105.1340\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1113.7931 - val_loss: 1111.3635\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1109.6811 - val_loss: 1112.2969\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1102.0686 - val_loss: 1106.9510\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1114.4023 - val_loss: 1105.3293\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1103.7645 - val_loss: 1106.8950\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1104.9382 - val_loss: 1103.3708\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1098.9957 - val_loss: 1100.0188\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1101.8843 - val_loss: 1105.1213\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1101.1337 - val_loss: 1106.1533\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1101.9900 - val_loss: 1107.6320\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1102.1272 - val_loss: 1106.8279\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1101.6061 - val_loss: 1110.4094\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.9936 - val_loss: 1109.5690\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1096.9917 - val_loss: 1102.2713\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1100.95 - 0s 21us/sample - loss: 1104.6567 - val_loss: 1096.3868\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1096.4944 - val_loss: 1099.2049\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1094.7702 - val_loss: 1106.3374\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1094.6513 - val_loss: 1107.8971\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1094.1042 - val_loss: 1096.1510\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1091.6585 - val_loss: 1107.0860\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1098.1919 - val_loss: 1104.9959\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1099.1543 - val_loss: 1111.3654\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1098.8945 - val_loss: 1101.8464\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1093.6709 - val_loss: 1096.3425\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1092.1056 - val_loss: 1109.5733\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1090.0126 - val_loss: 1108.3903\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1081.4325 - val_loss: 1100.8479\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1090.4031 - val_loss: 1106.0873\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1090.4837 - val_loss: 1111.1488\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1088.9690 - val_loss: 1101.3610\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1094.5511 - val_loss: 1104.1421\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1086.9520 - val_loss: 1099.8020\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1085.7866 - val_loss: 1098.1283\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1084.7606 - val_loss: 1104.6591\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1086.0850 - val_loss: 1100.2430\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1087.5028 - val_loss: 1102.2326\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1082.5225 - val_loss: 1102.1906\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1084.5245 - val_loss: 1103.3167\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1081.4963 - val_loss: 1102.1222\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1088.6544 - val_loss: 1097.3180\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1079.0352 - val_loss: 1103.1909\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1082.3746 - val_loss: 1111.8271\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1081.0319 - val_loss: 1101.4902\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1081.8449 - val_loss: 1099.9394\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.1251 - val_loss: 1094.8733\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1078.8570 - val_loss: 1096.3798\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1085.1342 - val_loss: 1096.4180\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1085.2571 - val_loss: 1098.6562\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1080.5318 - val_loss: 1107.7152\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1078.6969 - val_loss: 1108.0521\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1077.5375 - val_loss: 1101.5287\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1071.1120 - val_loss: 1098.1856\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1077.1595 - val_loss: 1094.5964\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1075.2353 - val_loss: 1100.1945\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1087.5291 - val_loss: 1092.0785\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1073.3235 - val_loss: 1097.3244\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.8969 - val_loss: 1094.4608\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1079.8893 - val_loss: 1097.9683\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1069.7678 - val_loss: 1098.1421\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1068.6344 - val_loss: 1093.5751\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1075.4138 - val_loss: 1095.8512\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1068.6818 - val_loss: 1093.3492\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1082.7457 - val_loss: 1098.9834\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1074.6787 - val_loss: 1096.2587\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1074.2305 - val_loss: 1088.8716\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1067.8775 - val_loss: 1090.1486\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1071.3312 - val_loss: 1098.8719\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1068.1387 - val_loss: 1093.8814\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1069.4008 - val_loss: 1096.0005\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1063.3785 - val_loss: 1110.6851\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1072.4472 - val_loss: 1094.1406\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1059.4326 - val_loss: 1095.8437\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1062.7347 - val_loss: 1097.5160\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1072.1596 - val_loss: 1097.2680\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1070.7763 - val_loss: 1093.1715\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1073.2806 - val_loss: 1094.1303\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1062.1902 - val_loss: 1094.8611\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1062.0487 - val_loss: 1094.9236\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1072.3822 - val_loss: 1096.6150\n",
      "Epoch 282/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1065.2670 - val_loss: 1088.4336\n",
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1067.1925 - val_loss: 1092.1124\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1063.4818 - val_loss: 1099.9645\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.9365 - val_loss: 1093.7499\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1056.9760 - val_loss: 1090.9625\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1069.5802 - val_loss: 1092.2567\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1065.8106 - val_loss: 1099.0495\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1053.5089 - val_loss: 1097.3236\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1062.0779 - val_loss: 1095.9379\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1064.9317 - val_loss: 1090.5419\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1057.1674 - val_loss: 1090.8994\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1061.4672 - val_loss: 1090.7341\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1054.0075 - val_loss: 1094.1926\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1061.0897 - val_loss: 1090.0577\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1054.9827 - val_loss: 1090.5793\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1058.9333 - val_loss: 1092.8253\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1052.6941 - val_loss: 1083.5727\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1058.2072 - val_loss: 1097.9591\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1060.5729 - val_loss: 1090.8213\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10040.183617248596,
          9714.907188377281,
          9333.057560790869,
          8890.424117353557,
          8379.577640439362,
          7795.554827708292,
          7140.071942323479,
          6401.25660134318,
          5583.100618541803,
          4691.22182929523,
          3757.0229371267637,
          2812.310357907121,
          2053.2429169845504,
          1629.8907991833455,
          1499.8135395681115,
          1461.0207611077865,
          1443.4812646425628,
          1415.5900464498711,
          1403.7270050813756,
          1414.9671142773946,
          1397.3743710209044,
          1369.458215848511,
          1376.615761991921,
          1359.5955638468613,
          1366.3200910257294,
          1351.6425717852744,
          1348.522057985528,
          1343.4523977429208,
          1335.4904006765246,
          1329.5341615005655,
          1326.1063514894586,
          1328.9291045145112,
          1319.8636734562597,
          1320.6552465609805,
          1322.6679022685546,
          1314.1244145177852,
          1294.964188359461,
          1309.0313517538177,
          1290.8828927868708,
          1294.694035033078,
          1292.8066654209147,
          1292.168432675934,
          1290.7918425552923,
          1282.8365844457308,
          1274.506601073925,
          1280.9680165255836,
          1275.2248755210815,
          1271.8114278520638,
          1268.146717255881,
          1271.4807500722582,
          1262.38437726174,
          1266.2823865242992,
          1257.974091353339,
          1259.3846336901727,
          1259.353054097881,
          1256.8261991921183,
          1250.7304340161372,
          1246.7766151221692,
          1254.6903427426935,
          1241.5759551249264,
          1247.1129030205095,
          1243.1447360304762,
          1232.7249637142509,
          1239.1896230002694,
          1242.7366101081561,
          1232.106571078946,
          1227.1215759118238,
          1232.6195616659957,
          1228.282906283877,
          1232.4166471252993,
          1231.1300985834655,
          1220.4725079885436,
          1225.4575653045574,
          1217.025765917166,
          1217.0573486670812,
          1208.3480029277293,
          1213.939514686427,
          1209.8773857439498,
          1210.7351470512785,
          1219.3644352269025,
          1216.2346734321736,
          1209.9385777533305,
          1211.6419985908676,
          1205.8390601942003,
          1195.9159005797887,
          1201.051675298158,
          1193.7293289212107,
          1205.458621762285,
          1205.1726151078742,
          1198.11498783556,
          1198.5556226951794,
          1197.7689316441497,
          1190.738873659405,
          1200.3373689072057,
          1190.7958955491308,
          1182.4199502446595,
          1187.4481017099538,
          1192.5507024317915,
          1179.8771093172327,
          1186.623191097635,
          1188.113304944418,
          1189.9631079870162,
          1183.1828594433418,
          1176.229248805684,
          1184.2396275237493,
          1175.00977788833,
          1177.5937681135013,
          1176.0393521446777,
          1179.2779915769281,
          1178.7488667067578,
          1171.6943965687742,
          1174.9187370806676,
          1178.5786136484155,
          1167.9480640877218,
          1169.6081221821266,
          1168.9499206481773,
          1161.481125535768,
          1165.5419933379521,
          1165.2613554274317,
          1162.5362424481386,
          1167.7863445935996,
          1164.968525147806,
          1160.6452831561278,
          1152.460499593866,
          1162.2979374547651,
          1160.7330464725865,
          1152.7017106537544,
          1153.2791086904076,
          1163.3394575652555,
          1154.052526926454,
          1157.4745317072436,
          1153.4533184962543,
          1157.4082180563998,
          1155.8153638424749,
          1151.3752949563395,
          1157.213361257825,
          1155.1334156061619,
          1145.1991943212508,
          1150.7927814221273,
          1147.0808764163778,
          1141.6937489621453,
          1143.3862828755155,
          1144.5126915429335,
          1148.605498906532,
          1135.8821806814906,
          1140.4117938818663,
          1142.5293011585984,
          1142.0536606113394,
          1140.8962988584774,
          1135.0483455960107,
          1143.4147339234353,
          1132.5962823164446,
          1135.290252801816,
          1137.798890665535,
          1136.9804729112097,
          1139.7213493363997,
          1129.956207697631,
          1133.3355456119505,
          1133.5363794743287,
          1130.0471863074463,
          1134.157145467865,
          1134.8316147373796,
          1134.5239225991484,
          1126.8617191318522,
          1124.9136540895,
          1132.8231830004104,
          1127.3813852540043,
          1129.482861274274,
          1127.0785336552772,
          1129.9412217370632,
          1130.232008079405,
          1125.852503178185,
          1122.9858475297492,
          1111.4072070537695,
          1122.151762600339,
          1121.5885059700142,
          1120.9846805551856,
          1122.1546148893374,
          1118.8702446918628,
          1118.456143492612,
          1115.531825275012,
          1108.8029540623982,
          1103.023711968501,
          1122.0220576673178,
          1112.4465616462176,
          1110.2699465142573,
          1122.0378333275892,
          1108.3362960249383,
          1109.7231478969343,
          1107.6734496507324,
          1104.8287039221898,
          1103.0915904545177,
          1113.7930511664313,
          1109.6810803058656,
          1102.0685801149396,
          1114.402251713831,
          1103.764525902111,
          1104.9382116647816,
          1098.9957027431478,
          1101.884261261898,
          1101.1337215285132,
          1101.9900258494351,
          1102.1271619932527,
          1101.6060506095143,
          1100.9935591081735,
          1096.9917079817683,
          1104.6567298364419,
          1096.4944253741369,
          1094.7702334663877,
          1094.6512914534821,
          1094.1042474838878,
          1091.6585151667773,
          1098.1919035145675,
          1099.1542738904354,
          1098.894463740491,
          1093.6708831634123,
          1092.1055692187383,
          1090.0125559364503,
          1081.4324853261057,
          1090.4030622930165,
          1090.4837012511828,
          1088.9690066242542,
          1094.5511292250476,
          1086.952016453717,
          1085.7865757855188,
          1084.7605698889379,
          1086.0850072982723,
          1087.5028239070998,
          1082.5224585998792,
          1084.5244676197176,
          1081.4962952015082,
          1088.6544178438157,
          1079.0352007994222,
          1082.374628942477,
          1081.0319301864536,
          1081.84488363593,
          1078.1251306130314,
          1078.8570424950574,
          1085.1341687851304,
          1085.257099366556,
          1080.5318428989592,
          1078.6968831119113,
          1077.5374559792963,
          1071.1119560393156,
          1077.1595071257536,
          1075.2352651317253,
          1087.5290623051574,
          1073.3234994726524,
          1078.8969067622695,
          1079.8893364691558,
          1069.7678098064735,
          1068.634405509011,
          1075.4137653896223,
          1068.681820144485,
          1082.745739851348,
          1074.6786660209123,
          1074.230450367244,
          1067.8774947186905,
          1071.3312154937798,
          1068.1387410000375,
          1069.4007662108984,
          1063.3784797015207,
          1072.4472017871465,
          1059.4325956226423,
          1062.7347133797875,
          1072.1595563259395,
          1070.7763117210216,
          1073.2806310078079,
          1062.1901608978264,
          1062.0486659846852,
          1072.3822220488112,
          1065.26704676299,
          1067.1925318817207,
          1063.481800630859,
          1061.9365034147377,
          1056.9760084716356,
          1069.5802037690573,
          1065.810585304996,
          1053.5089394535166,
          1062.0779272691027,
          1064.9317104354134,
          1057.1673919573705,
          1061.467192361272,
          1054.0074620282264,
          1061.089729096432,
          1054.9827161704416,
          1058.9333021226282,
          1052.6941312794124,
          1058.2071614101937,
          1060.5729204035963
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9581.619440819568,
          9136.778497335259,
          8748.869379919039,
          8359.206856772922,
          7688.425864474196,
          7268.510835202777,
          6221.252022054216,
          5334.00872326643,
          4761.386120514901,
          3809.7076238062714,
          2039.7415827048765,
          1855.6512730952038,
          2846.782493222613,
          3255.671551306835,
          3902.057359535182,
          2770.073827792103,
          2410.6838148326274,
          2080.588277862286,
          1790.5891259413147,
          1553.1481705461545,
          1477.677282369708,
          1382.230816284449,
          1348.341088465753,
          1295.70812148696,
          1275.3115075759474,
          1302.5815106092246,
          1241.2812085102908,
          1271.8138059100145,
          1236.449042118884,
          1224.1246480350865,
          1231.8365726229995,
          1224.922381908783,
          1229.8052728598261,
          1216.0824470282892,
          1219.4857707431902,
          1207.2325403960454,
          1196.6081001766702,
          1194.7369127994505,
          1198.8374355110395,
          1239.607462556945,
          1210.218418376059,
          1188.8215802492732,
          1181.0191496181087,
          1179.3445671881657,
          1194.7125050277205,
          1181.7769492183584,
          1177.7937481690676,
          1174.446157078717,
          1180.976435754446,
          1178.3790054826143,
          1173.4899955215592,
          1178.0919281978847,
          1180.8703533483933,
          1177.5390774313998,
          1177.539696497025,
          1162.8140114492994,
          1173.6703141791704,
          1166.459240778955,
          1159.388391017818,
          1157.6831809090354,
          1173.248493642063,
          1181.3267303583066,
          1156.1024661189406,
          1175.2471625689684,
          1155.6676036405593,
          1189.41371902885,
          1169.5915708233986,
          1164.6877079870553,
          1150.4728733182837,
          1161.3373202700068,
          1156.4642049798226,
          1156.9850861752275,
          1153.2237987958144,
          1148.8427887115877,
          1148.3521605882324,
          1151.804249838643,
          1160.9842875656393,
          1150.4902742736583,
          1151.4798180347027,
          1154.8104067646193,
          1148.3262001712264,
          1148.0450156246084,
          1160.7284334288681,
          1142.5684029453141,
          1148.9628298713374,
          1135.7508935830817,
          1143.0891375437466,
          1143.6163944958064,
          1146.320166123327,
          1149.274889737732,
          1145.3001097776094,
          1147.9182392531127,
          1133.4283211055777,
          1136.454829088834,
          1139.4579000616448,
          1136.88031873594,
          1138.620040547807,
          1141.4827619927046,
          1150.746763704773,
          1140.7666369817384,
          1136.8352449767833,
          1145.3122313082374,
          1142.459479950606,
          1137.536646109964,
          1131.016311844389,
          1135.4008883301663,
          1138.3073563530804,
          1135.6093423956975,
          1139.1658102081035,
          1132.5430608106467,
          1134.175872551838,
          1136.4285474509115,
          1136.6675351029864,
          1131.2008966476903,
          1132.706320295072,
          1129.1232715803276,
          1128.916636942574,
          1134.793570534366,
          1135.6370565668003,
          1127.9403153031867,
          1124.6838567139798,
          1134.2503192627003,
          1128.8197332106445,
          1130.7160271980588,
          1135.7670287473995,
          1118.2405164805448,
          1124.1185073133506,
          1130.0860249588384,
          1121.2910782879237,
          1129.4714023551076,
          1121.647675655924,
          1135.522260709877,
          1123.8697212361515,
          1120.3716887540418,
          1137.053638116329,
          1123.3693180883579,
          1122.7490267909454,
          1119.718960434826,
          1116.8692697204137,
          1141.1769202955027,
          1116.2341516164684,
          1125.268669022858,
          1114.2963166146042,
          1129.215137262155,
          1125.120924902794,
          1129.5868000453522,
          1116.9189827144326,
          1130.4048577571216,
          1127.8650412459112,
          1117.8553687340993,
          1129.247798915853,
          1119.76619944277,
          1123.1388948179329,
          1119.8523933956803,
          1116.2234909642023,
          1117.0050623075074,
          1118.7128676893908,
          1116.6450722562256,
          1111.8940646315184,
          1116.7111739791035,
          1110.9121049690132,
          1106.898951531795,
          1113.1964401878479,
          1114.9230986649272,
          1122.4244099600367,
          1105.8985838277176,
          1116.2803184030433,
          1113.3077299807626,
          1117.5042761815487,
          1121.448674463762,
          1118.592646936723,
          1109.806060062804,
          1115.8490911330396,
          1102.9208470587982,
          1118.3836190893198,
          1110.4959788271772,
          1121.6849291556484,
          1107.0690143004624,
          1112.126964458177,
          1119.839597137127,
          1115.2766122338,
          1106.4537902795314,
          1105.8357704641442,
          1116.242423611938,
          1122.0044647333066,
          1106.8720478419673,
          1108.0901666109228,
          1107.1150740949906,
          1102.4950166330898,
          1126.1183882048404,
          1114.7517535093195,
          1105.1339786667993,
          1111.3634870095843,
          1112.2968514279705,
          1106.9509540968627,
          1105.329251929039,
          1106.8950348787707,
          1103.3707980593292,
          1100.0187642166509,
          1105.1213070976728,
          1106.1532629612382,
          1107.6320422845492,
          1106.8278699912742,
          1110.4093890208292,
          1109.569009355966,
          1102.2712621908759,
          1096.3868052786854,
          1099.2048934162417,
          1106.3374055503293,
          1107.8971030000266,
          1096.151035147438,
          1107.0860088769864,
          1104.9959295780357,
          1111.365360288312,
          1101.8464344943336,
          1096.3425220269967,
          1109.5732884503616,
          1108.3903413768376,
          1100.8478569426995,
          1106.0872962573785,
          1111.1488227790694,
          1101.3609703588706,
          1104.142103355443,
          1099.8019522144293,
          1098.1282563179893,
          1104.6591282843206,
          1100.2430254942146,
          1102.2326233754636,
          1102.1906490811657,
          1103.3166586396114,
          1102.1221762764828,
          1097.3179901288272,
          1103.1908949841854,
          1111.8271349013294,
          1101.4901802303175,
          1099.9394387321097,
          1094.8733426880592,
          1096.37984751565,
          1096.4179784186933,
          1098.6562248613975,
          1107.715238257164,
          1108.0520893630078,
          1101.5287135468234,
          1098.1855676076157,
          1094.5963609633093,
          1100.1944656938117,
          1092.078463086055,
          1097.3244036056246,
          1094.4607706844436,
          1097.968285731378,
          1098.1421278086698,
          1093.5750500128458,
          1095.8511812107965,
          1093.349248426769,
          1098.9833875938768,
          1096.2586800143263,
          1088.871647802842,
          1090.1485712461777,
          1098.8719477526333,
          1093.8813814110044,
          1096.0005404187607,
          1110.6851022130195,
          1094.1406356722791,
          1095.8436656987856,
          1097.516022811654,
          1097.2680310604446,
          1093.1715251891246,
          1094.130300426617,
          1094.8611031748953,
          1094.9236303941027,
          1096.6149900875089,
          1088.4336114229027,
          1092.1123707283468,
          1099.9644972680924,
          1093.74992908809,
          1090.962475336265,
          1092.2567464470121,
          1099.0494707626547,
          1097.3235934670377,
          1095.9379484560247,
          1090.5419061238908,
          1090.899370736963,
          1090.7341385453742,
          1094.1926442833399,
          1090.057687022391,
          1090.579284141698,
          1092.8252733963775,
          1083.5727166756615,
          1097.9590776105765,
          1090.821307445256
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866,
          1112.9759580230866
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"852f5d79-587a-47f5-9298-bf622a57ed38\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"852f5d79-587a-47f5-9298-bf622a57ed38\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '852f5d79-587a-47f5-9298-bf622a57ed38',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10040.183617248596, 9714.907188377281, 9333.057560790869, 8890.424117353557, 8379.577640439362, 7795.554827708292, 7140.071942323479, 6401.25660134318, 5583.100618541803, 4691.22182929523, 3757.0229371267637, 2812.310357907121, 2053.2429169845504, 1629.8907991833455, 1499.8135395681115, 1461.0207611077865, 1443.4812646425628, 1415.5900464498711, 1403.7270050813756, 1414.9671142773946, 1397.3743710209044, 1369.458215848511, 1376.615761991921, 1359.5955638468613, 1366.3200910257294, 1351.6425717852744, 1348.522057985528, 1343.4523977429208, 1335.4904006765246, 1329.5341615005655, 1326.1063514894586, 1328.9291045145112, 1319.8636734562597, 1320.6552465609805, 1322.6679022685546, 1314.1244145177852, 1294.964188359461, 1309.0313517538177, 1290.8828927868708, 1294.694035033078, 1292.8066654209147, 1292.168432675934, 1290.7918425552923, 1282.8365844457308, 1274.506601073925, 1280.9680165255836, 1275.2248755210815, 1271.8114278520638, 1268.146717255881, 1271.4807500722582, 1262.38437726174, 1266.2823865242992, 1257.974091353339, 1259.3846336901727, 1259.353054097881, 1256.8261991921183, 1250.7304340161372, 1246.7766151221692, 1254.6903427426935, 1241.5759551249264, 1247.1129030205095, 1243.1447360304762, 1232.7249637142509, 1239.1896230002694, 1242.7366101081561, 1232.106571078946, 1227.1215759118238, 1232.6195616659957, 1228.282906283877, 1232.4166471252993, 1231.1300985834655, 1220.4725079885436, 1225.4575653045574, 1217.025765917166, 1217.0573486670812, 1208.3480029277293, 1213.939514686427, 1209.8773857439498, 1210.7351470512785, 1219.3644352269025, 1216.2346734321736, 1209.9385777533305, 1211.6419985908676, 1205.8390601942003, 1195.9159005797887, 1201.051675298158, 1193.7293289212107, 1205.458621762285, 1205.1726151078742, 1198.11498783556, 1198.5556226951794, 1197.7689316441497, 1190.738873659405, 1200.3373689072057, 1190.7958955491308, 1182.4199502446595, 1187.4481017099538, 1192.5507024317915, 1179.8771093172327, 1186.623191097635, 1188.113304944418, 1189.9631079870162, 1183.1828594433418, 1176.229248805684, 1184.2396275237493, 1175.00977788833, 1177.5937681135013, 1176.0393521446777, 1179.2779915769281, 1178.7488667067578, 1171.6943965687742, 1174.9187370806676, 1178.5786136484155, 1167.9480640877218, 1169.6081221821266, 1168.9499206481773, 1161.481125535768, 1165.5419933379521, 1165.2613554274317, 1162.5362424481386, 1167.7863445935996, 1164.968525147806, 1160.6452831561278, 1152.460499593866, 1162.2979374547651, 1160.7330464725865, 1152.7017106537544, 1153.2791086904076, 1163.3394575652555, 1154.052526926454, 1157.4745317072436, 1153.4533184962543, 1157.4082180563998, 1155.8153638424749, 1151.3752949563395, 1157.213361257825, 1155.1334156061619, 1145.1991943212508, 1150.7927814221273, 1147.0808764163778, 1141.6937489621453, 1143.3862828755155, 1144.5126915429335, 1148.605498906532, 1135.8821806814906, 1140.4117938818663, 1142.5293011585984, 1142.0536606113394, 1140.8962988584774, 1135.0483455960107, 1143.4147339234353, 1132.5962823164446, 1135.290252801816, 1137.798890665535, 1136.9804729112097, 1139.7213493363997, 1129.956207697631, 1133.3355456119505, 1133.5363794743287, 1130.0471863074463, 1134.157145467865, 1134.8316147373796, 1134.5239225991484, 1126.8617191318522, 1124.9136540895, 1132.8231830004104, 1127.3813852540043, 1129.482861274274, 1127.0785336552772, 1129.9412217370632, 1130.232008079405, 1125.852503178185, 1122.9858475297492, 1111.4072070537695, 1122.151762600339, 1121.5885059700142, 1120.9846805551856, 1122.1546148893374, 1118.8702446918628, 1118.456143492612, 1115.531825275012, 1108.8029540623982, 1103.023711968501, 1122.0220576673178, 1112.4465616462176, 1110.2699465142573, 1122.0378333275892, 1108.3362960249383, 1109.7231478969343, 1107.6734496507324, 1104.8287039221898, 1103.0915904545177, 1113.7930511664313, 1109.6810803058656, 1102.0685801149396, 1114.402251713831, 1103.764525902111, 1104.9382116647816, 1098.9957027431478, 1101.884261261898, 1101.1337215285132, 1101.9900258494351, 1102.1271619932527, 1101.6060506095143, 1100.9935591081735, 1096.9917079817683, 1104.6567298364419, 1096.4944253741369, 1094.7702334663877, 1094.6512914534821, 1094.1042474838878, 1091.6585151667773, 1098.1919035145675, 1099.1542738904354, 1098.894463740491, 1093.6708831634123, 1092.1055692187383, 1090.0125559364503, 1081.4324853261057, 1090.4030622930165, 1090.4837012511828, 1088.9690066242542, 1094.5511292250476, 1086.952016453717, 1085.7865757855188, 1084.7605698889379, 1086.0850072982723, 1087.5028239070998, 1082.5224585998792, 1084.5244676197176, 1081.4962952015082, 1088.6544178438157, 1079.0352007994222, 1082.374628942477, 1081.0319301864536, 1081.84488363593, 1078.1251306130314, 1078.8570424950574, 1085.1341687851304, 1085.257099366556, 1080.5318428989592, 1078.6968831119113, 1077.5374559792963, 1071.1119560393156, 1077.1595071257536, 1075.2352651317253, 1087.5290623051574, 1073.3234994726524, 1078.8969067622695, 1079.8893364691558, 1069.7678098064735, 1068.634405509011, 1075.4137653896223, 1068.681820144485, 1082.745739851348, 1074.6786660209123, 1074.230450367244, 1067.8774947186905, 1071.3312154937798, 1068.1387410000375, 1069.4007662108984, 1063.3784797015207, 1072.4472017871465, 1059.4325956226423, 1062.7347133797875, 1072.1595563259395, 1070.7763117210216, 1073.2806310078079, 1062.1901608978264, 1062.0486659846852, 1072.3822220488112, 1065.26704676299, 1067.1925318817207, 1063.481800630859, 1061.9365034147377, 1056.9760084716356, 1069.5802037690573, 1065.810585304996, 1053.5089394535166, 1062.0779272691027, 1064.9317104354134, 1057.1673919573705, 1061.467192361272, 1054.0074620282264, 1061.089729096432, 1054.9827161704416, 1058.9333021226282, 1052.6941312794124, 1058.2071614101937, 1060.5729204035963]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9581.619440819568, 9136.778497335259, 8748.869379919039, 8359.206856772922, 7688.425864474196, 7268.510835202777, 6221.252022054216, 5334.00872326643, 4761.386120514901, 3809.7076238062714, 2039.7415827048765, 1855.6512730952038, 2846.782493222613, 3255.671551306835, 3902.057359535182, 2770.073827792103, 2410.6838148326274, 2080.588277862286, 1790.5891259413147, 1553.1481705461545, 1477.677282369708, 1382.230816284449, 1348.341088465753, 1295.70812148696, 1275.3115075759474, 1302.5815106092246, 1241.2812085102908, 1271.8138059100145, 1236.449042118884, 1224.1246480350865, 1231.8365726229995, 1224.922381908783, 1229.8052728598261, 1216.0824470282892, 1219.4857707431902, 1207.2325403960454, 1196.6081001766702, 1194.7369127994505, 1198.8374355110395, 1239.607462556945, 1210.218418376059, 1188.8215802492732, 1181.0191496181087, 1179.3445671881657, 1194.7125050277205, 1181.7769492183584, 1177.7937481690676, 1174.446157078717, 1180.976435754446, 1178.3790054826143, 1173.4899955215592, 1178.0919281978847, 1180.8703533483933, 1177.5390774313998, 1177.539696497025, 1162.8140114492994, 1173.6703141791704, 1166.459240778955, 1159.388391017818, 1157.6831809090354, 1173.248493642063, 1181.3267303583066, 1156.1024661189406, 1175.2471625689684, 1155.6676036405593, 1189.41371902885, 1169.5915708233986, 1164.6877079870553, 1150.4728733182837, 1161.3373202700068, 1156.4642049798226, 1156.9850861752275, 1153.2237987958144, 1148.8427887115877, 1148.3521605882324, 1151.804249838643, 1160.9842875656393, 1150.4902742736583, 1151.4798180347027, 1154.8104067646193, 1148.3262001712264, 1148.0450156246084, 1160.7284334288681, 1142.5684029453141, 1148.9628298713374, 1135.7508935830817, 1143.0891375437466, 1143.6163944958064, 1146.320166123327, 1149.274889737732, 1145.3001097776094, 1147.9182392531127, 1133.4283211055777, 1136.454829088834, 1139.4579000616448, 1136.88031873594, 1138.620040547807, 1141.4827619927046, 1150.746763704773, 1140.7666369817384, 1136.8352449767833, 1145.3122313082374, 1142.459479950606, 1137.536646109964, 1131.016311844389, 1135.4008883301663, 1138.3073563530804, 1135.6093423956975, 1139.1658102081035, 1132.5430608106467, 1134.175872551838, 1136.4285474509115, 1136.6675351029864, 1131.2008966476903, 1132.706320295072, 1129.1232715803276, 1128.916636942574, 1134.793570534366, 1135.6370565668003, 1127.9403153031867, 1124.6838567139798, 1134.2503192627003, 1128.8197332106445, 1130.7160271980588, 1135.7670287473995, 1118.2405164805448, 1124.1185073133506, 1130.0860249588384, 1121.2910782879237, 1129.4714023551076, 1121.647675655924, 1135.522260709877, 1123.8697212361515, 1120.3716887540418, 1137.053638116329, 1123.3693180883579, 1122.7490267909454, 1119.718960434826, 1116.8692697204137, 1141.1769202955027, 1116.2341516164684, 1125.268669022858, 1114.2963166146042, 1129.215137262155, 1125.120924902794, 1129.5868000453522, 1116.9189827144326, 1130.4048577571216, 1127.8650412459112, 1117.8553687340993, 1129.247798915853, 1119.76619944277, 1123.1388948179329, 1119.8523933956803, 1116.2234909642023, 1117.0050623075074, 1118.7128676893908, 1116.6450722562256, 1111.8940646315184, 1116.7111739791035, 1110.9121049690132, 1106.898951531795, 1113.1964401878479, 1114.9230986649272, 1122.4244099600367, 1105.8985838277176, 1116.2803184030433, 1113.3077299807626, 1117.5042761815487, 1121.448674463762, 1118.592646936723, 1109.806060062804, 1115.8490911330396, 1102.9208470587982, 1118.3836190893198, 1110.4959788271772, 1121.6849291556484, 1107.0690143004624, 1112.126964458177, 1119.839597137127, 1115.2766122338, 1106.4537902795314, 1105.8357704641442, 1116.242423611938, 1122.0044647333066, 1106.8720478419673, 1108.0901666109228, 1107.1150740949906, 1102.4950166330898, 1126.1183882048404, 1114.7517535093195, 1105.1339786667993, 1111.3634870095843, 1112.2968514279705, 1106.9509540968627, 1105.329251929039, 1106.8950348787707, 1103.3707980593292, 1100.0187642166509, 1105.1213070976728, 1106.1532629612382, 1107.6320422845492, 1106.8278699912742, 1110.4093890208292, 1109.569009355966, 1102.2712621908759, 1096.3868052786854, 1099.2048934162417, 1106.3374055503293, 1107.8971030000266, 1096.151035147438, 1107.0860088769864, 1104.9959295780357, 1111.365360288312, 1101.8464344943336, 1096.3425220269967, 1109.5732884503616, 1108.3903413768376, 1100.8478569426995, 1106.0872962573785, 1111.1488227790694, 1101.3609703588706, 1104.142103355443, 1099.8019522144293, 1098.1282563179893, 1104.6591282843206, 1100.2430254942146, 1102.2326233754636, 1102.1906490811657, 1103.3166586396114, 1102.1221762764828, 1097.3179901288272, 1103.1908949841854, 1111.8271349013294, 1101.4901802303175, 1099.9394387321097, 1094.8733426880592, 1096.37984751565, 1096.4179784186933, 1098.6562248613975, 1107.715238257164, 1108.0520893630078, 1101.5287135468234, 1098.1855676076157, 1094.5963609633093, 1100.1944656938117, 1092.078463086055, 1097.3244036056246, 1094.4607706844436, 1097.968285731378, 1098.1421278086698, 1093.5750500128458, 1095.8511812107965, 1093.349248426769, 1098.9833875938768, 1096.2586800143263, 1088.871647802842, 1090.1485712461777, 1098.8719477526333, 1093.8813814110044, 1096.0005404187607, 1110.6851022130195, 1094.1406356722791, 1095.8436656987856, 1097.516022811654, 1097.2680310604446, 1093.1715251891246, 1094.130300426617, 1094.8611031748953, 1094.9236303941027, 1096.6149900875089, 1088.4336114229027, 1092.1123707283468, 1099.9644972680924, 1093.74992908809, 1090.962475336265, 1092.2567464470121, 1099.0494707626547, 1097.3235934670377, 1095.9379484560247, 1090.5419061238908, 1090.899370736963, 1090.7341385453742, 1094.1926442833399, 1090.057687022391, 1090.579284141698, 1092.8252733963775, 1083.5727166756615, 1097.9590776105765, 1090.821307445256]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866, 1112.9759580230866]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('852f5d79-587a-47f5-9298-bf622a57ed38');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.33% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1091   : Mean absolute error \n",
      "\n",
      "9.43% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Learning_rate_decay\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating learning rate decay\n",
    "learning_rate = 0.005\n",
    "decay = 5e-4\n",
    "n_epochs=400\n",
    "n_steps_per_epoch = len(X_train) // 1024\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          0.005,
          0.004952947003467063,
          0.004906771344455349,
          0.004861448711716091,
          0.004816955684007707,
          0.0047732696897374695,
          0.004730368968779565,
          0.0046882325363338025,
          0.004646840148698885,
          0.00460617227084293,
          0.004566210045662101,
          0.004526935264825713,
          0.004488330341113105,
          0.004450378282153984,
          0.0044130626654898504,
          0.00437636761487965,
          0.004340277777777778,
          0.004304778303917348,
          0.004269854824935952,
          0.004235493434985176,
          0.004201680672268908,
          0.004168403501458941,
          0.0041356492969396195,
          0.004103405826836275,
          0.004071661237785016,
          0.00404040404040404,
          0.00400962309542903,
          0.003979307600477517,
          0.0039494470774091624,
          0.003920031360250882,
          0.003891050583657588,
          0.003862495171881035,
          0.003834355828220859,
          0.0038066235249333844,
          0.003779289493575208,
          0.00375234521575985,
          0.0037257824143070045,
          0.003699593044765076,
          0.0036737692872887582,
          0.0036483035388544327,
          0.003623188405797102,
          0.003598416696653473,
          0.0035739814152966403,
          0.0035498757543485976,
          0.003526093088857546,
          0.0035026269702276708,
          0.003479471120389701,
          0.0034566194262011757,
          0.0034340659340659344,
          0.0034118048447628795,
          0.003389830508474576,
          0.003368137420006736,
          0.0033467202141900937,
          0.003325573661456602,
          0.0033046926635822873,
          0.0032840722495894913,
          0.0032637075718015664,
          0.003243593902043464,
          0.003223726627981947,
          0.0032041012495994873,
          0.003184713375796179,
          0.003165558721114277,
          0.0031466331025802393,
          0.0031279324366593683,
          0.0031094527363184077,
          0.0030911901081916537,
          0.003073140749846343,
          0.0030553009471432934,
          0.003037667071688943,
          0.0030202355783751134,
          0.003003003003003003,
          0.002985965959988056,
          0.0029691211401425177,
          0.002952465308532625,
          0.0029359953024075164,
          0.0029197080291970805,
          0.0029036004645760743,
          0.0028876696505919725,
          0.002871912693854107,
          0.0028563267637817763,
          0.002840909090909091,
          0.0028256569652444193,
          0.002810567734682406,
          0.0027956388034665923,
          0.0027808676307007787,
          0.0027662517289073303,
          0.0027517886626307097,
          0.0027374760470845875,
          0.0027233115468409588,
          0.00270929287455974,
          0.0026954177897574125,
          0.0026816840976133013,
          0.0026680896478121665,
          0.002654632333421821,
          0.002641310089804543,
          0.002628120893561104,
          0.0026150627615062765,
          0.0026021337496747333,
          0.002589331952356292,
          0.0025766555011594947,
          0.002564102564102564,
          0.0025516713447307987,
          0.0025393600812595226,
          0.0025271670457417236,
          0.0025150905432595573,
          0.0025031289111389237,
          0.0024912805181863482,
          0.0024795437639474342,
          0.00246791707798618,
          0.0024563989191844758,
          0.0024449877750611247,
          0.002433682161109759,
          0.0024224806201550387,
          0.0024113817217265494,
          0.002400384061449832,
          0.0023894862604540022,
          0.0023786869647954324,
          0.0023679848448969923,
          0.0023573785950023575,
          0.002346866932644919,
          0.0023364485981308414,
          0.0023261223540358227,
          0.0023158869847151463,
          0.0023057412958266085,
          0.002295684113865932,
          0.002285714285714286,
          0.002275830678197542,
          0.0022660321776569224,
          0.002256317689530686,
          0.0022466861379465287,
          0.0022371364653243843,
          0.0022276676319893073,
          0.0022182786157941437,
          0.002208968411751712,
          0.002199736031676199,
          0.002190580503833516,
          0.0021815008726003495,
          0.0021724961981316533,
          0.002163565556036348,
          0.0021547080370609784,
          0.002145922746781116,
          0.0021372088053002777,
          0.0021285653469561515,
          0.0021199915200339195,
          0.002111486486486486,
          0.002103049421661409,
          0.0020946795140343527,
          0.002086375964948884,
          0.0020781379883624274,
          0.00206996481059822,
          0.002061855670103093,
          0.0020538098172109263,
          0.0020458265139116204,
          0.002037905033625433,
          0.0020300446609825416,
          0.0020222446916076846,
          0.00201450443190975,
          0.002006823198876179,
          0.0019992003198720507,
          0.0019916351324437364,
          0.001984126984126984,
          0.0019766752322593396,
          0.0019692792437967705,
          0.001961938395134393,
          0.0019546520719311965,
          0.0019474196689386564,
          0.0019402405898331393,
          0.0019331142470520007,
          0.001926040061633282,
          0.0019190174630589138,
          0.0019120458891013384,
          0.0019051247856734614,
          0.0018982536066818525,
          0.0018914318138831092,
          0.0018846588767433092,
          0.0018779342723004694,
          0.0018712574850299403,
          0.001864628006712661,
          0.001858045336306206,
          0.0018515089798185522,
          0.001845018450184502,
          0.0018385732671446957,
          0.0018321729571271527,
          0.0018258170531312763,
          0.001819505094614265,
          0.0018132366273798729,
          0.0018070112034694616,
          0.0018008283810552854,
          0.0017946877243359657,
          0.0017885888034340906,
          0.0017825311942959003,
          0.0017765144785930006,
          0.0017705382436260624,
          0.0017646020822304571,
          0.0017587055926837848,
          0.0017528483786152498,
          0.0017470300489168414,
          0.001741250217656277,
          0.0017355085039916696,
          0.001729804532087874,
          0.0017241379310344825,
          0.0017185083347654234,
          0.0017129153819801302,
          0.0017073587160662455,
          0.0017018379850238256,
          0.0016963528413910093,
          0.0016909029421711195,
          0.0016854879487611665,
          0.0016801075268817205,
          0.0016747613465081226,
          0.001669449081803005,
          0.0016641704110500917,
          0.0016589250165892503,
          0.0016537125847527702,
          0.0016485328058028356,
          0.0016433853738701727,
          0.00163826998689384,
          0.0016331863465621427,
          0.0016281341582546401,
          0.0016231131309852299,
          0.0016181229773462784,
          0.0016131634134537829,
          0.0016082341588935349,
          0.00160333493666827,
          0.00159846547314578,
          0.001593625498007968,
          0.001588814744200826,
          0.0015840329478853162,
          0.0015792798483891346,
          0.001574555188159345,
          0.0015698587127158557,
          0.0015651901706057285,
          0.001560549313358302,
          0.0015559358954411078,
          0.0015513496742165683,
          0.0015467904098994587,
          0.0015422578655151142,
          0.001537751806858373,
          0.0015332720024532351,
          0.0015288182235132241,
          0.001524390243902439,
          0.0015199878400972793,
          0.001515610791148833,
          0.001511258878645912,
          0.0015069318866787222,
          0.0015026296018031556,
          0.0014983518130056938,
          0.0014940983116689078,
          0.0014898688915375448,
          0.001485663348685188,
          0.0014814814814814814,
          0.0014773230905599055,
          0.0014731879787860931,
          0.0014690759512266784,
          0.001464986815118664,
          0.0014609203798392988,
          0.001456876456876457,
          0.001452854859799506,
          0.0014488554042306578,
          0.0014448779078167894,
          0.001440922190201729,
          0.001436988072998994,
          0.0014330753797649758,
          0.0014291839359725598,
          0.0014253135689851768,
          0.0014214641080312722,
          0.001417635384179189,
          0.0014138272303124558,
          0.0014100394811054709,
          0.001406271972999578,
          0.001402524544179523,
          0.0013987970345502867,
          0.0013950892857142857,
          0.0013914011409489355,
          0.0013877324451845683,
          0.001384083044982699,
          0.0013804527885146326,
          0.0013768415255404102,
          0.0013732491073880802,
          0.0013696753869332967,
          0.001366120218579235,
          0.0013625834582368169,
          0.001359064963305246,
          0.0013555645926528399,
          0.001352082206598161,
          0.001348617666891436,
          0.0013451708366962604,
          0.001341741580571582,
          0.0013383297644539614,
          0.0013349352556401015,
          0.0013315579227696406,
          0.0013281976358082083,
          0.0013248542660307366,
          0.001321527686005022,
          0.001318217769575534,
          0.0013149243918474688,
          0.001311647429171039,
          0.0013083867591259977,
          0.0013051422605063953,
          0.0013019138133055592,
          0.0012987012987012987,
          0.0012955045990413268,
          0.0012923235978288964,
          0.0012891581797086504,
          0.001286008230452675,
          0.0012828736369467609,
          0.001279754287176862,
          0.0012766500702157538,
          0.0012735608762098828,
          0.0012704865963664084,
          0.001267427122940431,
          0.001264382349222405,
          0.0012613521695257316,
          0.0012583364791745313,
          0.0012553351744915891,
          0.0012523481527864746,
          0.001249375312343828,
          0.0012464165524118161,
          0.0012434717731907485,
          0.0012405408758218584,
          0.0012376237623762376,
          0.0012347203358439314,
          0.001231830500123183,
          0.0012289541600098315,
          0.0012260912211868563,
          0.0012232415902140672,
          0.0012204051745179402,
          0.00121758188238159,
          0.0012147716229348885,
          0.0012119743061447096,
          0.0012091898428053206,
          0.0012064181445288935,
          0.001203659123736158,
          0.001200912693647172,
          0.0011981787682722263,
          0.001195457262402869,
          0.0011927480916030535,
          0.0011900511722004047,
          0.0011873664212776061,
          0.0011846937566639022,
          0.001182033096926714,
          0.0011793843613633684,
          0.0011767474699929394,
          0.0011741223435481978,
          0.0011715089034676663,
          0.001168907071887785,
          0.0011663167716351758,
          0.0011637379262190155,
          0.0011611704598235018,
          0.0011586142973004287,
          0.0011560693641618498,
          0.0011535355865728457,
          0.0011510128913443832,
          0.0011485012059262662,
          0.0011460004584001836,
          0.0011435105774728418,
          0.0011410314924691922,
          0.001138563133325743,
          0.0011361054305839583,
          0.0011336583153837435,
          0.0011312217194570137,
          0.0011287955751213454,
          0.0011263798152737104,
          0.0011239743733842868,
          0.0011215791834903544,
          0.001119194180190263,
          0.0011168192986374804,
          0.0011144544745347153,
          0.0011120996441281138,
          0.0011097547442015317,
          0.0011074197120708748,
          0.001105094485578517,
          0.0011027790030877812,
          0.0011004732034774953,
          0.0010981770261366134,
          0.001095890410958904,
          0.0010936132983377078,
          0.0010913456291607553,
          0.0010890873448050533,
          0.0010868383871318334,
          0.001084598698481562,
          0.0010823682216690117,
          0.0010801468999783973,
          0.001077934677158564,
          0.0010757314974182445,
          0.0010735373054213632,
          0.0010713520462824085,
          0.0010691756655618518,
          0.0010670081092616305,
          0.0010648493238206793,
          0.0010626992561105207,
          0.0010605578534309047,
          0.0010584250635055038,
          0.001056300834477659,
          0.0010541851149061775,
          0.0010520778537611783,
          0.0010499790004199914,
          0.001047888504663104,
          0.0010458063166701526,
          0.001043732387015969
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning rate decay"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Learning rate"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"881a5b76-523e-4a36-80ff-697e2a0b4a3e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"881a5b76-523e-4a36-80ff-697e2a0b4a3e\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '881a5b76-523e-4a36-80ff-697e2a0b4a3e',\n",
       "                        [{\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [0.005, 0.004952947003467063, 0.004906771344455349, 0.004861448711716091, 0.004816955684007707, 0.0047732696897374695, 0.004730368968779565, 0.0046882325363338025, 0.004646840148698885, 0.00460617227084293, 0.004566210045662101, 0.004526935264825713, 0.004488330341113105, 0.004450378282153984, 0.0044130626654898504, 0.00437636761487965, 0.004340277777777778, 0.004304778303917348, 0.004269854824935952, 0.004235493434985176, 0.004201680672268908, 0.004168403501458941, 0.0041356492969396195, 0.004103405826836275, 0.004071661237785016, 0.00404040404040404, 0.00400962309542903, 0.003979307600477517, 0.0039494470774091624, 0.003920031360250882, 0.003891050583657588, 0.003862495171881035, 0.003834355828220859, 0.0038066235249333844, 0.003779289493575208, 0.00375234521575985, 0.0037257824143070045, 0.003699593044765076, 0.0036737692872887582, 0.0036483035388544327, 0.003623188405797102, 0.003598416696653473, 0.0035739814152966403, 0.0035498757543485976, 0.003526093088857546, 0.0035026269702276708, 0.003479471120389701, 0.0034566194262011757, 0.0034340659340659344, 0.0034118048447628795, 0.003389830508474576, 0.003368137420006736, 0.0033467202141900937, 0.003325573661456602, 0.0033046926635822873, 0.0032840722495894913, 0.0032637075718015664, 0.003243593902043464, 0.003223726627981947, 0.0032041012495994873, 0.003184713375796179, 0.003165558721114277, 0.0031466331025802393, 0.0031279324366593683, 0.0031094527363184077, 0.0030911901081916537, 0.003073140749846343, 0.0030553009471432934, 0.003037667071688943, 0.0030202355783751134, 0.003003003003003003, 0.002985965959988056, 0.0029691211401425177, 0.002952465308532625, 0.0029359953024075164, 0.0029197080291970805, 0.0029036004645760743, 0.0028876696505919725, 0.002871912693854107, 0.0028563267637817763, 0.002840909090909091, 0.0028256569652444193, 0.002810567734682406, 0.0027956388034665923, 0.0027808676307007787, 0.0027662517289073303, 0.0027517886626307097, 0.0027374760470845875, 0.0027233115468409588, 0.00270929287455974, 0.0026954177897574125, 0.0026816840976133013, 0.0026680896478121665, 0.002654632333421821, 0.002641310089804543, 0.002628120893561104, 0.0026150627615062765, 0.0026021337496747333, 0.002589331952356292, 0.0025766555011594947, 0.002564102564102564, 0.0025516713447307987, 0.0025393600812595226, 0.0025271670457417236, 0.0025150905432595573, 0.0025031289111389237, 0.0024912805181863482, 0.0024795437639474342, 0.00246791707798618, 0.0024563989191844758, 0.0024449877750611247, 0.002433682161109759, 0.0024224806201550387, 0.0024113817217265494, 0.002400384061449832, 0.0023894862604540022, 0.0023786869647954324, 0.0023679848448969923, 0.0023573785950023575, 0.002346866932644919, 0.0023364485981308414, 0.0023261223540358227, 0.0023158869847151463, 0.0023057412958266085, 0.002295684113865932, 0.002285714285714286, 0.002275830678197542, 0.0022660321776569224, 0.002256317689530686, 0.0022466861379465287, 0.0022371364653243843, 0.0022276676319893073, 0.0022182786157941437, 0.002208968411751712, 0.002199736031676199, 0.002190580503833516, 0.0021815008726003495, 0.0021724961981316533, 0.002163565556036348, 0.0021547080370609784, 0.002145922746781116, 0.0021372088053002777, 0.0021285653469561515, 0.0021199915200339195, 0.002111486486486486, 0.002103049421661409, 0.0020946795140343527, 0.002086375964948884, 0.0020781379883624274, 0.00206996481059822, 0.002061855670103093, 0.0020538098172109263, 0.0020458265139116204, 0.002037905033625433, 0.0020300446609825416, 0.0020222446916076846, 0.00201450443190975, 0.002006823198876179, 0.0019992003198720507, 0.0019916351324437364, 0.001984126984126984, 0.0019766752322593396, 0.0019692792437967705, 0.001961938395134393, 0.0019546520719311965, 0.0019474196689386564, 0.0019402405898331393, 0.0019331142470520007, 0.001926040061633282, 0.0019190174630589138, 0.0019120458891013384, 0.0019051247856734614, 0.0018982536066818525, 0.0018914318138831092, 0.0018846588767433092, 0.0018779342723004694, 0.0018712574850299403, 0.001864628006712661, 0.001858045336306206, 0.0018515089798185522, 0.001845018450184502, 0.0018385732671446957, 0.0018321729571271527, 0.0018258170531312763, 0.001819505094614265, 0.0018132366273798729, 0.0018070112034694616, 0.0018008283810552854, 0.0017946877243359657, 0.0017885888034340906, 0.0017825311942959003, 0.0017765144785930006, 0.0017705382436260624, 0.0017646020822304571, 0.0017587055926837848, 0.0017528483786152498, 0.0017470300489168414, 0.001741250217656277, 0.0017355085039916696, 0.001729804532087874, 0.0017241379310344825, 0.0017185083347654234, 0.0017129153819801302, 0.0017073587160662455, 0.0017018379850238256, 0.0016963528413910093, 0.0016909029421711195, 0.0016854879487611665, 0.0016801075268817205, 0.0016747613465081226, 0.001669449081803005, 0.0016641704110500917, 0.0016589250165892503, 0.0016537125847527702, 0.0016485328058028356, 0.0016433853738701727, 0.00163826998689384, 0.0016331863465621427, 0.0016281341582546401, 0.0016231131309852299, 0.0016181229773462784, 0.0016131634134537829, 0.0016082341588935349, 0.00160333493666827, 0.00159846547314578, 0.001593625498007968, 0.001588814744200826, 0.0015840329478853162, 0.0015792798483891346, 0.001574555188159345, 0.0015698587127158557, 0.0015651901706057285, 0.001560549313358302, 0.0015559358954411078, 0.0015513496742165683, 0.0015467904098994587, 0.0015422578655151142, 0.001537751806858373, 0.0015332720024532351, 0.0015288182235132241, 0.001524390243902439, 0.0015199878400972793, 0.001515610791148833, 0.001511258878645912, 0.0015069318866787222, 0.0015026296018031556, 0.0014983518130056938, 0.0014940983116689078, 0.0014898688915375448, 0.001485663348685188, 0.0014814814814814814, 0.0014773230905599055, 0.0014731879787860931, 0.0014690759512266784, 0.001464986815118664, 0.0014609203798392988, 0.001456876456876457, 0.001452854859799506, 0.0014488554042306578, 0.0014448779078167894, 0.001440922190201729, 0.001436988072998994, 0.0014330753797649758, 0.0014291839359725598, 0.0014253135689851768, 0.0014214641080312722, 0.001417635384179189, 0.0014138272303124558, 0.0014100394811054709, 0.001406271972999578, 0.001402524544179523, 0.0013987970345502867, 0.0013950892857142857, 0.0013914011409489355, 0.0013877324451845683, 0.001384083044982699, 0.0013804527885146326, 0.0013768415255404102, 0.0013732491073880802, 0.0013696753869332967, 0.001366120218579235, 0.0013625834582368169, 0.001359064963305246, 0.0013555645926528399, 0.001352082206598161, 0.001348617666891436, 0.0013451708366962604, 0.001341741580571582, 0.0013383297644539614, 0.0013349352556401015, 0.0013315579227696406, 0.0013281976358082083, 0.0013248542660307366, 0.001321527686005022, 0.001318217769575534, 0.0013149243918474688, 0.001311647429171039, 0.0013083867591259977, 0.0013051422605063953, 0.0013019138133055592, 0.0012987012987012987, 0.0012955045990413268, 0.0012923235978288964, 0.0012891581797086504, 0.001286008230452675, 0.0012828736369467609, 0.001279754287176862, 0.0012766500702157538, 0.0012735608762098828, 0.0012704865963664084, 0.001267427122940431, 0.001264382349222405, 0.0012613521695257316, 0.0012583364791745313, 0.0012553351744915891, 0.0012523481527864746, 0.001249375312343828, 0.0012464165524118161, 0.0012434717731907485, 0.0012405408758218584, 0.0012376237623762376, 0.0012347203358439314, 0.001231830500123183, 0.0012289541600098315, 0.0012260912211868563, 0.0012232415902140672, 0.0012204051745179402, 0.00121758188238159, 0.0012147716229348885, 0.0012119743061447096, 0.0012091898428053206, 0.0012064181445288935, 0.001203659123736158, 0.001200912693647172, 0.0011981787682722263, 0.001195457262402869, 0.0011927480916030535, 0.0011900511722004047, 0.0011873664212776061, 0.0011846937566639022, 0.001182033096926714, 0.0011793843613633684, 0.0011767474699929394, 0.0011741223435481978, 0.0011715089034676663, 0.001168907071887785, 0.0011663167716351758, 0.0011637379262190155, 0.0011611704598235018, 0.0011586142973004287, 0.0011560693641618498, 0.0011535355865728457, 0.0011510128913443832, 0.0011485012059262662, 0.0011460004584001836, 0.0011435105774728418, 0.0011410314924691922, 0.001138563133325743, 0.0011361054305839583, 0.0011336583153837435, 0.0011312217194570137, 0.0011287955751213454, 0.0011263798152737104, 0.0011239743733842868, 0.0011215791834903544, 0.001119194180190263, 0.0011168192986374804, 0.0011144544745347153, 0.0011120996441281138, 0.0011097547442015317, 0.0011074197120708748, 0.001105094485578517, 0.0011027790030877812, 0.0011004732034774953, 0.0010981770261366134, 0.001095890410958904, 0.0010936132983377078, 0.0010913456291607553, 0.0010890873448050533, 0.0010868383871318334, 0.001084598698481562, 0.0010823682216690117, 0.0010801468999783973, 0.001077934677158564, 0.0010757314974182445, 0.0010735373054213632, 0.0010713520462824085, 0.0010691756655618518, 0.0010670081092616305, 0.0010648493238206793, 0.0010626992561105207, 0.0010605578534309047, 0.0010584250635055038, 0.001056300834477659, 0.0010541851149061775, 0.0010520778537611783, 0.0010499790004199914, 0.001047888504663104, 0.0010458063166701526, 0.001043732387015969]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning rate decay\"}, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Learning rate\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('881a5b76-523e-4a36-80ff-697e2a0b4a3e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing learning rate decay\n",
    "\n",
    "trace0=go.Scatter(\n",
    "            y=lrs,\n",
    "            x=epochs,\n",
    "            mode='lines',\n",
    "            marker=dict(\n",
    "            color=\"red\",\n",
    "            size=5,\n",
    "            opacity=0.5\n",
    "            )\n",
    "    )\n",
    "        \n",
    "\n",
    "data=[trace0]\n",
    "figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning rate decay\",\n",
    "                yaxis=dict(title=\"Learning rate\"),\n",
    "                xaxis=dict(title=\"Epoch\"),\n",
    "                legend=dict(\n",
    "                    x=1,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None\n",
    "\n",
    "\n",
    "            )))\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/400\n",
      "19948/19948 [==============================] - 3s 132us/sample - loss: 11171.3893 - val_loss: 10215.7505\n",
      "Epoch 2/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 10988.3081 - val_loss: 7682.4615\n",
      "Epoch 3/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 10367.4705 - val_loss: 5755.6816\n",
      "Epoch 4/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 8943.1965 - val_loss: 8368.0323\n",
      "Epoch 5/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 6419.0709 - val_loss: 16154.6143\n",
      "Epoch 6/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 4153.5336 - val_loss: 10507.1267\n",
      "Epoch 7/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1950.1331 - val_loss: 11215.8439\n",
      "Epoch 8/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1583.2384 - val_loss: 9686.1994\n",
      "Epoch 9/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1546.2508 - val_loss: 6067.5494\n",
      "Epoch 10/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1495.0806 - val_loss: 4068.7214\n",
      "Epoch 11/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1478.9997 - val_loss: 3176.4033\n",
      "Epoch 12/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1472.8891 - val_loss: 2925.2330\n",
      "Epoch 13/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1454.5666 - val_loss: 2240.1676\n",
      "Epoch 14/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1441.8253 - val_loss: 1949.4417\n",
      "Epoch 15/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1421.9105 - val_loss: 1759.7442\n",
      "Epoch 16/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1412.6027 - val_loss: 1534.8839\n",
      "Epoch 17/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1401.4311 - val_loss: 1512.4535\n",
      "Epoch 18/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1390.3983 - val_loss: 1465.2659\n",
      "Epoch 19/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1375.1768 - val_loss: 1448.2785\n",
      "Epoch 20/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1368.7974 - val_loss: 1332.5446\n",
      "Epoch 21/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1363.0600 - val_loss: 1374.6246\n",
      "Epoch 22/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1371.3599 - val_loss: 1247.3151\n",
      "Epoch 23/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1353.9471 - val_loss: 1291.7592\n",
      "Epoch 24/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1344.5593 - val_loss: 1308.7781\n",
      "Epoch 25/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1333.3613 - val_loss: 1304.6396\n",
      "Epoch 26/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1331.0421 - val_loss: 1231.2942\n",
      "Epoch 27/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1333.9522 - val_loss: 1242.1768\n",
      "Epoch 28/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1321.0983 - val_loss: 1320.4540\n",
      "Epoch 29/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1310.7241 - val_loss: 1230.5342\n",
      "Epoch 30/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1306.8174 - val_loss: 1195.4961\n",
      "Epoch 31/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1312.1211 - val_loss: 1207.9169\n",
      "Epoch 32/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1307.4930 - val_loss: 1203.8857\n",
      "Epoch 33/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1300.9074 - val_loss: 1200.1559\n",
      "Epoch 34/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1308.7684 - val_loss: 1193.7384\n",
      "Epoch 35/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1292.1311 - val_loss: 1221.5822\n",
      "Epoch 36/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1283.8276 - val_loss: 1223.2765\n",
      "Epoch 37/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1284.8816 - val_loss: 1243.4543\n",
      "Epoch 38/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1282.5827 - val_loss: 1199.3604\n",
      "Epoch 39/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1278.0270 - val_loss: 1227.0352\n",
      "Epoch 40/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1270.5384 - val_loss: 1186.7755\n",
      "Epoch 41/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1271.1846 - val_loss: 1205.7774\n",
      "Epoch 42/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1270.7635 - val_loss: 1195.8363\n",
      "Epoch 43/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1268.2521 - val_loss: 1265.7327\n",
      "Epoch 44/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1266.5519 - val_loss: 1199.3532\n",
      "Epoch 45/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1252.9612 - val_loss: 1206.9446\n",
      "Epoch 46/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1255.3886 - val_loss: 1247.0357\n",
      "Epoch 47/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1246.0041 - val_loss: 1196.9381\n",
      "Epoch 48/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1256.4910 - val_loss: 1182.9023\n",
      "Epoch 49/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1239.4938 - val_loss: 1181.0256\n",
      "Epoch 50/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1237.3325 - val_loss: 1170.4183\n",
      "Epoch 51/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1245.1568 - val_loss: 1174.5783\n",
      "Epoch 52/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1226.4495 - val_loss: 1165.6469\n",
      "Epoch 53/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1237.4502 - val_loss: 1173.5118\n",
      "Epoch 54/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1220.7018 - val_loss: 1167.2413\n",
      "Epoch 55/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1233.4919 - val_loss: 1169.8997\n",
      "Epoch 56/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1237.9195 - val_loss: 1168.1987\n",
      "Epoch 57/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1234.1683 - val_loss: 1167.8557\n",
      "Epoch 58/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1218.9414 - val_loss: 1166.2843\n",
      "Epoch 59/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1223.8054 - val_loss: 1161.1528\n",
      "Epoch 60/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1218.6093 - val_loss: 1171.8446\n",
      "Epoch 61/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1223.0719 - val_loss: 1160.7840\n",
      "Epoch 62/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1222.1706 - val_loss: 1156.5714\n",
      "Epoch 63/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1213.2591 - val_loss: 1162.5355\n",
      "Epoch 64/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1208.4999 - val_loss: 1155.5768\n",
      "Epoch 65/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1205.1530 - val_loss: 1156.2761\n",
      "Epoch 66/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1206.3395 - val_loss: 1153.3960\n",
      "Epoch 67/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1205.1062 - val_loss: 1146.3801\n",
      "Epoch 68/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1207.7370 - val_loss: 1151.8381\n",
      "Epoch 69/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1195.9755 - val_loss: 1147.0857\n",
      "Epoch 70/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1200.5442 - val_loss: 1161.9752\n",
      "Epoch 71/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1207.0779 - val_loss: 1166.1350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1203.2417 - val_loss: 1173.7623\n",
      "Epoch 73/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1192.2533 - val_loss: 1148.6615\n",
      "Epoch 74/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1187.7563 - val_loss: 1150.8937\n",
      "Epoch 75/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1190.1098 - val_loss: 1156.0838\n",
      "Epoch 76/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1192.0337 - val_loss: 1156.8406\n",
      "Epoch 77/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1181.2613 - val_loss: 1144.4151\n",
      "Epoch 78/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1184.4125 - val_loss: 1150.8255\n",
      "Epoch 79/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1188.5842 - val_loss: 1154.2351\n",
      "Epoch 80/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1198.5131 - val_loss: 1140.9981\n",
      "Epoch 81/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1187.7368 - val_loss: 1144.3241\n",
      "Epoch 82/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1179.0636 - val_loss: 1145.4419\n",
      "Epoch 83/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1179.9847 - val_loss: 1139.4797\n",
      "Epoch 84/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1180.2423 - val_loss: 1140.7573\n",
      "Epoch 85/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1174.5527 - val_loss: 1130.5320\n",
      "Epoch 86/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1166.7675 - val_loss: 1151.3620\n",
      "Epoch 87/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1174.5553 - val_loss: 1137.5939\n",
      "Epoch 88/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1168.8724 - val_loss: 1134.2736\n",
      "Epoch 89/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1172.4297 - val_loss: 1130.5407\n",
      "Epoch 90/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1168.5507 - val_loss: 1150.8820\n",
      "Epoch 91/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1169.4566 - val_loss: 1149.9186\n",
      "Epoch 92/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1167.9000 - val_loss: 1139.0605\n",
      "Epoch 93/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1163.7815 - val_loss: 1140.0165\n",
      "Epoch 94/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1164.6243 - val_loss: 1138.4549\n",
      "Epoch 95/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1161.6407 - val_loss: 1120.4373\n",
      "Epoch 96/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1160.1164 - val_loss: 1129.1393\n",
      "Epoch 97/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1157.2494 - val_loss: 1132.6673\n",
      "Epoch 98/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1162.2527 - val_loss: 1129.9865\n",
      "Epoch 99/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1154.6709 - val_loss: 1136.8660\n",
      "Epoch 100/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1156.8919 - val_loss: 1127.3669\n",
      "Epoch 101/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1155.0649 - val_loss: 1126.7519\n",
      "Epoch 102/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1154.5463 - val_loss: 1127.5029\n",
      "Epoch 103/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1157.8409 - val_loss: 1131.3542\n",
      "Epoch 104/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1154.8048 - val_loss: 1130.0401\n",
      "Epoch 105/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1151.5264 - val_loss: 1129.7626\n",
      "Epoch 106/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1152.2884 - val_loss: 1125.9748\n",
      "Epoch 107/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1147.8908 - val_loss: 1124.8047\n",
      "Epoch 108/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1151.4173 - val_loss: 1143.2182\n",
      "Epoch 109/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1156.6572 - val_loss: 1124.0528\n",
      "Epoch 110/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1144.2975 - val_loss: 1118.5714\n",
      "Epoch 111/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1147.7631 - val_loss: 1126.5930\n",
      "Epoch 112/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1155.7009 - val_loss: 1131.1191\n",
      "Epoch 113/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1147.5153 - val_loss: 1131.1156\n",
      "Epoch 114/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1144.6151 - val_loss: 1126.2678\n",
      "Epoch 115/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1141.8574 - val_loss: 1120.9895\n",
      "Epoch 116/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1136.2450 - val_loss: 1122.8627\n",
      "Epoch 117/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1142.4214 - val_loss: 1117.9967\n",
      "Epoch 118/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1139.8607 - val_loss: 1125.7288\n",
      "Epoch 119/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1141.0425 - val_loss: 1118.0810\n",
      "Epoch 120/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1136.3512 - val_loss: 1117.9243\n",
      "Epoch 121/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1143.2770 - val_loss: 1118.5669\n",
      "Epoch 122/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1140.7866 - val_loss: 1120.8252\n",
      "Epoch 123/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1128.6995 - val_loss: 1121.8296\n",
      "Epoch 124/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1129.1282 - val_loss: 1111.7082\n",
      "Epoch 125/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1131.3403 - val_loss: 1113.6793\n",
      "Epoch 126/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1135.2570 - val_loss: 1117.9399\n",
      "Epoch 127/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1134.8993 - val_loss: 1120.6955\n",
      "Epoch 128/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1131.4895 - val_loss: 1110.2656\n",
      "Epoch 129/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1128.5214 - val_loss: 1114.9713\n",
      "Epoch 130/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1132.1713 - val_loss: 1118.3960\n",
      "Epoch 131/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1131.6324 - val_loss: 1107.2108\n",
      "Epoch 132/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1128.8726 - val_loss: 1120.0576\n",
      "Epoch 133/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1127.5354 - val_loss: 1108.6468\n",
      "Epoch 134/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1117.3829 - val_loss: 1109.9262\n",
      "Epoch 135/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1126.6394 - val_loss: 1111.6981\n",
      "Epoch 136/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1126.4704 - val_loss: 1107.6814\n",
      "Epoch 137/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1125.1154 - val_loss: 1108.9067\n",
      "Epoch 138/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1120.6588 - val_loss: 1111.7730\n",
      "Epoch 139/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1126.4646 - val_loss: 1111.1691\n",
      "Epoch 140/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1116.0568 - val_loss: 1113.5757\n",
      "Epoch 141/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1122.6625 - val_loss: 1112.8443\n",
      "Epoch 142/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1120.0865 - val_loss: 1115.5196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1117.7140 - val_loss: 1108.5362\n",
      "Epoch 144/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.4595 - val_loss: 1109.4751\n",
      "Epoch 145/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1119.1809 - val_loss: 1111.0172\n",
      "Epoch 146/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.7701 - val_loss: 1104.1957\n",
      "Epoch 147/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1115.8989 - val_loss: 1110.8588\n",
      "Epoch 148/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1109.6549 - val_loss: 1109.1258\n",
      "Epoch 149/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1118.5132 - val_loss: 1106.3682\n",
      "Epoch 150/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1117.1358 - val_loss: 1107.2845\n",
      "Epoch 151/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1114.9052 - val_loss: 1106.6571\n",
      "Epoch 152/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1112.2109 - val_loss: 1101.8939\n",
      "Epoch 153/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1109.1722 - val_loss: 1102.3778\n",
      "Epoch 154/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1117.8148 - val_loss: 1108.3752\n",
      "Epoch 155/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1103.6752 - val_loss: 1109.2845\n",
      "Epoch 156/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1115.7078 - val_loss: 1104.0246\n",
      "Epoch 157/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1107.8874 - val_loss: 1100.5159\n",
      "Epoch 158/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1112.8440 - val_loss: 1113.5960\n",
      "Epoch 159/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1105.4168 - val_loss: 1100.3390\n",
      "Epoch 160/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1101.6083 - val_loss: 1105.7742\n",
      "Epoch 161/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1106.1950 - val_loss: 1098.0293\n",
      "Epoch 162/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1107.2327 - val_loss: 1104.0315\n",
      "Epoch 163/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1103.1002 - val_loss: 1099.7386\n",
      "Epoch 164/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1104.4701 - val_loss: 1101.8725\n",
      "Epoch 165/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1099.5218 - val_loss: 1101.8611\n",
      "Epoch 166/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1106.5458 - val_loss: 1106.3864\n",
      "Epoch 167/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1108.5025 - val_loss: 1092.6008\n",
      "Epoch 168/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1095.9835 - val_loss: 1096.7116\n",
      "Epoch 169/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1104.8462 - val_loss: 1107.7853\n",
      "Epoch 170/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1101.3917 - val_loss: 1100.3317\n",
      "Epoch 171/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.3220 - val_loss: 1101.2572\n",
      "Epoch 172/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1098.9848 - val_loss: 1101.2376\n",
      "Epoch 173/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1096.6288 - val_loss: 1096.8616\n",
      "Epoch 174/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1098.8665 - val_loss: 1099.1326\n",
      "Epoch 175/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1094.7533 - val_loss: 1095.1618\n",
      "Epoch 176/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1092.6363 - val_loss: 1095.5623\n",
      "Epoch 177/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1099.3568 - val_loss: 1101.8331\n",
      "Epoch 178/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1094.7517 - val_loss: 1099.3504\n",
      "Epoch 179/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1105.3904 - val_loss: 1101.4520\n",
      "Epoch 180/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1093.5449 - val_loss: 1099.1122\n",
      "Epoch 181/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.8883 - val_loss: 1094.8943\n",
      "Epoch 182/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.5388 - val_loss: 1103.6997\n",
      "Epoch 183/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1094.2083 - val_loss: 1100.7850\n",
      "Epoch 184/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1088.7394 - val_loss: 1098.5143\n",
      "Epoch 185/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1089.7365 - val_loss: 1097.2167\n",
      "Epoch 186/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.7526 - val_loss: 1097.8138\n",
      "Epoch 187/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1091.7663 - val_loss: 1094.4930\n",
      "Epoch 188/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1089.0234 - val_loss: 1095.7774\n",
      "Epoch 189/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1096.7020 - val_loss: 1098.0533\n",
      "Epoch 190/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1089.0520 - val_loss: 1097.5655\n",
      "Epoch 191/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1098.0838 - val_loss: 1100.2971\n",
      "Epoch 192/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.3164 - val_loss: 1097.8012\n",
      "Epoch 193/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1083.4976 - val_loss: 1096.8706\n",
      "Epoch 194/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1091.5693 - val_loss: 1093.4119\n",
      "Epoch 195/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1085.7604 - val_loss: 1091.2357\n",
      "Epoch 196/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.7862 - val_loss: 1092.3613\n",
      "Epoch 197/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1085.1065 - val_loss: 1093.3472\n",
      "Epoch 198/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1080.5316 - val_loss: 1094.5073\n",
      "Epoch 199/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1075.3875 - val_loss: 1099.3507\n",
      "Epoch 200/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1079.3184 - val_loss: 1097.3032\n",
      "Epoch 201/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.8325 - val_loss: 1086.2357\n",
      "Epoch 202/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.7885 - val_loss: 1092.7492\n",
      "Epoch 203/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1082.9738 - val_loss: 1093.9966\n",
      "Epoch 204/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.2540 - val_loss: 1091.0481\n",
      "Epoch 205/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1080.0107 - val_loss: 1096.5929\n",
      "Epoch 206/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1077.5711 - val_loss: 1096.7779\n",
      "Epoch 207/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1081.4254 - val_loss: 1087.7464\n",
      "Epoch 208/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1076.7644 - val_loss: 1085.7544\n",
      "Epoch 209/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.0232 - val_loss: 1086.2027\n",
      "Epoch 210/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1075.1860 - val_loss: 1092.3493\n",
      "Epoch 211/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1073.6048 - val_loss: 1084.7346\n",
      "Epoch 212/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1078.4589 - val_loss: 1088.9995\n",
      "Epoch 213/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.0377 - val_loss: 1087.7657\n",
      "Epoch 214/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1074.1259 - val_loss: 1089.6979\n",
      "Epoch 215/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1073.0438 - val_loss: 1085.8602\n",
      "Epoch 216/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1069.6029 - val_loss: 1088.4962\n",
      "Epoch 217/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1079.5407 - val_loss: 1092.4877\n",
      "Epoch 218/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1069.6670 - val_loss: 1092.2363\n",
      "Epoch 219/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1076.1975 - val_loss: 1088.8018\n",
      "Epoch 220/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1071.0242 - val_loss: 1085.1493\n",
      "Epoch 221/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1069.7800 - val_loss: 1087.7727\n",
      "Epoch 222/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1068.6001 - val_loss: 1088.4943\n",
      "Epoch 223/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.0350 - val_loss: 1091.1181\n",
      "Epoch 224/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1080.2758 - val_loss: 1089.9955\n",
      "Epoch 225/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.6514 - val_loss: 1093.0104\n",
      "Epoch 226/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1082.2536 - val_loss: 1087.7780\n",
      "Epoch 227/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1076.1673 - val_loss: 1089.4311\n",
      "Epoch 228/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1074.6386 - val_loss: 1092.6947\n",
      "Epoch 229/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1070.5686 - val_loss: 1090.2853\n",
      "Epoch 230/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1070.0977 - val_loss: 1084.2384\n",
      "Epoch 231/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1065.4820 - val_loss: 1086.2786\n",
      "Epoch 232/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1066.0367 - val_loss: 1091.3852\n",
      "Epoch 233/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1074.6910 - val_loss: 1097.5967\n",
      "Epoch 234/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1071.6857 - val_loss: 1088.2849\n",
      "Epoch 235/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1074.9989 - val_loss: 1089.5529\n",
      "Epoch 236/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1060.0846 - val_loss: 1086.1546\n",
      "Epoch 237/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1065.6253 - val_loss: 1091.3485\n",
      "Epoch 238/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1060.2092 - val_loss: 1086.7716\n",
      "Epoch 239/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1073.5367 - val_loss: 1083.5543\n",
      "Epoch 240/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1068.6450 - val_loss: 1089.8350\n",
      "Epoch 241/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.4132 - val_loss: 1083.7648\n",
      "Epoch 242/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1058.5523 - val_loss: 1087.6375\n",
      "Epoch 243/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1066.0034 - val_loss: 1085.4240\n",
      "Epoch 244/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1066.6940 - val_loss: 1080.5247\n",
      "Epoch 245/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1055.4880 - val_loss: 1084.8566\n",
      "Epoch 246/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1063.4115 - val_loss: 1083.5987\n",
      "Epoch 247/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.8680 - val_loss: 1095.1377\n",
      "Epoch 248/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1063.4285 - val_loss: 1083.9684\n",
      "Epoch 249/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1066.7281 - val_loss: 1083.7339\n",
      "Epoch 250/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1071.7207 - val_loss: 1080.5616\n",
      "Epoch 251/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1058.4674 - val_loss: 1079.6714\n",
      "Epoch 252/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1067.8178 - val_loss: 1083.2639\n",
      "Epoch 253/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1058.0461 - val_loss: 1082.6835\n",
      "Epoch 254/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1049.0416 - val_loss: 1081.7728\n",
      "Epoch 255/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1060.6555 - val_loss: 1078.4692\n",
      "Epoch 256/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1060.9950 - val_loss: 1082.1213\n",
      "Epoch 257/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1059.1054 - val_loss: 1076.0935\n",
      "Epoch 258/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1060.8247 - val_loss: 1082.1962\n",
      "Epoch 259/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1061.3914 - val_loss: 1088.6413\n",
      "Epoch 260/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1057.7937 - val_loss: 1078.2023\n",
      "Epoch 261/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.8193 - val_loss: 1082.0822\n",
      "Epoch 262/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1053.6752 - val_loss: 1080.0130\n",
      "Epoch 263/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1060.1831 - val_loss: 1089.4708\n",
      "Epoch 264/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1054.3287 - val_loss: 1079.7782\n",
      "Epoch 265/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1059.5240 - val_loss: 1077.9140\n",
      "Epoch 266/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.0120 - val_loss: 1083.9268\n",
      "Epoch 267/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1056.7647 - val_loss: 1083.6305\n",
      "Epoch 268/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1057.6678 - val_loss: 1081.0535\n",
      "Epoch 269/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1056.0879 - val_loss: 1076.8147\n",
      "Epoch 270/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1055.9868 - val_loss: 1082.0397\n",
      "Epoch 271/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1052.7976 - val_loss: 1081.6051\n",
      "Epoch 272/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.2024 - val_loss: 1083.7360\n",
      "Epoch 273/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1052.3303 - val_loss: 1079.1762\n",
      "Epoch 274/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1050.6994 - val_loss: 1082.0782\n",
      "Epoch 275/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1049.7799 - val_loss: 1079.2557\n",
      "Epoch 276/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1042.2714 - val_loss: 1080.5887\n",
      "Epoch 277/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1047.5212 - val_loss: 1082.9982\n",
      "Epoch 278/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1047.4878 - val_loss: 1077.7551\n",
      "Epoch 279/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1055.8213 - val_loss: 1080.5026\n",
      "Epoch 280/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1050.5874 - val_loss: 1084.1258\n",
      "Epoch 281/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1057.0626 - val_loss: 1083.3686\n",
      "Epoch 282/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1059.1993 - val_loss: 1079.9126\n",
      "Epoch 283/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1049.3661 - val_loss: 1077.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1055.7779 - val_loss: 1076.7997\n",
      "Epoch 285/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.3570 - val_loss: 1076.7343\n",
      "Epoch 286/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1048.9203 - val_loss: 1084.3446\n",
      "Epoch 287/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1046.8679 - val_loss: 1073.3456\n",
      "Epoch 288/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1050.6687 - val_loss: 1077.4098\n",
      "Epoch 289/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1048.5415 - val_loss: 1083.4796\n",
      "Epoch 290/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1051.8813 - val_loss: 1076.3183\n",
      "Epoch 291/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1053.8248 - val_loss: 1077.0233\n",
      "Epoch 292/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1055.1272 - val_loss: 1080.3614\n",
      "Epoch 293/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1048.7420 - val_loss: 1075.0910\n",
      "Epoch 294/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1050.8621 - val_loss: 1077.6786\n",
      "Epoch 295/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1042.2555 - val_loss: 1075.0299\n",
      "Epoch 296/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1050.2470 - val_loss: 1076.3751\n",
      "Epoch 297/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1047.6746 - val_loss: 1084.3798\n",
      "Epoch 298/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1045.3850 - val_loss: 1073.6561\n",
      "Epoch 299/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1050.5658 - val_loss: 1076.9353\n",
      "Epoch 300/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1047.6280 - val_loss: 1079.6361\n",
      "Epoch 301/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1046.8511 - val_loss: 1078.5719\n",
      "Epoch 302/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1042.9338 - val_loss: 1075.6714\n",
      "Epoch 303/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1043.7836 - val_loss: 1075.1331\n",
      "Epoch 304/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1045.8016 - val_loss: 1072.7389\n",
      "Epoch 305/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1035.7242 - val_loss: 1075.2676\n",
      "Epoch 306/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1040.1327 - val_loss: 1076.3358\n",
      "Epoch 307/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1039.0981 - val_loss: 1072.6109\n",
      "Epoch 308/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1040.4760 - val_loss: 1072.8943\n",
      "Epoch 309/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1038.2528 - val_loss: 1082.5384\n",
      "Epoch 310/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1037.6637 - val_loss: 1077.2426\n",
      "Epoch 311/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1032.2667 - val_loss: 1073.0878\n",
      "Epoch 312/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1037.1072 - val_loss: 1076.9957\n",
      "Epoch 313/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1040.3423 - val_loss: 1072.2930\n",
      "Epoch 314/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1040.6864 - val_loss: 1074.9147\n",
      "Epoch 315/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1039.3279 - val_loss: 1073.2667\n",
      "Epoch 316/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1043.5453 - val_loss: 1079.3000\n",
      "Epoch 317/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1043.0192 - val_loss: 1079.4087\n",
      "Epoch 318/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1040.4101 - val_loss: 1072.2052\n",
      "Epoch 319/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1033.5006 - val_loss: 1067.7451\n",
      "Epoch 320/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1036.2847 - val_loss: 1073.8088\n",
      "Epoch 321/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1032.2816 - val_loss: 1079.1437\n",
      "Epoch 322/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1041.4653 - val_loss: 1071.8066\n",
      "Epoch 323/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1034.2911 - val_loss: 1073.5925\n",
      "Epoch 324/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1037.7277 - val_loss: 1071.6459\n",
      "Epoch 325/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1027.8050 - val_loss: 1073.1800\n",
      "Epoch 326/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1038.7056 - val_loss: 1074.7075\n",
      "Epoch 327/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1036.5017 - val_loss: 1069.5111\n",
      "Epoch 328/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1042.4970 - val_loss: 1071.7565\n",
      "Epoch 329/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1041.1949 - val_loss: 1071.3960\n",
      "Epoch 330/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1042.1789 - val_loss: 1071.3009\n",
      "Epoch 331/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1034.2974 - val_loss: 1070.3314\n",
      "Epoch 332/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1038.6284 - val_loss: 1069.4231\n",
      "Epoch 333/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1039.6905 - val_loss: 1072.1397\n",
      "Epoch 334/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1032.6177 - val_loss: 1071.1201\n",
      "Epoch 335/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1035.5333 - val_loss: 1073.5794\n",
      "Epoch 336/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1039.6341 - val_loss: 1074.9993\n",
      "Epoch 337/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1040.8236 - val_loss: 1073.7423\n",
      "Epoch 338/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1028.5599 - val_loss: 1067.8728\n",
      "Epoch 339/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1038.1488 - val_loss: 1071.4049\n",
      "Epoch 340/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1039.1799 - val_loss: 1078.8421\n",
      "Epoch 341/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1032.6197 - val_loss: 1076.6545\n",
      "Epoch 342/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1032.0885 - val_loss: 1074.5037\n",
      "Epoch 343/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1029.9613 - val_loss: 1078.7162\n",
      "Epoch 344/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1036.2199 - val_loss: 1066.2473\n",
      "Epoch 345/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1031.0728 - val_loss: 1073.5251\n",
      "Epoch 346/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1033.1165 - val_loss: 1076.9898\n",
      "Epoch 347/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1031.5090 - val_loss: 1070.5883\n",
      "Epoch 348/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1028.5131 - val_loss: 1066.5913\n",
      "Epoch 349/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1027.1641 - val_loss: 1066.8899\n",
      "Epoch 350/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1028.1115 - val_loss: 1073.2386\n",
      "Epoch 351/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1032.6555 - val_loss: 1069.7635\n",
      "Epoch 352/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1030.2118 - val_loss: 1074.2858\n",
      "Epoch 353/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1031.4084 - val_loss: 1072.1583\n",
      "Epoch 354/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.4894 - val_loss: 1070.5929\n",
      "Epoch 355/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1035.8107 - val_loss: 1072.0432\n",
      "Epoch 356/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1027.9213 - val_loss: 1070.3164\n",
      "Epoch 357/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1025.1526 - val_loss: 1078.2158\n",
      "Epoch 358/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1024.7813 - val_loss: 1075.0656\n",
      "Epoch 359/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1033.7411 - val_loss: 1071.1496\n",
      "Epoch 360/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1022.8825 - val_loss: 1070.2869\n",
      "Epoch 361/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1040.3852 - val_loss: 1065.6129\n",
      "Epoch 362/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1023.3456 - val_loss: 1070.0668\n",
      "Epoch 363/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1028.2705 - val_loss: 1067.5024\n",
      "Epoch 364/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1019.5449 - val_loss: 1066.9342\n",
      "Epoch 365/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1025.8041 - val_loss: 1068.3989\n",
      "Epoch 366/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1030.6188 - val_loss: 1069.5820\n",
      "Epoch 367/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1026.2477 - val_loss: 1070.2160\n",
      "Epoch 368/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1022.6025 - val_loss: 1070.4396\n",
      "Epoch 369/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1027.8856 - val_loss: 1076.5739\n",
      "Epoch 370/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1031.1045 - val_loss: 1072.3554\n",
      "Epoch 371/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1024.0567 - val_loss: 1074.1577\n",
      "Epoch 372/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1018.7142 - val_loss: 1067.3759\n",
      "Epoch 373/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1033.0188 - val_loss: 1070.5759\n",
      "Epoch 374/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1027.3155 - val_loss: 1067.9343\n",
      "Epoch 375/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1021.4900 - val_loss: 1064.4866\n",
      "Epoch 376/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1030.5263 - val_loss: 1064.3210\n",
      "Epoch 377/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1026.7668 - val_loss: 1065.6292\n",
      "Epoch 378/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1022.4423 - val_loss: 1066.8045\n",
      "Epoch 379/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1021.9672 - val_loss: 1070.4493\n",
      "Epoch 380/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1013.7303 - val_loss: 1062.9708\n",
      "Epoch 381/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1015.9349 - val_loss: 1063.8772\n",
      "Epoch 382/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1021.1475 - val_loss: 1067.7817\n",
      "Epoch 383/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1026.4083 - val_loss: 1064.4195\n",
      "Epoch 384/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1027.1437 - val_loss: 1064.9116\n",
      "Epoch 385/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1028.5734 - val_loss: 1069.2882\n",
      "Epoch 386/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1017.1592 - val_loss: 1068.3257\n",
      "Epoch 387/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1023.9964 - val_loss: 1074.0278\n",
      "Epoch 388/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1018.0035 - val_loss: 1064.6950\n",
      "Epoch 389/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.8607 - val_loss: 1065.4146\n",
      "Epoch 390/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1023.5371 - val_loss: 1068.0762\n",
      "Epoch 391/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1030.2239 - val_loss: 1066.8726\n",
      "Epoch 392/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1024.8107 - val_loss: 1062.3883\n",
      "Epoch 393/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1013.5896 - val_loss: 1062.8908\n",
      "Epoch 394/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1019.1798 - val_loss: 1062.4958\n",
      "Epoch 395/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1021.2433 - val_loss: 1063.6545\n",
      "Epoch 396/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1019.9591 - val_loss: 1063.8099\n",
      "Epoch 397/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1019.4605 - val_loss: 1064.3482\n",
      "Epoch 398/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1021.0884 - val_loss: 1066.3893\n",
      "Epoch 399/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1019.2256 - val_loss: 1067.1334\n",
      "Epoch 400/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1015.7815 - val_loss: 1068.4816\n"
     ]
    }
   ],
   "source": [
    "#Added learning rate decay to Adam optimizer\n",
    "optimizer = keras.optimizers.Adam(lr=0.005, decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=400, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1478.99966281962,
          1472.8891243453684,
          1454.5666171988576,
          1441.8253299888304,
          1421.9104675329686,
          1412.602749742103,
          1401.4311202290016,
          1390.39827893343,
          1375.1768305357602,
          1368.797414488608,
          1363.059952140213,
          1371.3599360534656,
          1353.9470804511652,
          1344.5592730307785,
          1333.3612518769503,
          1331.042146862115,
          1333.9522094359397,
          1321.0983241976012,
          1310.7240932870777,
          1306.817432673584,
          1312.1211035900371,
          1307.4930013102426,
          1300.9073942592538,
          1308.7683538234567,
          1292.1311209437506,
          1283.8275770421064,
          1284.881551775828,
          1282.5827207135035,
          1278.0270008907926,
          1270.5384348182226,
          1271.1845723196718,
          1270.763450253863,
          1268.2521297071605,
          1266.5519449935066,
          1252.9612493949112,
          1255.388608086102,
          1246.0041188633415,
          1256.4910132066027,
          1239.4938102738997,
          1237.3324542736898,
          1245.156792376977,
          1226.4494625479372,
          1237.4501617046117,
          1220.7017508461454,
          1233.4918862528903,
          1237.9194775557073,
          1234.1682545272004,
          1218.9413919794983,
          1223.8054163726079,
          1218.6093014689757,
          1223.0718765763643,
          1222.1706468801306,
          1213.259082574655,
          1208.4999148665436,
          1205.152996056348,
          1206.339488602985,
          1205.1062442771126,
          1207.7369549011023,
          1195.97549945777,
          1200.5442434019858,
          1207.0779058266337,
          1203.2417241370533,
          1192.2532986154627,
          1187.7563438866835,
          1190.1097604014578,
          1192.0336934623772,
          1181.2612990552782,
          1184.4124601160283,
          1188.5841929250425,
          1198.5130580231257,
          1187.7367831655272,
          1179.0635568004154,
          1179.984651451195,
          1180.2423232043939,
          1174.5526941336539,
          1166.7675162208852,
          1174.5552752831188,
          1168.8723756473864,
          1172.42971383801,
          1168.5506561934076,
          1169.4566281511618,
          1167.8999610168078,
          1163.7814648897681,
          1164.624303854083,
          1161.640697576394,
          1160.1164003215,
          1157.2493951069657,
          1162.252695827511,
          1154.6708740577062,
          1156.8918511616532,
          1155.064887310523,
          1154.5462585779667,
          1157.8408902942572,
          1154.8047621814765,
          1151.5263781779893,
          1152.2884359686748,
          1147.8908096108476,
          1151.4172552493906,
          1156.6572323881937,
          1144.29745840161,
          1147.7631249206922,
          1155.7009024733838,
          1147.5152852739388,
          1144.6150523587892,
          1141.8574135281028,
          1136.2449765875651,
          1142.4214262012874,
          1139.8607221549466,
          1141.0424918508827,
          1136.351213325545,
          1143.2770091642567,
          1140.7866129181966,
          1128.6994829868204,
          1129.1281653343615,
          1131.34028863228,
          1135.2569923989874,
          1134.8992981360914,
          1131.4894634986513,
          1128.521355793618,
          1132.171312086229,
          1131.6324039426336,
          1128.8726293098384,
          1127.5353686185645,
          1117.38288639819,
          1126.6394430206035,
          1126.4704297579958,
          1125.1154053028108,
          1120.65879164245,
          1126.4645836499117,
          1116.056772510872,
          1122.6624632491744,
          1120.086546823499,
          1117.7140481511697,
          1123.459524524506,
          1119.1808914358974,
          1123.7701000628979,
          1115.89886116011,
          1109.6548777064509,
          1118.5131645011404,
          1117.135768941631,
          1114.9051936157055,
          1112.210883575617,
          1109.1722258878945,
          1117.8147946379336,
          1103.6752200986236,
          1115.7078382799184,
          1107.8874131090445,
          1112.8440137962218,
          1105.4167868440368,
          1101.6082669187936,
          1106.1949647452518,
          1107.232744466668,
          1103.1002311086952,
          1104.4700591655492,
          1099.521798693204,
          1106.5457678979017,
          1108.5025473702328,
          1095.983519308405,
          1104.8462054074969,
          1101.39166618364,
          1100.3220336008387,
          1098.984785662449,
          1096.6287914006102,
          1098.8665246437613,
          1094.753266525192,
          1092.6362981006475,
          1099.3568123556795,
          1094.7517154954444,
          1105.39038144684,
          1093.5448644013497,
          1092.8882655451048,
          1092.5387568224257,
          1094.2082986810628,
          1088.7394004563232,
          1089.7364589828085,
          1090.7526468721019,
          1091.7663200688978,
          1089.0234140748369,
          1096.7019743765431,
          1089.0520398201338,
          1098.0837737688303,
          1092.3163533781583,
          1083.4975550934382,
          1091.5692917307656,
          1085.760357716195,
          1078.7862194390964,
          1085.1064766684394,
          1080.531554282345,
          1075.3874754292801,
          1079.3183515666124,
          1083.8324658761217,
          1078.7885354950215,
          1082.9738180352902,
          1086.254034415261,
          1080.0106626839156,
          1077.5710866318834,
          1081.4254419400597,
          1076.7643846290393,
          1083.0232231487414,
          1075.1860253406905,
          1073.604825231148,
          1078.4589345383936,
          1078.0377030572456,
          1074.1258742334562,
          1073.0438176367932,
          1069.6028555592195,
          1079.5407269839081,
          1069.6669835591092,
          1076.1974663176995,
          1071.0241559206281,
          1069.7799631581174,
          1068.6000784902074,
          1079.0349766326042,
          1080.2758114212186,
          1079.651402239573,
          1082.253630802391,
          1076.1673270914534,
          1074.6386487190914,
          1070.568592329314,
          1070.0977280920629,
          1065.4819737127077,
          1066.0366658634716,
          1074.6910442492276,
          1071.685679176993,
          1074.998949784087,
          1060.084618053815,
          1065.6252958620146,
          1060.2092478652505,
          1073.536702041519,
          1068.6449980887808,
          1061.4132009704724,
          1058.5522727010148,
          1066.0033874450523,
          1066.6940182903284,
          1055.4880164501924,
          1063.4114851690567,
          1061.8680016284527,
          1063.4285427389534,
          1066.728128240848,
          1071.7207295119654,
          1058.4674466601446,
          1067.8177963877959,
          1058.0460711521832,
          1049.0416140803982,
          1060.65547988246,
          1060.9949919350859,
          1059.10544422334,
          1060.8247131017208,
          1061.3914461290763,
          1057.7937250131592,
          1047.8193067111208,
          1053.6752497411237,
          1060.183122040156,
          1054.328683532262,
          1059.5240319655322,
          1060.0120043803342,
          1056.7646648130844,
          1057.6678371089051,
          1056.087926839764,
          1055.9867624573892,
          1052.7975756987698,
          1045.202410385655,
          1052.3303458866098,
          1050.69942519496,
          1049.7798957220414,
          1042.2713613011017,
          1047.5212320832993,
          1047.4878385951913,
          1055.821293958041,
          1050.5874162348473,
          1057.062635435139,
          1059.1993248608492,
          1049.366058429162,
          1055.777896138358,
          1047.3569777637679,
          1048.9202524463997,
          1046.867937496867,
          1050.6686816141892,
          1048.5415315171008,
          1051.8813495655108,
          1053.8248443266948,
          1055.1271740852387,
          1048.7420365704738,
          1050.8621114605005,
          1042.2555217173049,
          1050.2469738213888,
          1047.6745589558243,
          1045.3850249662796,
          1050.5657679693766,
          1047.6279530637078,
          1046.8511337240498,
          1042.9338450993089,
          1043.7836331125966,
          1045.801625828717,
          1035.7241870856415,
          1040.1326654869065,
          1039.0981408595944,
          1040.4759667371493,
          1038.2528469039032,
          1037.6636731576318,
          1032.2667474985744,
          1037.1071668662742,
          1040.3422615352652,
          1040.6863901196823,
          1039.3278765268212,
          1043.5453349240252,
          1043.019188855869,
          1040.410112092221,
          1033.5005952855354,
          1036.2846651782918,
          1032.2816148279471,
          1041.4653075535455,
          1034.2911160692606,
          1037.7276586850421,
          1027.804989310097,
          1038.705642287134,
          1036.5016645940577,
          1042.496960701339,
          1041.1949375749996,
          1042.17894788168,
          1034.2974278044792,
          1038.6284322882073,
          1039.6905149433449,
          1032.6176941272895,
          1035.533308011964,
          1039.6341494844603,
          1040.8235570197357,
          1028.5598808366597,
          1038.1487591370376,
          1039.179914249216,
          1032.6196973214342,
          1032.0884635256746,
          1029.9612989622628,
          1036.2198940879098,
          1031.072786559508,
          1033.1165130168515,
          1031.508961042852,
          1028.5130576070048,
          1027.1641023619418,
          1028.1115119280832,
          1032.655545409275,
          1030.211787390379,
          1031.4084050660624,
          1039.4894240528304,
          1035.8107391228914,
          1027.9213466732253,
          1025.152620543883,
          1024.7813283164157,
          1033.7410999555877,
          1022.882513933199,
          1040.3851883549573,
          1023.3456203169255,
          1028.2704501919836,
          1019.5449153761695,
          1025.8041287841552,
          1030.6187668504517,
          1026.247722031383,
          1022.6024886751452,
          1027.8856225894356,
          1031.104503447244,
          1024.0566809397794,
          1018.7141823624236,
          1033.0188339046756,
          1027.3155394210494,
          1021.4899664420462,
          1030.5262886140686,
          1026.7667908975445,
          1022.4422768729947,
          1021.9671788985346,
          1013.7303020201156,
          1015.9348954988047,
          1021.1474773865175,
          1026.4082830202276,
          1027.1437040504532,
          1028.5733753584516,
          1017.1591946311386,
          1023.9964300247049,
          1018.0035089523777,
          1023.8607117274445,
          1023.5371281616382,
          1030.2238516431785,
          1024.8107044869396,
          1013.5896459701473,
          1019.1798487346398,
          1021.2432706629032,
          1019.9590836810473,
          1019.4605342298179,
          1021.0883898477839,
          1019.2255620472604,
          1015.7815078359007
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          3176.4033021989985,
          2925.233001849731,
          2240.1676048644445,
          1949.4417072523327,
          1759.7442377770092,
          1534.883931547218,
          1512.453470209067,
          1465.2659018917936,
          1448.2784908241897,
          1332.5446000666186,
          1374.624598051614,
          1247.3151499807702,
          1291.7591988437516,
          1308.7781487237912,
          1304.6395950271487,
          1231.294219193536,
          1242.1768180521308,
          1320.4539813035376,
          1230.5341558706934,
          1195.4961390582312,
          1207.916859542819,
          1203.8857345015008,
          1200.155897153889,
          1193.738374705876,
          1221.5822289564196,
          1223.2765396084508,
          1243.454296992493,
          1199.3603764563254,
          1227.0352329386483,
          1186.7755000158616,
          1205.7774465808368,
          1195.8362696869283,
          1265.7326805553814,
          1199.3532496502626,
          1206.9446388980489,
          1247.035688615596,
          1196.9380797789095,
          1182.9022788596053,
          1181.0256115118057,
          1170.4182651994795,
          1174.578333843775,
          1165.646901989117,
          1173.5118045198767,
          1167.2412985921599,
          1169.8997073543558,
          1168.1986884650091,
          1167.8557190834842,
          1166.2842599254154,
          1161.1527893421332,
          1171.844645859508,
          1160.7840406296605,
          1156.5714109614685,
          1162.535514799416,
          1155.5767586055777,
          1156.2761384188734,
          1153.3959773438282,
          1146.3800950341983,
          1151.8381440916305,
          1147.0857179349903,
          1161.9751535633272,
          1166.1350153710193,
          1173.7623299093268,
          1148.6614842633817,
          1150.8936799154365,
          1156.0837613096785,
          1156.8406039051142,
          1144.415112862779,
          1150.8254700747334,
          1154.2351165765365,
          1140.9980712792715,
          1144.3240970223753,
          1145.4418636893422,
          1139.479655306923,
          1140.7573425035953,
          1130.5320366644683,
          1151.3620174171597,
          1137.593902936698,
          1134.2736022349516,
          1130.5406746016204,
          1150.8820362140784,
          1149.918554868635,
          1139.0604566012257,
          1140.016518974725,
          1138.4549264856205,
          1120.4373024404467,
          1129.1393048194839,
          1132.6673424634519,
          1129.9864804987108,
          1136.8659857915736,
          1127.3669268369244,
          1126.751854210596,
          1127.5028863374853,
          1131.3541947833899,
          1130.0401218705745,
          1129.7625575275013,
          1125.9748276182136,
          1124.804712516214,
          1143.2182076964561,
          1124.052827830876,
          1118.571376227606,
          1126.5929502644376,
          1131.119084473146,
          1131.1155723386664,
          1126.2678044458562,
          1120.9894804372227,
          1122.8627089367903,
          1117.9967257888088,
          1125.7287640002678,
          1118.081008791804,
          1117.9243264910642,
          1118.5668917163139,
          1120.8251517421859,
          1121.8295527110722,
          1111.7081850066031,
          1113.6792553216487,
          1117.9399384444478,
          1120.6955025204202,
          1110.2655647358915,
          1114.9713466193743,
          1118.3960499447,
          1107.210761382916,
          1120.0576056095458,
          1108.6467762226712,
          1109.9262174671724,
          1111.6981317930527,
          1107.6813922497322,
          1108.9066794123705,
          1111.7729582755007,
          1111.169140081595,
          1113.5757335772228,
          1112.8442748754183,
          1115.5195680644676,
          1108.536196944086,
          1109.475051515777,
          1111.0172487774855,
          1104.195680571243,
          1110.858793116008,
          1109.1257966513717,
          1106.3681520684247,
          1107.2844837495456,
          1106.6571230462876,
          1101.893931864449,
          1102.3778421062732,
          1108.3751677701869,
          1109.2845452375395,
          1104.0245600964852,
          1100.5159495498845,
          1113.5960256187573,
          1100.3390347129098,
          1105.7741579277997,
          1098.029332832748,
          1104.0315431449894,
          1099.738591088784,
          1101.872508879532,
          1101.861139744586,
          1106.3863637008958,
          1092.600806873261,
          1096.7115724761334,
          1107.785286153178,
          1100.3317174213503,
          1101.2571979382724,
          1101.2376202785445,
          1096.8615889838973,
          1099.1325992991935,
          1095.1618487364021,
          1095.562334236985,
          1101.8331327956594,
          1099.3504459739854,
          1101.4520126841508,
          1099.112156646343,
          1094.894320986518,
          1103.6997006670467,
          1100.7850490621317,
          1098.51434324948,
          1097.2166592368674,
          1097.8138449764308,
          1094.4930387611305,
          1095.7774002934975,
          1098.0533440901422,
          1097.5654552667013,
          1100.297057089644,
          1097.8011901255766,
          1096.8706183685003,
          1093.4118551985162,
          1091.2357034539802,
          1092.3612858520044,
          1093.347170122788,
          1094.5072566358078,
          1099.3507447733248,
          1097.3031542165882,
          1086.2357418839765,
          1092.749208586857,
          1093.9966001447513,
          1091.0481395182162,
          1096.5929194714852,
          1096.7778543793943,
          1087.7464418474362,
          1085.7543863556966,
          1086.2027415812363,
          1092.3493311369325,
          1084.7346068283396,
          1088.999546844256,
          1087.7657338033966,
          1089.6978583036598,
          1085.8602266772514,
          1088.496206494307,
          1092.4876639222916,
          1092.2362832818453,
          1088.8018065720873,
          1085.1493290024769,
          1087.7727120542709,
          1088.4943281976498,
          1091.1180569235873,
          1089.995510274957,
          1093.0103653532387,
          1087.7779683231088,
          1089.431099643252,
          1092.694660781117,
          1090.2853493056557,
          1084.2383802378372,
          1086.278550794566,
          1091.3852471748812,
          1097.5967209079545,
          1088.2848588703675,
          1089.552858305618,
          1086.1545918313395,
          1091.3485007308063,
          1086.771561993801,
          1083.5543114490254,
          1089.8350210302647,
          1083.7647882051929,
          1087.6375274444024,
          1085.4239614061012,
          1080.5247177573801,
          1084.8566104390948,
          1083.5987345663177,
          1095.1377031453655,
          1083.968427579676,
          1083.733867258975,
          1080.561581253838,
          1079.6713812602218,
          1083.26387501547,
          1082.6834606891982,
          1081.7727890611293,
          1078.469235221537,
          1082.121302348998,
          1076.093483168543,
          1082.196235554238,
          1088.6412898389328,
          1078.2022584403041,
          1082.082179462501,
          1080.0130313179502,
          1089.4708113654094,
          1079.7781967490475,
          1077.9140156251958,
          1083.926811908227,
          1083.6305471863564,
          1081.0534847145548,
          1076.8146731795637,
          1082.0397497849879,
          1081.6050647699644,
          1083.7360429597422,
          1079.1762322761838,
          1082.0782185782643,
          1079.2556817158204,
          1080.5887032603127,
          1082.9981748199616,
          1077.7551411012382,
          1080.5025837196242,
          1084.1257960883845,
          1083.3685565624921,
          1079.9126362331124,
          1077.0552852700223,
          1076.7996589227696,
          1076.7343343914881,
          1084.344567579809,
          1073.3455895974064,
          1077.4098443061334,
          1083.479593990273,
          1076.3183410411993,
          1077.0233113174331,
          1080.3614194757934,
          1075.0910229878934,
          1077.6786487641305,
          1075.0299046319258,
          1076.3750660653245,
          1084.3797547941053,
          1073.65606360228,
          1076.9353147284503,
          1079.6360769690648,
          1078.571857361366,
          1075.671387306215,
          1075.1330568364467,
          1072.738860906521,
          1075.2675892134002,
          1076.3357990051477,
          1072.6109065454948,
          1072.89430806229,
          1082.5384352588214,
          1077.2425529521286,
          1073.0877837308642,
          1076.9956674952532,
          1072.2930045608816,
          1074.9147493199114,
          1073.2667286752196,
          1079.300009810664,
          1079.4086820068114,
          1072.2052212216606,
          1067.745129793518,
          1073.8087567225568,
          1079.1437312941382,
          1071.8065633488868,
          1073.5925266288052,
          1071.645946966997,
          1073.1800091370376,
          1074.7075498101706,
          1069.51110869217,
          1071.7564874728982,
          1071.3959877958082,
          1071.3009168320054,
          1070.3314093694387,
          1069.4231161420082,
          1072.1396783002995,
          1071.1201056095458,
          1073.5793541234555,
          1074.9993449031936,
          1073.7423088359815,
          1067.87275426852,
          1071.4049123913583,
          1078.8421163579997,
          1076.654482685255,
          1074.5037380636923,
          1078.7162291146433,
          1066.247284786147,
          1073.525110355283,
          1076.9898315454166,
          1070.5882724771907,
          1066.5912822802177,
          1066.8898868013866,
          1073.238574860066,
          1069.7635320828294,
          1074.285821235799,
          1072.1582938638508,
          1070.5928884582336,
          1072.0431882170988,
          1070.3163773663086,
          1078.2158124796345,
          1075.0656338315557,
          1071.1496449362091,
          1070.2868634475026,
          1065.612860820947,
          1070.0668035965778,
          1067.5023658435762,
          1066.9341963421302,
          1068.3988988068197,
          1069.5820362679294,
          1070.2159729554655,
          1070.4396094151434,
          1076.5739487364413,
          1072.3553526277697,
          1074.1576659862517,
          1067.37589338726,
          1070.575866026082,
          1067.9342609143148,
          1064.4865948340687,
          1064.3209635579851,
          1065.6291978284164,
          1066.804478558314,
          1070.4493326937145,
          1062.9708286956243,
          1063.8771735467294,
          1067.7817000225978,
          1064.4195357147332,
          1064.911611327146,
          1069.2882386049432,
          1068.3256526294538,
          1074.027774533122,
          1064.6950412136007,
          1065.4145708529677,
          1068.0761646540773,
          1066.8726055909426,
          1062.3882993732925,
          1062.8908260353874,
          1062.4958231979317,
          1063.6544944835086,
          1063.8099005656895,
          1064.3481616411655,
          1066.3892838176132,
          1067.1334001362527,
          1068.4816212092849
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256,
          1090.821307445256
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          399
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"b24a578a-bda5-4187-b7b1-1c9034dc8ab2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"b24a578a-bda5-4187-b7b1-1c9034dc8ab2\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'b24a578a-bda5-4187-b7b1-1c9034dc8ab2',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1478.99966281962, 1472.8891243453684, 1454.5666171988576, 1441.8253299888304, 1421.9104675329686, 1412.602749742103, 1401.4311202290016, 1390.39827893343, 1375.1768305357602, 1368.797414488608, 1363.059952140213, 1371.3599360534656, 1353.9470804511652, 1344.5592730307785, 1333.3612518769503, 1331.042146862115, 1333.9522094359397, 1321.0983241976012, 1310.7240932870777, 1306.817432673584, 1312.1211035900371, 1307.4930013102426, 1300.9073942592538, 1308.7683538234567, 1292.1311209437506, 1283.8275770421064, 1284.881551775828, 1282.5827207135035, 1278.0270008907926, 1270.5384348182226, 1271.1845723196718, 1270.763450253863, 1268.2521297071605, 1266.5519449935066, 1252.9612493949112, 1255.388608086102, 1246.0041188633415, 1256.4910132066027, 1239.4938102738997, 1237.3324542736898, 1245.156792376977, 1226.4494625479372, 1237.4501617046117, 1220.7017508461454, 1233.4918862528903, 1237.9194775557073, 1234.1682545272004, 1218.9413919794983, 1223.8054163726079, 1218.6093014689757, 1223.0718765763643, 1222.1706468801306, 1213.259082574655, 1208.4999148665436, 1205.152996056348, 1206.339488602985, 1205.1062442771126, 1207.7369549011023, 1195.97549945777, 1200.5442434019858, 1207.0779058266337, 1203.2417241370533, 1192.2532986154627, 1187.7563438866835, 1190.1097604014578, 1192.0336934623772, 1181.2612990552782, 1184.4124601160283, 1188.5841929250425, 1198.5130580231257, 1187.7367831655272, 1179.0635568004154, 1179.984651451195, 1180.2423232043939, 1174.5526941336539, 1166.7675162208852, 1174.5552752831188, 1168.8723756473864, 1172.42971383801, 1168.5506561934076, 1169.4566281511618, 1167.8999610168078, 1163.7814648897681, 1164.624303854083, 1161.640697576394, 1160.1164003215, 1157.2493951069657, 1162.252695827511, 1154.6708740577062, 1156.8918511616532, 1155.064887310523, 1154.5462585779667, 1157.8408902942572, 1154.8047621814765, 1151.5263781779893, 1152.2884359686748, 1147.8908096108476, 1151.4172552493906, 1156.6572323881937, 1144.29745840161, 1147.7631249206922, 1155.7009024733838, 1147.5152852739388, 1144.6150523587892, 1141.8574135281028, 1136.2449765875651, 1142.4214262012874, 1139.8607221549466, 1141.0424918508827, 1136.351213325545, 1143.2770091642567, 1140.7866129181966, 1128.6994829868204, 1129.1281653343615, 1131.34028863228, 1135.2569923989874, 1134.8992981360914, 1131.4894634986513, 1128.521355793618, 1132.171312086229, 1131.6324039426336, 1128.8726293098384, 1127.5353686185645, 1117.38288639819, 1126.6394430206035, 1126.4704297579958, 1125.1154053028108, 1120.65879164245, 1126.4645836499117, 1116.056772510872, 1122.6624632491744, 1120.086546823499, 1117.7140481511697, 1123.459524524506, 1119.1808914358974, 1123.7701000628979, 1115.89886116011, 1109.6548777064509, 1118.5131645011404, 1117.135768941631, 1114.9051936157055, 1112.210883575617, 1109.1722258878945, 1117.8147946379336, 1103.6752200986236, 1115.7078382799184, 1107.8874131090445, 1112.8440137962218, 1105.4167868440368, 1101.6082669187936, 1106.1949647452518, 1107.232744466668, 1103.1002311086952, 1104.4700591655492, 1099.521798693204, 1106.5457678979017, 1108.5025473702328, 1095.983519308405, 1104.8462054074969, 1101.39166618364, 1100.3220336008387, 1098.984785662449, 1096.6287914006102, 1098.8665246437613, 1094.753266525192, 1092.6362981006475, 1099.3568123556795, 1094.7517154954444, 1105.39038144684, 1093.5448644013497, 1092.8882655451048, 1092.5387568224257, 1094.2082986810628, 1088.7394004563232, 1089.7364589828085, 1090.7526468721019, 1091.7663200688978, 1089.0234140748369, 1096.7019743765431, 1089.0520398201338, 1098.0837737688303, 1092.3163533781583, 1083.4975550934382, 1091.5692917307656, 1085.760357716195, 1078.7862194390964, 1085.1064766684394, 1080.531554282345, 1075.3874754292801, 1079.3183515666124, 1083.8324658761217, 1078.7885354950215, 1082.9738180352902, 1086.254034415261, 1080.0106626839156, 1077.5710866318834, 1081.4254419400597, 1076.7643846290393, 1083.0232231487414, 1075.1860253406905, 1073.604825231148, 1078.4589345383936, 1078.0377030572456, 1074.1258742334562, 1073.0438176367932, 1069.6028555592195, 1079.5407269839081, 1069.6669835591092, 1076.1974663176995, 1071.0241559206281, 1069.7799631581174, 1068.6000784902074, 1079.0349766326042, 1080.2758114212186, 1079.651402239573, 1082.253630802391, 1076.1673270914534, 1074.6386487190914, 1070.568592329314, 1070.0977280920629, 1065.4819737127077, 1066.0366658634716, 1074.6910442492276, 1071.685679176993, 1074.998949784087, 1060.084618053815, 1065.6252958620146, 1060.2092478652505, 1073.536702041519, 1068.6449980887808, 1061.4132009704724, 1058.5522727010148, 1066.0033874450523, 1066.6940182903284, 1055.4880164501924, 1063.4114851690567, 1061.8680016284527, 1063.4285427389534, 1066.728128240848, 1071.7207295119654, 1058.4674466601446, 1067.8177963877959, 1058.0460711521832, 1049.0416140803982, 1060.65547988246, 1060.9949919350859, 1059.10544422334, 1060.8247131017208, 1061.3914461290763, 1057.7937250131592, 1047.8193067111208, 1053.6752497411237, 1060.183122040156, 1054.328683532262, 1059.5240319655322, 1060.0120043803342, 1056.7646648130844, 1057.6678371089051, 1056.087926839764, 1055.9867624573892, 1052.7975756987698, 1045.202410385655, 1052.3303458866098, 1050.69942519496, 1049.7798957220414, 1042.2713613011017, 1047.5212320832993, 1047.4878385951913, 1055.821293958041, 1050.5874162348473, 1057.062635435139, 1059.1993248608492, 1049.366058429162, 1055.777896138358, 1047.3569777637679, 1048.9202524463997, 1046.867937496867, 1050.6686816141892, 1048.5415315171008, 1051.8813495655108, 1053.8248443266948, 1055.1271740852387, 1048.7420365704738, 1050.8621114605005, 1042.2555217173049, 1050.2469738213888, 1047.6745589558243, 1045.3850249662796, 1050.5657679693766, 1047.6279530637078, 1046.8511337240498, 1042.9338450993089, 1043.7836331125966, 1045.801625828717, 1035.7241870856415, 1040.1326654869065, 1039.0981408595944, 1040.4759667371493, 1038.2528469039032, 1037.6636731576318, 1032.2667474985744, 1037.1071668662742, 1040.3422615352652, 1040.6863901196823, 1039.3278765268212, 1043.5453349240252, 1043.019188855869, 1040.410112092221, 1033.5005952855354, 1036.2846651782918, 1032.2816148279471, 1041.4653075535455, 1034.2911160692606, 1037.7276586850421, 1027.804989310097, 1038.705642287134, 1036.5016645940577, 1042.496960701339, 1041.1949375749996, 1042.17894788168, 1034.2974278044792, 1038.6284322882073, 1039.6905149433449, 1032.6176941272895, 1035.533308011964, 1039.6341494844603, 1040.8235570197357, 1028.5598808366597, 1038.1487591370376, 1039.179914249216, 1032.6196973214342, 1032.0884635256746, 1029.9612989622628, 1036.2198940879098, 1031.072786559508, 1033.1165130168515, 1031.508961042852, 1028.5130576070048, 1027.1641023619418, 1028.1115119280832, 1032.655545409275, 1030.211787390379, 1031.4084050660624, 1039.4894240528304, 1035.8107391228914, 1027.9213466732253, 1025.152620543883, 1024.7813283164157, 1033.7410999555877, 1022.882513933199, 1040.3851883549573, 1023.3456203169255, 1028.2704501919836, 1019.5449153761695, 1025.8041287841552, 1030.6187668504517, 1026.247722031383, 1022.6024886751452, 1027.8856225894356, 1031.104503447244, 1024.0566809397794, 1018.7141823624236, 1033.0188339046756, 1027.3155394210494, 1021.4899664420462, 1030.5262886140686, 1026.7667908975445, 1022.4422768729947, 1021.9671788985346, 1013.7303020201156, 1015.9348954988047, 1021.1474773865175, 1026.4082830202276, 1027.1437040504532, 1028.5733753584516, 1017.1591946311386, 1023.9964300247049, 1018.0035089523777, 1023.8607117274445, 1023.5371281616382, 1030.2238516431785, 1024.8107044869396, 1013.5896459701473, 1019.1798487346398, 1021.2432706629032, 1019.9590836810473, 1019.4605342298179, 1021.0883898477839, 1019.2255620472604, 1015.7815078359007]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [3176.4033021989985, 2925.233001849731, 2240.1676048644445, 1949.4417072523327, 1759.7442377770092, 1534.883931547218, 1512.453470209067, 1465.2659018917936, 1448.2784908241897, 1332.5446000666186, 1374.624598051614, 1247.3151499807702, 1291.7591988437516, 1308.7781487237912, 1304.6395950271487, 1231.294219193536, 1242.1768180521308, 1320.4539813035376, 1230.5341558706934, 1195.4961390582312, 1207.916859542819, 1203.8857345015008, 1200.155897153889, 1193.738374705876, 1221.5822289564196, 1223.2765396084508, 1243.454296992493, 1199.3603764563254, 1227.0352329386483, 1186.7755000158616, 1205.7774465808368, 1195.8362696869283, 1265.7326805553814, 1199.3532496502626, 1206.9446388980489, 1247.035688615596, 1196.9380797789095, 1182.9022788596053, 1181.0256115118057, 1170.4182651994795, 1174.578333843775, 1165.646901989117, 1173.5118045198767, 1167.2412985921599, 1169.8997073543558, 1168.1986884650091, 1167.8557190834842, 1166.2842599254154, 1161.1527893421332, 1171.844645859508, 1160.7840406296605, 1156.5714109614685, 1162.535514799416, 1155.5767586055777, 1156.2761384188734, 1153.3959773438282, 1146.3800950341983, 1151.8381440916305, 1147.0857179349903, 1161.9751535633272, 1166.1350153710193, 1173.7623299093268, 1148.6614842633817, 1150.8936799154365, 1156.0837613096785, 1156.8406039051142, 1144.415112862779, 1150.8254700747334, 1154.2351165765365, 1140.9980712792715, 1144.3240970223753, 1145.4418636893422, 1139.479655306923, 1140.7573425035953, 1130.5320366644683, 1151.3620174171597, 1137.593902936698, 1134.2736022349516, 1130.5406746016204, 1150.8820362140784, 1149.918554868635, 1139.0604566012257, 1140.016518974725, 1138.4549264856205, 1120.4373024404467, 1129.1393048194839, 1132.6673424634519, 1129.9864804987108, 1136.8659857915736, 1127.3669268369244, 1126.751854210596, 1127.5028863374853, 1131.3541947833899, 1130.0401218705745, 1129.7625575275013, 1125.9748276182136, 1124.804712516214, 1143.2182076964561, 1124.052827830876, 1118.571376227606, 1126.5929502644376, 1131.119084473146, 1131.1155723386664, 1126.2678044458562, 1120.9894804372227, 1122.8627089367903, 1117.9967257888088, 1125.7287640002678, 1118.081008791804, 1117.9243264910642, 1118.5668917163139, 1120.8251517421859, 1121.8295527110722, 1111.7081850066031, 1113.6792553216487, 1117.9399384444478, 1120.6955025204202, 1110.2655647358915, 1114.9713466193743, 1118.3960499447, 1107.210761382916, 1120.0576056095458, 1108.6467762226712, 1109.9262174671724, 1111.6981317930527, 1107.6813922497322, 1108.9066794123705, 1111.7729582755007, 1111.169140081595, 1113.5757335772228, 1112.8442748754183, 1115.5195680644676, 1108.536196944086, 1109.475051515777, 1111.0172487774855, 1104.195680571243, 1110.858793116008, 1109.1257966513717, 1106.3681520684247, 1107.2844837495456, 1106.6571230462876, 1101.893931864449, 1102.3778421062732, 1108.3751677701869, 1109.2845452375395, 1104.0245600964852, 1100.5159495498845, 1113.5960256187573, 1100.3390347129098, 1105.7741579277997, 1098.029332832748, 1104.0315431449894, 1099.738591088784, 1101.872508879532, 1101.861139744586, 1106.3863637008958, 1092.600806873261, 1096.7115724761334, 1107.785286153178, 1100.3317174213503, 1101.2571979382724, 1101.2376202785445, 1096.8615889838973, 1099.1325992991935, 1095.1618487364021, 1095.562334236985, 1101.8331327956594, 1099.3504459739854, 1101.4520126841508, 1099.112156646343, 1094.894320986518, 1103.6997006670467, 1100.7850490621317, 1098.51434324948, 1097.2166592368674, 1097.8138449764308, 1094.4930387611305, 1095.7774002934975, 1098.0533440901422, 1097.5654552667013, 1100.297057089644, 1097.8011901255766, 1096.8706183685003, 1093.4118551985162, 1091.2357034539802, 1092.3612858520044, 1093.347170122788, 1094.5072566358078, 1099.3507447733248, 1097.3031542165882, 1086.2357418839765, 1092.749208586857, 1093.9966001447513, 1091.0481395182162, 1096.5929194714852, 1096.7778543793943, 1087.7464418474362, 1085.7543863556966, 1086.2027415812363, 1092.3493311369325, 1084.7346068283396, 1088.999546844256, 1087.7657338033966, 1089.6978583036598, 1085.8602266772514, 1088.496206494307, 1092.4876639222916, 1092.2362832818453, 1088.8018065720873, 1085.1493290024769, 1087.7727120542709, 1088.4943281976498, 1091.1180569235873, 1089.995510274957, 1093.0103653532387, 1087.7779683231088, 1089.431099643252, 1092.694660781117, 1090.2853493056557, 1084.2383802378372, 1086.278550794566, 1091.3852471748812, 1097.5967209079545, 1088.2848588703675, 1089.552858305618, 1086.1545918313395, 1091.3485007308063, 1086.771561993801, 1083.5543114490254, 1089.8350210302647, 1083.7647882051929, 1087.6375274444024, 1085.4239614061012, 1080.5247177573801, 1084.8566104390948, 1083.5987345663177, 1095.1377031453655, 1083.968427579676, 1083.733867258975, 1080.561581253838, 1079.6713812602218, 1083.26387501547, 1082.6834606891982, 1081.7727890611293, 1078.469235221537, 1082.121302348998, 1076.093483168543, 1082.196235554238, 1088.6412898389328, 1078.2022584403041, 1082.082179462501, 1080.0130313179502, 1089.4708113654094, 1079.7781967490475, 1077.9140156251958, 1083.926811908227, 1083.6305471863564, 1081.0534847145548, 1076.8146731795637, 1082.0397497849879, 1081.6050647699644, 1083.7360429597422, 1079.1762322761838, 1082.0782185782643, 1079.2556817158204, 1080.5887032603127, 1082.9981748199616, 1077.7551411012382, 1080.5025837196242, 1084.1257960883845, 1083.3685565624921, 1079.9126362331124, 1077.0552852700223, 1076.7996589227696, 1076.7343343914881, 1084.344567579809, 1073.3455895974064, 1077.4098443061334, 1083.479593990273, 1076.3183410411993, 1077.0233113174331, 1080.3614194757934, 1075.0910229878934, 1077.6786487641305, 1075.0299046319258, 1076.3750660653245, 1084.3797547941053, 1073.65606360228, 1076.9353147284503, 1079.6360769690648, 1078.571857361366, 1075.671387306215, 1075.1330568364467, 1072.738860906521, 1075.2675892134002, 1076.3357990051477, 1072.6109065454948, 1072.89430806229, 1082.5384352588214, 1077.2425529521286, 1073.0877837308642, 1076.9956674952532, 1072.2930045608816, 1074.9147493199114, 1073.2667286752196, 1079.300009810664, 1079.4086820068114, 1072.2052212216606, 1067.745129793518, 1073.8087567225568, 1079.1437312941382, 1071.8065633488868, 1073.5925266288052, 1071.645946966997, 1073.1800091370376, 1074.7075498101706, 1069.51110869217, 1071.7564874728982, 1071.3959877958082, 1071.3009168320054, 1070.3314093694387, 1069.4231161420082, 1072.1396783002995, 1071.1201056095458, 1073.5793541234555, 1074.9993449031936, 1073.7423088359815, 1067.87275426852, 1071.4049123913583, 1078.8421163579997, 1076.654482685255, 1074.5037380636923, 1078.7162291146433, 1066.247284786147, 1073.525110355283, 1076.9898315454166, 1070.5882724771907, 1066.5912822802177, 1066.8898868013866, 1073.238574860066, 1069.7635320828294, 1074.285821235799, 1072.1582938638508, 1070.5928884582336, 1072.0431882170988, 1070.3163773663086, 1078.2158124796345, 1075.0656338315557, 1071.1496449362091, 1070.2868634475026, 1065.612860820947, 1070.0668035965778, 1067.5023658435762, 1066.9341963421302, 1068.3988988068197, 1069.5820362679294, 1070.2159729554655, 1070.4396094151434, 1076.5739487364413, 1072.3553526277697, 1074.1576659862517, 1067.37589338726, 1070.575866026082, 1067.9342609143148, 1064.4865948340687, 1064.3209635579851, 1065.6291978284164, 1066.804478558314, 1070.4493326937145, 1062.9708286956243, 1063.8771735467294, 1067.7817000225978, 1064.4195357147332, 1064.911611327146, 1069.2882386049432, 1068.3256526294538, 1074.027774533122, 1064.6950412136007, 1065.4145708529677, 1068.0761646540773, 1066.8726055909426, 1062.3882993732925, 1062.8908260353874, 1062.4958231979317, 1063.6544944835086, 1063.8099005656895, 1064.3481616411655, 1066.3892838176132, 1067.1334001362527, 1068.4816212092849]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256, 1090.821307445256]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 399], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('b24a578a-bda5-4187-b7b1-1c9034dc8ab2');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.05% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1068   : Mean absolute error \n",
      "\n",
      "9.21% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Callbacks_stop\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Callbacks\n",
    "# Setting weights saving convention\n",
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
    "# Defining checkpoint behaviour\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 11174.9780\n",
      "Epoch 00001: val_loss improved from inf to 10216.48293, saving model to Weights\\Weights-001--10216.48293.hdf5\n",
      "19948/19948 [==============================] - 3s 143us/sample - loss: 11171.3892 - val_loss: 10216.4829\n",
      "Epoch 2/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 11008.7835\n",
      "Epoch 00002: val_loss improved from 10216.48293 to 7990.07066, saving model to Weights\\Weights-002--7990.07066.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 10988.2583 - val_loss: 7990.0707\n",
      "Epoch 3/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 10423.5730\n",
      "Epoch 00003: val_loss improved from 7990.07066 to 6717.19043, saving model to Weights\\Weights-003--6717.19043.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 10370.8623 - val_loss: 6717.1904\n",
      "Epoch 4/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 9015.2763\n",
      "Epoch 00004: val_loss did not improve from 6717.19043\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 8942.5752 - val_loss: 8635.5518\n",
      "Epoch 5/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 6427.0541\n",
      "Epoch 00005: val_loss did not improve from 6717.19043\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 6391.3745 - val_loss: 17601.5793\n",
      "Epoch 6/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 4245.3422\n",
      "Epoch 00006: val_loss improved from 6717.19043 to 6502.45218, saving model to Weights\\Weights-006--6502.45218.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 4216.8192 - val_loss: 6502.4522\n",
      "Epoch 7/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 2086.3391\n",
      "Epoch 00007: val_loss did not improve from 6502.45218\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 2031.6613 - val_loss: 7421.4388\n",
      "Epoch 8/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1612.9753\n",
      "Epoch 00008: val_loss did not improve from 6502.45218\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1609.5591 - val_loss: 7162.1514\n",
      "Epoch 9/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1570.8095\n",
      "Epoch 00009: val_loss improved from 6502.45218 to 4736.14557, saving model to Weights\\Weights-009--4736.14557.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1575.7235 - val_loss: 4736.1456\n",
      "Epoch 10/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1515.5427\n",
      "Epoch 00010: val_loss improved from 4736.14557 to 4065.77565, saving model to Weights\\Weights-010--4065.77565.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1513.3185 - val_loss: 4065.7757\n",
      "Epoch 11/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1479.2655\n",
      "Epoch 00011: val_loss improved from 4065.77565 to 2775.13568, saving model to Weights\\Weights-011--2775.13568.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1479.1711 - val_loss: 2775.1357\n",
      "Epoch 12/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1480.9681\n",
      "Epoch 00012: val_loss improved from 2775.13568 to 2201.12799, saving model to Weights\\Weights-012--2201.12799.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1478.4766 - val_loss: 2201.1280\n",
      "Epoch 13/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1451.3402\n",
      "Epoch 00013: val_loss improved from 2201.12799 to 1955.94877, saving model to Weights\\Weights-013--1955.94877.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1456.2948 - val_loss: 1955.9488\n",
      "Epoch 14/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1427.4028\n",
      "Epoch 00014: val_loss improved from 1955.94877 to 1604.26250, saving model to Weights\\Weights-014--1604.26250.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1432.9945 - val_loss: 1604.2625\n",
      "Epoch 15/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1428.4853\n",
      "Epoch 00015: val_loss improved from 1604.26250 to 1446.19086, saving model to Weights\\Weights-015--1446.19086.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1421.8478 - val_loss: 1446.1909\n",
      "Epoch 16/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1412.9200\n",
      "Epoch 00016: val_loss improved from 1446.19086 to 1419.48988, saving model to Weights\\Weights-016--1419.48988.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1410.2908 - val_loss: 1419.4899\n",
      "Epoch 17/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1399.4078\n",
      "Epoch 00017: val_loss improved from 1419.48988 to 1291.17431, saving model to Weights\\Weights-017--1291.17431.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1396.0419 - val_loss: 1291.1743\n",
      "Epoch 18/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1389.1506\n",
      "Epoch 00018: val_loss did not improve from 1291.17431\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1389.1999 - val_loss: 1366.0734\n",
      "Epoch 19/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1356.4132\n",
      "Epoch 00019: val_loss did not improve from 1291.17431\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1361.7629 - val_loss: 1300.2459\n",
      "Epoch 20/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1361.2498\n",
      "Epoch 00020: val_loss improved from 1291.17431 to 1254.13846, saving model to Weights\\Weights-020--1254.13846.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1360.6113 - val_loss: 1254.1385\n",
      "Epoch 21/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1349.0534\n",
      "Epoch 00021: val_loss did not improve from 1254.13846\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1354.8874 - val_loss: 1310.5606\n",
      "Epoch 22/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1359.0408\n",
      "Epoch 00022: val_loss did not improve from 1254.13846\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1360.5650 - val_loss: 1320.1308\n",
      "Epoch 23/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1341.2471\n",
      "Epoch 00023: val_loss did not improve from 1254.13846\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1339.8866 - val_loss: 1269.0962\n",
      "Epoch 24/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1332.1488\n",
      "Epoch 00024: val_loss did not improve from 1254.13846\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1336.1306 - val_loss: 1277.5868\n",
      "Epoch 25/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1324.0589\n",
      "Epoch 00025: val_loss improved from 1254.13846 to 1214.16086, saving model to Weights\\Weights-025--1214.16086.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1324.5234 - val_loss: 1214.1609\n",
      "Epoch 26/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1318.6098 ETA: 0s - loss: 131\n",
      "Epoch 00026: val_loss did not improve from 1214.16086\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1317.5454 - val_loss: 1263.3197\n",
      "Epoch 27/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1319.8921\n",
      "Epoch 00027: val_loss did not improve from 1214.16086\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1320.4019 - val_loss: 1239.7793\n",
      "Epoch 28/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1298.6745\n",
      "Epoch 00028: val_loss improved from 1214.16086 to 1211.06013, saving model to Weights\\Weights-028--1211.06013.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1298.7647 - val_loss: 1211.0601\n",
      "Epoch 29/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1299.8010\n",
      "Epoch 00029: val_loss improved from 1211.06013 to 1200.41962, saving model to Weights\\Weights-029--1200.41962.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1301.1991 - val_loss: 1200.4196\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1302.6645\n",
      "Epoch 00030: val_loss improved from 1200.41962 to 1190.78107, saving model to Weights\\Weights-030--1190.78107.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1299.2807 - val_loss: 1190.7811\n",
      "Epoch 31/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1293.9886\n",
      "Epoch 00031: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1295.3961 - val_loss: 1204.6287\n",
      "Epoch 32/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1290.5589\n",
      "Epoch 00032: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1291.1362 - val_loss: 1204.9877\n",
      "Epoch 33/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1293.4991\n",
      "Epoch 00033: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1295.4941 - val_loss: 1197.4476\n",
      "Epoch 34/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1291.4167\n",
      "Epoch 00034: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1290.0450 - val_loss: 1219.0027\n",
      "Epoch 35/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1293.7091\n",
      "Epoch 00035: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1294.4529 - val_loss: 1209.3864\n",
      "Epoch 36/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1267.5709\n",
      "Epoch 00036: val_loss did not improve from 1190.78107\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1272.3309 - val_loss: 1192.7740\n",
      "Epoch 37/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1287.1927\n",
      "Epoch 00037: val_loss improved from 1190.78107 to 1184.38869, saving model to Weights\\Weights-037--1184.38869.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1281.6215 - val_loss: 1184.3887\n",
      "Epoch 38/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1268.9566\n",
      "Epoch 00038: val_loss did not improve from 1184.38869\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1273.1153 - val_loss: 1190.4663\n",
      "Epoch 39/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1275.5174\n",
      "Epoch 00039: val_loss did not improve from 1184.38869\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1268.3523 - val_loss: 1202.5608\n",
      "Epoch 40/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1276.0962\n",
      "Epoch 00040: val_loss did not improve from 1184.38869\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1270.2736 - val_loss: 1193.3279\n",
      "Epoch 41/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1272.7673\n",
      "Epoch 00041: val_loss did not improve from 1184.38869\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1267.5290 - val_loss: 1206.1273\n",
      "Epoch 42/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1259.6497\n",
      "Epoch 00042: val_loss improved from 1184.38869 to 1177.99022, saving model to Weights\\Weights-042--1177.99022.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1259.0840 - val_loss: 1177.9902\n",
      "Epoch 43/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1251.9590\n",
      "Epoch 00043: val_loss did not improve from 1177.99022\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1254.4663 - val_loss: 1187.1225\n",
      "Epoch 44/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1252.7162\n",
      "Epoch 00044: val_loss did not improve from 1177.99022\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1251.3792 - val_loss: 1183.0423\n",
      "Epoch 45/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1252.6155\n",
      "Epoch 00045: val_loss did not improve from 1177.99022\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1254.2196 - val_loss: 1182.1081\n",
      "Epoch 46/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1249.1121\n",
      "Epoch 00046: val_loss did not improve from 1177.99022\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1247.0685 - val_loss: 1196.1986\n",
      "Epoch 47/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1240.7290\n",
      "Epoch 00047: val_loss improved from 1177.99022 to 1176.41626, saving model to Weights\\Weights-047--1176.41626.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1240.1170 - val_loss: 1176.4163\n",
      "Epoch 48/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1243.8698\n",
      "Epoch 00048: val_loss improved from 1176.41626 to 1171.18046, saving model to Weights\\Weights-048--1171.18046.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1242.8750 - val_loss: 1171.1805\n",
      "Epoch 49/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1234.4323\n",
      "Epoch 00049: val_loss did not improve from 1171.18046\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1231.3602 - val_loss: 1174.0241\n",
      "Epoch 50/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1242.4602\n",
      "Epoch 00050: val_loss improved from 1171.18046 to 1160.68166, saving model to Weights\\Weights-050--1160.68166.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1238.5945 - val_loss: 1160.6817\n",
      "Epoch 51/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1231.9467\n",
      "Epoch 00051: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1233.9732 - val_loss: 1178.7826\n",
      "Epoch 52/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1226.8498\n",
      "Epoch 00052: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1227.7401 - val_loss: 1170.3438\n",
      "Epoch 53/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1225.1620\n",
      "Epoch 00053: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1227.9790 - val_loss: 1167.4894\n",
      "Epoch 54/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1215.9536\n",
      "Epoch 00054: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1219.1718 - val_loss: 1178.8715\n",
      "Epoch 55/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1221.1705\n",
      "Epoch 00055: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1222.5838 - val_loss: 1169.3796\n",
      "Epoch 56/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1232.6326\n",
      "Epoch 00056: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1232.1347 - val_loss: 1175.7767\n",
      "Epoch 57/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1227.7453\n",
      "Epoch 00057: val_loss did not improve from 1160.68166\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1227.6786 - val_loss: 1172.0561\n",
      "Epoch 58/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1217.1950\n",
      "Epoch 00058: val_loss improved from 1160.68166 to 1157.38679, saving model to Weights\\Weights-058--1157.38679.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1217.6481 - val_loss: 1157.3868\n",
      "Epoch 59/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1221.0485\n",
      "Epoch 00059: val_loss did not improve from 1157.38679\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1218.4487 - val_loss: 1161.9171\n",
      "Epoch 60/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1214.9147\n",
      "Epoch 00060: val_loss did not improve from 1157.38679\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1214.7963 - val_loss: 1161.4155\n",
      "Epoch 61/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1214.8450\n",
      "Epoch 00061: val_loss did not improve from 1157.38679\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1211.6523 - val_loss: 1161.0391\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19456/19948 [============================>.] - ETA: 0s - loss: 1218.9368\n",
      "Epoch 00062: val_loss did not improve from 1157.38679\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1219.9214 - val_loss: 1162.8586\n",
      "Epoch 63/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1214.2023\n",
      "Epoch 00063: val_loss improved from 1157.38679 to 1152.31716, saving model to Weights\\Weights-063--1152.31716.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1212.6485 - val_loss: 1152.3172\n",
      "Epoch 64/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1205.1397\n",
      "Epoch 00064: val_loss did not improve from 1152.31716\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1205.7224 - val_loss: 1160.6493\n",
      "Epoch 65/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1209.9151\n",
      "Epoch 00065: val_loss did not improve from 1152.31716\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1205.7106 - val_loss: 1168.8807\n",
      "Epoch 66/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1205.9854\n",
      "Epoch 00066: val_loss did not improve from 1152.31716\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1206.7001 - val_loss: 1153.2550\n",
      "Epoch 67/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1186.0619\n",
      "Epoch 00067: val_loss improved from 1152.31716 to 1151.75475, saving model to Weights\\Weights-067--1151.75475.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1188.5234 - val_loss: 1151.7547\n",
      "Epoch 68/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1203.1388\n",
      "Epoch 00068: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1201.2699 - val_loss: 1156.9639\n",
      "Epoch 69/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1190.3387\n",
      "Epoch 00069: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1185.7338 - val_loss: 1156.2140\n",
      "Epoch 70/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1189.8909\n",
      "Epoch 00070: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1187.9081 - val_loss: 1160.1411\n",
      "Epoch 71/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1197.2672\n",
      "Epoch 00071: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1197.1926 - val_loss: 1178.3814\n",
      "Epoch 72/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1199.9939\n",
      "Epoch 00072: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1191.7077 - val_loss: 1153.5183\n",
      "Epoch 73/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1183.6745\n",
      "Epoch 00073: val_loss did not improve from 1151.75475\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1183.5439 - val_loss: 1155.9389\n",
      "Epoch 74/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1187.4142\n",
      "Epoch 00074: val_loss improved from 1151.75475 to 1140.25112, saving model to Weights\\Weights-074--1140.25112.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1188.4704 - val_loss: 1140.2511\n",
      "Epoch 75/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1182.8667\n",
      "Epoch 00075: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1184.7850 - val_loss: 1158.3255\n",
      "Epoch 76/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1179.8329\n",
      "Epoch 00076: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1185.5763 - val_loss: 1166.4448\n",
      "Epoch 77/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1180.3191\n",
      "Epoch 00077: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1182.2232 - val_loss: 1159.5968\n",
      "Epoch 78/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1180.7470\n",
      "Epoch 00078: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1181.0266 - val_loss: 1157.1223\n",
      "Epoch 79/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1188.0388\n",
      "Epoch 00079: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1184.9112 - val_loss: 1143.5432\n",
      "Epoch 80/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1184.4478\n",
      "Epoch 00080: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1184.3600 - val_loss: 1151.7218\n",
      "Epoch 81/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1187.9298\n",
      "Epoch 00081: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1183.7279 - val_loss: 1149.2435\n",
      "Epoch 82/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1161.7440 ETA: 0s - loss: 1162.32\n",
      "Epoch 00082: val_loss did not improve from 1140.25112\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1168.8306 - val_loss: 1152.6051\n",
      "Epoch 83/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1173.6339\n",
      "Epoch 00083: val_loss improved from 1140.25112 to 1135.90358, saving model to Weights\\Weights-083--1135.90358.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1173.6044 - val_loss: 1135.9036\n",
      "Epoch 84/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1176.1817\n",
      "Epoch 00084: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1177.2677 - val_loss: 1144.2255\n",
      "Epoch 85/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1173.2052\n",
      "Epoch 00085: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1171.9388 - val_loss: 1139.8102\n",
      "Epoch 86/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1162.8828\n",
      "Epoch 00086: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1166.3348 - val_loss: 1142.5649\n",
      "Epoch 87/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1171.8152\n",
      "Epoch 00087: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1173.0599 - val_loss: 1141.6020\n",
      "Epoch 88/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1175.1439\n",
      "Epoch 00088: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1168.8545 - val_loss: 1143.0737\n",
      "Epoch 89/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1172.3432\n",
      "Epoch 00089: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1170.3536 - val_loss: 1145.5783\n",
      "Epoch 90/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1161.3461\n",
      "Epoch 00090: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1163.2526 - val_loss: 1160.5466\n",
      "Epoch 91/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1173.9683\n",
      "Epoch 00091: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1168.1937 - val_loss: 1159.4470\n",
      "Epoch 92/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1174.0827\n",
      "Epoch 00092: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1165.9743 - val_loss: 1140.6248\n",
      "Epoch 93/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1162.6439\n",
      "Epoch 00093: val_loss did not improve from 1135.90358\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1159.0289 - val_loss: 1140.6412\n",
      "Epoch 94/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1154.8259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00094: val_loss improved from 1135.90358 to 1135.62465, saving model to Weights\\Weights-094--1135.62465.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1155.8324 - val_loss: 1135.6247\n",
      "Epoch 95/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1159.7991\n",
      "Epoch 00095: val_loss did not improve from 1135.62465\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1157.9927 - val_loss: 1139.6051\n",
      "Epoch 96/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1157.8116\n",
      "Epoch 00096: val_loss improved from 1135.62465 to 1130.00532, saving model to Weights\\Weights-096--1130.00532.hdf5\n",
      "19948/19948 [==============================] - 1s 42us/sample - loss: 1157.1680 - val_loss: 1130.0053\n",
      "Epoch 97/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1158.3897\n",
      "Epoch 00097: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1157.1595 - val_loss: 1131.1229\n",
      "Epoch 98/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1160.5500\n",
      "Epoch 00098: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1162.1003 - val_loss: 1132.8139\n",
      "Epoch 99/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1151.8860 ETA: 0s - los\n",
      "Epoch 00099: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1151.8576 - val_loss: 1136.8036\n",
      "Epoch 100/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1156.8781\n",
      "Epoch 00100: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1156.6044 - val_loss: 1138.4573\n",
      "Epoch 101/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1155.4522\n",
      "Epoch 00101: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1153.0557 - val_loss: 1142.2812\n",
      "Epoch 102/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1150.1046\n",
      "Epoch 00102: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1150.7658 - val_loss: 1132.1475\n",
      "Epoch 103/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1150.6897\n",
      "Epoch 00103: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1151.0065 - val_loss: 1140.7462\n",
      "Epoch 104/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1149.2027\n",
      "Epoch 00104: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1147.4658 - val_loss: 1130.6595\n",
      "Epoch 105/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1146.1038\n",
      "Epoch 00105: val_loss did not improve from 1130.00532\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1147.5011 - val_loss: 1135.6516\n",
      "Epoch 106/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1148.6446\n",
      "Epoch 00106: val_loss improved from 1130.00532 to 1129.77358, saving model to Weights\\Weights-106--1129.77358.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1146.5687 - val_loss: 1129.7736\n",
      "Epoch 107/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1141.4933\n",
      "Epoch 00107: val_loss did not improve from 1129.77358\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1141.8779 - val_loss: 1132.5499\n",
      "Epoch 108/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1149.9568\n",
      "Epoch 00108: val_loss did not improve from 1129.77358\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1148.5282 - val_loss: 1140.6980\n",
      "Epoch 109/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1141.3495\n",
      "Epoch 00109: val_loss did not improve from 1129.77358\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1143.7406 - val_loss: 1130.5441\n",
      "Epoch 110/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1143.5822\n",
      "Epoch 00110: val_loss improved from 1129.77358 to 1121.54480, saving model to Weights\\Weights-110--1121.54480.hdf5\n",
      "19948/19948 [==============================] - 1s 43us/sample - loss: 1143.0686 - val_loss: 1121.5448\n",
      "Epoch 111/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1137.4890\n",
      "Epoch 00111: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1141.0194 - val_loss: 1138.4846\n",
      "Epoch 112/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1152.0645\n",
      "Epoch 00112: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1151.3147 - val_loss: 1131.4020\n",
      "Epoch 113/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1144.0116 ETA: 0s - loss: 1148.\n",
      "Epoch 00113: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1146.6682 - val_loss: 1130.7000\n",
      "Epoch 114/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1139.4404\n",
      "Epoch 00114: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1139.5911 - val_loss: 1137.8380\n",
      "Epoch 115/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1144.5748\n",
      "Epoch 00115: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1145.6138 - val_loss: 1142.8232\n",
      "Epoch 116/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1142.7345\n",
      "Epoch 00116: val_loss did not improve from 1121.54480\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1143.6042 - val_loss: 1126.9144\n",
      "Epoch 117/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1141.5556\n",
      "Epoch 00117: val_loss improved from 1121.54480 to 1121.45547, saving model to Weights\\Weights-117--1121.45547.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1139.2620 - val_loss: 1121.4555\n",
      "Epoch 118/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1125.3367\n",
      "Epoch 00118: val_loss improved from 1121.45547 to 1121.09202, saving model to Weights\\Weights-118--1121.09202.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1129.1966 - val_loss: 1121.0920\n",
      "Epoch 119/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1129.1785\n",
      "Epoch 00119: val_loss did not improve from 1121.09202\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.9721 - val_loss: 1129.3134\n",
      "Epoch 120/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1140.4994\n",
      "Epoch 00120: val_loss did not improve from 1121.09202\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1137.2371 - val_loss: 1121.9676\n",
      "Epoch 121/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1135.7914\n",
      "Epoch 00121: val_loss did not improve from 1121.09202\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1136.4237 - val_loss: 1131.1529\n",
      "Epoch 122/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1141.4014\n",
      "Epoch 00122: val_loss did not improve from 1121.09202\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1141.6015 - val_loss: 1121.6778\n",
      "Epoch 123/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1125.13 - ETA: 0s - loss: 1126.2205\n",
      "Epoch 00123: val_loss improved from 1121.09202 to 1119.28366, saving model to Weights\\Weights-123--1119.28366.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1129.5208 - val_loss: 1119.2837\n",
      "Epoch 124/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1133.9511\n",
      "Epoch 00124: val_loss improved from 1119.28366 to 1119.25944, saving model to Weights\\Weights-124--1119.25944.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1132.0159 - val_loss: 1119.2594\n",
      "Epoch 125/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1122.8183\n",
      "Epoch 00125: val_loss did not improve from 1119.25944\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1121.8818 - val_loss: 1127.1118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1130.4017\n",
      "Epoch 00126: val_loss did not improve from 1119.25944\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1131.1014 - val_loss: 1122.4961\n",
      "Epoch 127/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1136.3568\n",
      "Epoch 00127: val_loss did not improve from 1119.25944\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1130.9879 - val_loss: 1120.9249\n",
      "Epoch 128/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1120.8344\n",
      "Epoch 00128: val_loss did not improve from 1119.25944\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1123.8859 - val_loss: 1120.0509\n",
      "Epoch 129/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1126.3169 ETA: 0s - loss: 1\n",
      "Epoch 00129: val_loss did not improve from 1119.25944\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1121.9776 - val_loss: 1122.5373\n",
      "Epoch 130/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1117.5598\n",
      "Epoch 00130: val_loss improved from 1119.25944 to 1115.31478, saving model to Weights\\Weights-130--1115.31478.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1113.4780 - val_loss: 1115.3148\n",
      "Epoch 131/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1124.4633\n",
      "Epoch 00131: val_loss improved from 1115.31478 to 1110.86531, saving model to Weights\\Weights-131--1110.86531.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1127.5247 - val_loss: 1110.8653\n",
      "Epoch 132/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1125.7000\n",
      "Epoch 00132: val_loss did not improve from 1110.86531\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1122.6983 - val_loss: 1123.6418\n",
      "Epoch 133/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1111.1257\n",
      "Epoch 00133: val_loss did not improve from 1110.86531\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1111.3561 - val_loss: 1122.6788\n",
      "Epoch 134/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1126.2165\n",
      "Epoch 00134: val_loss did not improve from 1110.86531\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1122.8158 - val_loss: 1123.7864\n",
      "Epoch 135/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.4324\n",
      "Epoch 00135: val_loss did not improve from 1110.86531\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1122.2848 - val_loss: 1121.8384\n",
      "Epoch 136/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1117.0190\n",
      "Epoch 00136: val_loss improved from 1110.86531 to 1110.32437, saving model to Weights\\Weights-136--1110.32437.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1115.7642 - val_loss: 1110.3244\n",
      "Epoch 137/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1123.4628\n",
      "Epoch 00137: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1122.8534 - val_loss: 1144.0008\n",
      "Epoch 138/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1113.4951\n",
      "Epoch 00138: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1113.7283 - val_loss: 1128.5047\n",
      "Epoch 139/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1120.8438\n",
      "Epoch 00139: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1121.2803 - val_loss: 1116.9075\n",
      "Epoch 140/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1117.6741\n",
      "Epoch 00140: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1116.6992 - val_loss: 1114.7912\n",
      "Epoch 141/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1117.8082\n",
      "Epoch 00141: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1118.2466 - val_loss: 1120.0426\n",
      "Epoch 142/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1110.7119\n",
      "Epoch 00142: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1109.5582 - val_loss: 1126.5262\n",
      "Epoch 143/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1113.1248\n",
      "Epoch 00143: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1111.9095 - val_loss: 1123.4447\n",
      "Epoch 144/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1120.7605\n",
      "Epoch 00144: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1120.0928 - val_loss: 1114.9842\n",
      "Epoch 145/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1107.1729\n",
      "Epoch 00145: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1109.8108 - val_loss: 1129.6526\n",
      "Epoch 146/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1115.4781\n",
      "Epoch 00146: val_loss did not improve from 1110.32437\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1110.3896 - val_loss: 1113.6377\n",
      "Epoch 147/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1113.8373\n",
      "Epoch 00147: val_loss improved from 1110.32437 to 1109.67977, saving model to Weights\\Weights-147--1109.67977.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1114.1497 - val_loss: 1109.6798\n",
      "Epoch 148/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1101.2109\n",
      "Epoch 00148: val_loss did not improve from 1109.67977\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1102.2724 - val_loss: 1114.8415\n",
      "Epoch 149/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1106.4808\n",
      "Epoch 00149: val_loss improved from 1109.67977 to 1105.24592, saving model to Weights\\Weights-149--1105.24592.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1104.2352 - val_loss: 1105.2459\n",
      "Epoch 150/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1107.4350\n",
      "Epoch 00150: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1106.5158 - val_loss: 1115.3967\n",
      "Epoch 151/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1108.8496\n",
      "Epoch 00151: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1110.6226 - val_loss: 1112.6483\n",
      "Epoch 152/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1091.3021\n",
      "Epoch 00152: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1099.0906 - val_loss: 1121.5401\n",
      "Epoch 153/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1099.0884\n",
      "Epoch 00153: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1098.9574 - val_loss: 1107.8093\n",
      "Epoch 154/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1102.8969\n",
      "Epoch 00154: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1105.7775 - val_loss: 1114.0474\n",
      "Epoch 155/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1098.2820\n",
      "Epoch 00155: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1101.7315 - val_loss: 1121.4123\n",
      "Epoch 156/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1106.1950\n",
      "Epoch 00156: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1107.6616 - val_loss: 1111.6098\n",
      "Epoch 157/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1114.6988\n",
      "Epoch 00157: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1110.6572 - val_loss: 1111.9242\n",
      "Epoch 158/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1104.8340\n",
      "Epoch 00158: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1103.4594 - val_loss: 1126.5485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1098.3218\n",
      "Epoch 00159: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1100.7221 - val_loss: 1120.6939\n",
      "Epoch 160/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1096.8723\n",
      "Epoch 00160: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1097.9554 - val_loss: 1127.7281\n",
      "Epoch 161/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1102.2014\n",
      "Epoch 00161: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1103.2485 - val_loss: 1108.9671\n",
      "Epoch 162/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1095.6533\n",
      "Epoch 00162: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1096.8034 - val_loss: 1109.9130\n",
      "Epoch 163/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1095.1397\n",
      "Epoch 00163: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1098.6486 - val_loss: 1114.6809\n",
      "Epoch 164/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1096.7796\n",
      "Epoch 00164: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1095.3617 - val_loss: 1112.0131\n",
      "Epoch 165/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1091.8416\n",
      "Epoch 00165: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1097.6424 - val_loss: 1115.3762\n",
      "Epoch 166/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1096.9906 ETA: 0s - loss: 1097.\n",
      "Epoch 00166: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1099.7919 - val_loss: 1122.8053\n",
      "Epoch 167/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1101.8721\n",
      "Epoch 00167: val_loss did not improve from 1105.24592\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1101.3706 - val_loss: 1110.8408\n",
      "Epoch 168/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1100.3189\n",
      "Epoch 00168: val_loss improved from 1105.24592 to 1105.18129, saving model to Weights\\Weights-168--1105.18129.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1099.0457 - val_loss: 1105.1813\n",
      "Epoch 169/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1093.7041\n",
      "Epoch 00169: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1093.6811 - val_loss: 1116.9620\n",
      "Epoch 170/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1100.8770\n",
      "Epoch 00170: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1097.1899 - val_loss: 1116.9696\n",
      "Epoch 171/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1091.4923\n",
      "Epoch 00171: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1095.6071 - val_loss: 1133.0690\n",
      "Epoch 172/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.8709\n",
      "Epoch 00172: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.1633 - val_loss: 1108.0351\n",
      "Epoch 173/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.3153\n",
      "Epoch 00173: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1090.5528 - val_loss: 1107.6365\n",
      "Epoch 174/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1085.0658\n",
      "Epoch 00174: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1087.6471 - val_loss: 1106.5418\n",
      "Epoch 175/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1081.1575\n",
      "Epoch 00175: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1083.3100 - val_loss: 1108.8338\n",
      "Epoch 176/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1088.0712\n",
      "Epoch 00176: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1088.7746 - val_loss: 1110.7399\n",
      "Epoch 177/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1081.1416\n",
      "Epoch 00177: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1081.9559 - val_loss: 1108.5651\n",
      "Epoch 178/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1082.7976\n",
      "Epoch 00178: val_loss did not improve from 1105.18129\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1084.5539 - val_loss: 1108.3692\n",
      "Epoch 179/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1095.5543\n",
      "Epoch 00179: val_loss improved from 1105.18129 to 1104.67656, saving model to Weights\\Weights-179--1104.67656.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1096.6744 - val_loss: 1104.6766\n",
      "Epoch 180/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1083.8077\n",
      "Epoch 00180: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1080.9062 - val_loss: 1105.8852\n",
      "Epoch 181/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1085.6012\n",
      "Epoch 00181: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1089.6536 - val_loss: 1111.1337\n",
      "Epoch 182/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1084.9082\n",
      "Epoch 00182: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1087.6543 - val_loss: 1107.3345\n",
      "Epoch 183/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1089.5448\n",
      "Epoch 00183: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1089.2387 - val_loss: 1111.3343\n",
      "Epoch 184/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1084.7212\n",
      "Epoch 00184: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1082.6464 - val_loss: 1104.8102\n",
      "Epoch 185/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1074.4058\n",
      "Epoch 00185: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1077.2209 - val_loss: 1106.7545\n",
      "Epoch 186/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1083.7600\n",
      "Epoch 00186: val_loss did not improve from 1104.67656\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.8357 - val_loss: 1109.7661\n",
      "Epoch 187/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1093.9933\n",
      "Epoch 00187: val_loss improved from 1104.67656 to 1104.26310, saving model to Weights\\Weights-187--1104.26310.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1089.1142 - val_loss: 1104.2631\n",
      "Epoch 188/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1082.8053\n",
      "Epoch 00188: val_loss did not improve from 1104.26310\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1085.7231 - val_loss: 1104.7325\n",
      "Epoch 189/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1087.5227\n",
      "Epoch 00189: val_loss improved from 1104.26310 to 1101.99146, saving model to Weights\\Weights-189--1101.99146.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1085.5459 - val_loss: 1101.9915\n",
      "Epoch 190/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1087.4016\n",
      "Epoch 00190: val_loss did not improve from 1101.99146\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.0897 - val_loss: 1103.8049\n",
      "Epoch 191/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1088.8215\n",
      "Epoch 00191: val_loss did not improve from 1101.99146\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.6211 - val_loss: 1108.0743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1090.5722\n",
      "Epoch 00192: val_loss improved from 1101.99146 to 1100.73531, saving model to Weights\\Weights-192--1100.73531.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1086.6635 - val_loss: 1100.7353\n",
      "Epoch 193/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1083.4800\n",
      "Epoch 00193: val_loss did not improve from 1100.73531\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1083.0333 - val_loss: 1101.4884\n",
      "Epoch 194/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1086.5347\n",
      "Epoch 00194: val_loss did not improve from 1100.73531\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1087.6317 - val_loss: 1109.1951\n",
      "Epoch 195/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1076.1021\n",
      "Epoch 00195: val_loss did not improve from 1100.73531\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1076.7153 - val_loss: 1101.1904\n",
      "Epoch 196/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1071.6868\n",
      "Epoch 00196: val_loss improved from 1100.73531 to 1098.54780, saving model to Weights\\Weights-196--1098.54780.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1072.7646 - val_loss: 1098.5478\n",
      "Epoch 197/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1074.8223\n",
      "Epoch 00197: val_loss improved from 1098.54780 to 1098.28038, saving model to Weights\\Weights-197--1098.28038.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1074.1803 - val_loss: 1098.2804\n",
      "Epoch 198/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1082.2475\n",
      "Epoch 00198: val_loss improved from 1098.28038 to 1098.26635, saving model to Weights\\Weights-198--1098.26635.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1077.6499 - val_loss: 1098.2664\n",
      "Epoch 199/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1080.2397\n",
      "Epoch 00199: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1073.0229 - val_loss: 1099.9443\n",
      "Epoch 200/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1072.8993\n",
      "Epoch 00200: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1075.2932 - val_loss: 1112.1879\n",
      "Epoch 201/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1077.2366\n",
      "Epoch 00201: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1077.4007 - val_loss: 1098.4684\n",
      "Epoch 202/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1080.0284\n",
      "Epoch 00202: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.1730 - val_loss: 1098.4070\n",
      "Epoch 203/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1072.0886\n",
      "Epoch 00203: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1076.0336 - val_loss: 1098.7325\n",
      "Epoch 204/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1083.3448\n",
      "Epoch 00204: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1081.6389 - val_loss: 1102.6929\n",
      "Epoch 205/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1075.0591\n",
      "Epoch 00205: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1075.6304 - val_loss: 1102.6333\n",
      "Epoch 206/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1069.2615\n",
      "Epoch 00206: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1073.3042 - val_loss: 1105.2006\n",
      "Epoch 207/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1074.4084\n",
      "Epoch 00207: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1074.8631 - val_loss: 1098.2806\n",
      "Epoch 208/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1070.8478\n",
      "Epoch 00208: val_loss did not improve from 1098.26635\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1067.6238 - val_loss: 1098.3544\n",
      "Epoch 209/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1078.5083\n",
      "Epoch 00209: val_loss improved from 1098.26635 to 1095.54755, saving model to Weights\\Weights-209--1095.54755.hdf5\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1079.4711 - val_loss: 1095.5476\n",
      "Epoch 210/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1062.4773\n",
      "Epoch 00210: val_loss did not improve from 1095.54755\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1066.8697 - val_loss: 1101.2223\n",
      "Epoch 211/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.4411\n",
      "Epoch 00211: val_loss improved from 1095.54755 to 1095.36169, saving model to Weights\\Weights-211--1095.36169.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1072.9030 - val_loss: 1095.3617\n",
      "Epoch 212/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.0032\n",
      "Epoch 00212: val_loss improved from 1095.36169 to 1093.54001, saving model to Weights\\Weights-212--1093.54001.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1070.9637 - val_loss: 1093.5400\n",
      "Epoch 213/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1067.6936\n",
      "Epoch 00213: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1067.0355 - val_loss: 1099.7601\n",
      "Epoch 214/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1067.7269\n",
      "Epoch 00214: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1069.2706 - val_loss: 1096.9377\n",
      "Epoch 215/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1076.5015\n",
      "Epoch 00215: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1074.2363 - val_loss: 1094.2795\n",
      "Epoch 216/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1062.1359\n",
      "Epoch 00216: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1065.1269 - val_loss: 1097.7921\n",
      "Epoch 217/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1060.1873\n",
      "Epoch 00217: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1068.7416 - val_loss: 1101.6589\n",
      "Epoch 218/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1068.4820\n",
      "Epoch 00218: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1067.5111 - val_loss: 1095.4879\n",
      "Epoch 219/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1069.6170\n",
      "Epoch 00219: val_loss did not improve from 1093.54001\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.9674 - val_loss: 1094.1822\n",
      "Epoch 220/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1060.6938\n",
      "Epoch 00220: val_loss improved from 1093.54001 to 1091.69065, saving model to Weights\\Weights-220--1091.69065.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1058.1721 - val_loss: 1091.6907\n",
      "Epoch 221/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.8155\n",
      "Epoch 00221: val_loss did not improve from 1091.69065\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1058.5401 - val_loss: 1095.8586\n",
      "Epoch 222/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1063.6883\n",
      "Epoch 00222: val_loss improved from 1091.69065 to 1091.04269, saving model to Weights\\Weights-222--1091.04269.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1064.7862 - val_loss: 1091.0427\n",
      "Epoch 223/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1067.0165\n",
      "Epoch 00223: val_loss improved from 1091.04269 to 1089.86885, saving model to Weights\\Weights-223--1089.86885.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1069.2931 - val_loss: 1089.8689\n",
      "Epoch 224/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1078.3153\n",
      "Epoch 00224: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.1324 - val_loss: 1097.0611\n",
      "Epoch 225/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1063.0183 ETA: 0s - loss: 1074.\n",
      "Epoch 00225: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1069.4380 - val_loss: 1098.6881\n",
      "Epoch 226/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1067.2387\n",
      "Epoch 00226: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1067.9502 - val_loss: 1108.6431\n",
      "Epoch 227/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1069.0340\n",
      "Epoch 00227: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1064.9378 - val_loss: 1098.7127\n",
      "Epoch 228/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.5714\n",
      "Epoch 00228: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1061.5894 - val_loss: 1093.4873\n",
      "Epoch 229/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1063.9350\n",
      "Epoch 00229: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1059.7536 - val_loss: 1102.3358\n",
      "Epoch 230/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1060.6938\n",
      "Epoch 00230: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1066.5150 - val_loss: 1094.3974\n",
      "Epoch 231/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.1885\n",
      "Epoch 00231: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1058.7528 - val_loss: 1104.3566\n",
      "Epoch 232/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1055.5529\n",
      "Epoch 00232: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1057.8210 - val_loss: 1103.2686\n",
      "Epoch 233/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1061.0806\n",
      "Epoch 00233: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.3681 - val_loss: 1101.4018\n",
      "Epoch 234/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1058.3607\n",
      "Epoch 00234: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1062.3010 - val_loss: 1094.4443\n",
      "Epoch 235/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1050.2852\n",
      "Epoch 00235: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1055.2285 - val_loss: 1090.1832\n",
      "Epoch 236/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1059.1032\n",
      "Epoch 00236: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1056.9254 - val_loss: 1095.6190\n",
      "Epoch 237/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1058.7140\n",
      "Epoch 00237: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1052.8486 - val_loss: 1093.4625\n",
      "Epoch 238/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1057.4645\n",
      "Epoch 00238: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1055.7097 - val_loss: 1094.9650\n",
      "Epoch 239/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1073.7372\n",
      "Epoch 00239: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1068.2796 - val_loss: 1098.1460\n",
      "Epoch 240/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1063.9628\n",
      "Epoch 00240: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1066.2491 - val_loss: 1107.4750\n",
      "Epoch 241/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.6900\n",
      "Epoch 00241: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1051.7900 - val_loss: 1094.7264\n",
      "Epoch 242/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1053.2699\n",
      "Epoch 00242: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1053.2672 - val_loss: 1098.8890\n",
      "Epoch 243/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1054.6913\n",
      "Epoch 00243: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1058.7505 - val_loss: 1091.9851\n",
      "Epoch 244/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1050.6674\n",
      "Epoch 00244: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1054.3450 - val_loss: 1091.6776\n",
      "Epoch 245/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.0572\n",
      "Epoch 00245: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1044.1197 - val_loss: 1091.0884\n",
      "Epoch 246/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1057.0478\n",
      "Epoch 00246: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1056.3029 - val_loss: 1092.2368\n",
      "Epoch 247/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1059.1462\n",
      "Epoch 00247: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1058.3871 - val_loss: 1093.9584\n",
      "Epoch 248/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1051.2854\n",
      "Epoch 00248: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1051.4091 - val_loss: 1092.8235\n",
      "Epoch 249/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.0866\n",
      "Epoch 00249: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1054.4109 - val_loss: 1092.6962\n",
      "Epoch 250/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1049.2572\n",
      "Epoch 00250: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1050.8774 - val_loss: 1100.6263\n",
      "Epoch 251/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1057.4195\n",
      "Epoch 00251: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1058.0308 - val_loss: 1098.1593\n",
      "Epoch 252/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1063.8297\n",
      "Epoch 00252: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1064.3576 - val_loss: 1091.6416\n",
      "Epoch 253/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1054.9848\n",
      "Epoch 00253: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1054.7333 - val_loss: 1092.0530\n",
      "Epoch 254/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1050.2998\n",
      "Epoch 00254: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1050.4844 - val_loss: 1096.5838\n",
      "Epoch 255/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1050.2272\n",
      "Epoch 00255: val_loss did not improve from 1089.86885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1049.4414 - val_loss: 1091.6123\n",
      "Epoch 256/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1053.8849\n",
      "Epoch 00256: val_loss improved from 1089.86885 to 1089.79962, saving model to Weights\\Weights-256--1089.79962.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1051.0018 - val_loss: 1089.7996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.3347\n",
      "Epoch 00257: val_loss did not improve from 1089.79962\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1044.5005 - val_loss: 1091.6628\n",
      "Epoch 258/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1050.0058\n",
      "Epoch 00258: val_loss did not improve from 1089.79962\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1053.0864 - val_loss: 1090.6757\n",
      "Epoch 259/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1056.6353\n",
      "Epoch 00259: val_loss did not improve from 1089.79962\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1055.8172 - val_loss: 1094.3439\n",
      "Epoch 260/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.0393\n",
      "Epoch 00260: val_loss did not improve from 1089.79962\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.7582 - val_loss: 1091.5073\n",
      "Epoch 261/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1042.2625\n",
      "Epoch 00261: val_loss did not improve from 1089.79962\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1042.3706 - val_loss: 1090.5375\n",
      "Epoch 262/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1044.4473\n",
      "Epoch 00262: val_loss improved from 1089.79962 to 1087.17877, saving model to Weights\\Weights-262--1087.17877.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1042.5418 - val_loss: 1087.1788\n",
      "Epoch 263/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1032.7744\n",
      "Epoch 00263: val_loss did not improve from 1087.17877\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1041.4831 - val_loss: 1099.1591\n",
      "Epoch 264/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1044.8970\n",
      "Epoch 00264: val_loss improved from 1087.17877 to 1085.34038, saving model to Weights\\Weights-264--1085.34038.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1044.1706 - val_loss: 1085.3404\n",
      "Epoch 265/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1047.7565\n",
      "Epoch 00265: val_loss did not improve from 1085.34038\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1046.8113 - val_loss: 1088.3108\n",
      "Epoch 266/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.9474\n",
      "Epoch 00266: val_loss did not improve from 1085.34038\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1043.7740 - val_loss: 1087.6714\n",
      "Epoch 267/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1050.5392\n",
      "Epoch 00267: val_loss did not improve from 1085.34038\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1044.9787 - val_loss: 1086.7579\n",
      "Epoch 268/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1041.9201\n",
      "Epoch 00268: val_loss did not improve from 1085.34038\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1043.3510 - val_loss: 1093.4037\n",
      "Epoch 269/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1040.7999\n",
      "Epoch 00269: val_loss improved from 1085.34038 to 1081.71307, saving model to Weights\\Weights-269--1081.71307.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1040.8290 - val_loss: 1081.7131\n",
      "Epoch 270/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1047.3673\n",
      "Epoch 00270: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.7380 - val_loss: 1085.6875\n",
      "Epoch 271/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.1376\n",
      "Epoch 00271: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1049.6031 - val_loss: 1085.1303\n",
      "Epoch 272/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1035.0369\n",
      "Epoch 00272: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1035.0928 - val_loss: 1091.5109\n",
      "Epoch 273/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.7672\n",
      "Epoch 00273: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1052.5638 - val_loss: 1087.5815\n",
      "Epoch 274/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1041.7369\n",
      "Epoch 00274: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.2675 - val_loss: 1084.3428\n",
      "Epoch 275/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.4859\n",
      "Epoch 00275: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.7176 - val_loss: 1090.3637\n",
      "Epoch 276/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.8437\n",
      "Epoch 00276: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1050.2847 - val_loss: 1085.6041\n",
      "Epoch 277/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.4253\n",
      "Epoch 00277: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1050.6969 - val_loss: 1088.3299\n",
      "Epoch 278/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1037.8073\n",
      "Epoch 00278: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1038.0869 - val_loss: 1082.7769\n",
      "Epoch 279/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1041.3596\n",
      "Epoch 00279: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1042.2230 - val_loss: 1084.9305\n",
      "Epoch 280/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.8195\n",
      "Epoch 00280: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1044.7783 - val_loss: 1085.3799\n",
      "Epoch 281/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1043.6805\n",
      "Epoch 00281: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1041.6151 - val_loss: 1085.9660\n",
      "Epoch 282/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1042.6523\n",
      "Epoch 00282: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1042.6717 - val_loss: 1082.7707\n",
      "Epoch 283/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1038.0360\n",
      "Epoch 00283: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1038.3687 - val_loss: 1082.1285\n",
      "Epoch 284/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1049.7690\n",
      "Epoch 00284: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1045.9478 - val_loss: 1081.9294\n",
      "Epoch 285/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1037.5731\n",
      "Epoch 00285: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1036.6283 - val_loss: 1089.9196\n",
      "Epoch 286/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1034.8291\n",
      "Epoch 00286: val_loss did not improve from 1081.71307\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.6701 - val_loss: 1090.0303\n",
      "Epoch 287/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1043.2742\n",
      "Epoch 00287: val_loss improved from 1081.71307 to 1081.23791, saving model to Weights\\Weights-287--1081.23791.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1041.4607 - val_loss: 1081.2379\n",
      "Epoch 288/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1036.1232\n",
      "Epoch 00288: val_loss did not improve from 1081.23791\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1039.5286 - val_loss: 1086.1621\n",
      "Epoch 289/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1038.6324\n",
      "Epoch 00289: val_loss did not improve from 1081.23791\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1041.7604 - val_loss: 1082.4302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1038.6373\n",
      "Epoch 00290: val_loss did not improve from 1081.23791\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.5341 - val_loss: 1081.7029\n",
      "Epoch 291/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1035.5061\n",
      "Epoch 00291: val_loss did not improve from 1081.23791\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1041.6628 - val_loss: 1082.6575\n",
      "Epoch 292/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1038.7953\n",
      "Epoch 00292: val_loss improved from 1081.23791 to 1079.00480, saving model to Weights\\Weights-292--1079.00480.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1038.4272 - val_loss: 1079.0048\n",
      "Epoch 293/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1042.8749\n",
      "Epoch 00293: val_loss improved from 1079.00480 to 1077.95418, saving model to Weights\\Weights-293--1077.95418.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1044.4984 - val_loss: 1077.9542\n",
      "Epoch 294/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1040.2082\n",
      "Epoch 00294: val_loss did not improve from 1077.95418\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.5947 - val_loss: 1081.4238\n",
      "Epoch 295/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1041.4406\n",
      "Epoch 00295: val_loss improved from 1077.95418 to 1076.52419, saving model to Weights\\Weights-295--1076.52419.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1038.6514 - val_loss: 1076.5242\n",
      "Epoch 296/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1037.4970\n",
      "Epoch 00296: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1038.5120 - val_loss: 1078.8823\n",
      "Epoch 297/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1030.1885\n",
      "Epoch 00297: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1029.4743 - val_loss: 1084.8561\n",
      "Epoch 298/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1037.3746\n",
      "Epoch 00298: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1036.1225 - val_loss: 1080.9755\n",
      "Epoch 299/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1043.3806\n",
      "Epoch 00299: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1042.8724 - val_loss: 1088.6721\n",
      "Epoch 300/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1042.3450\n",
      "Epoch 00300: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1045.0999 - val_loss: 1085.9334\n",
      "Epoch 301/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1040.5377\n",
      "Epoch 00301: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1041.4444 - val_loss: 1081.2481\n",
      "Epoch 302/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1029.0616\n",
      "Epoch 00302: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1031.2733 - val_loss: 1084.4590\n",
      "Epoch 303/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1031.6951\n",
      "Epoch 00303: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1029.0366 - val_loss: 1086.9080\n",
      "Epoch 304/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1036.0140\n",
      "Epoch 00304: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1036.8133 - val_loss: 1079.3074\n",
      "Epoch 305/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1037.9789\n",
      "Epoch 00305: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1040.8760 - val_loss: 1084.5153\n",
      "Epoch 306/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1037.3740\n",
      "Epoch 00306: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1039.3204 - val_loss: 1077.7144\n",
      "Epoch 307/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1024.0448\n",
      "Epoch 00307: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1027.7661 - val_loss: 1081.3765\n",
      "Epoch 308/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1034.8952\n",
      "Epoch 00308: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1029.8933 - val_loss: 1084.0580\n",
      "Epoch 309/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1027.4831\n",
      "Epoch 00309: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1027.7949 - val_loss: 1082.6190\n",
      "Epoch 310/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1033.7865\n",
      "Epoch 00310: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1031.8901 - val_loss: 1084.4171\n",
      "Epoch 311/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1028.7825\n",
      "Epoch 00311: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1028.0448 - val_loss: 1082.6491\n",
      "Epoch 312/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1040.1637\n",
      "Epoch 00312: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1038.5904 - val_loss: 1080.0092\n",
      "Epoch 313/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1035.5982\n",
      "Epoch 00313: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1031.0561 - val_loss: 1081.6441\n",
      "Epoch 314/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1032.4118\n",
      "Epoch 00314: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1031.6166 - val_loss: 1086.9861\n",
      "Epoch 315/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1027.3162\n",
      "Epoch 00315: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1028.9809 - val_loss: 1079.3563\n",
      "Epoch 316/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1028.4533\n",
      "Epoch 00316: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1028.3476 - val_loss: 1081.4352\n",
      "Epoch 317/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1027.0604\n",
      "Epoch 00317: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1029.7385 - val_loss: 1084.6775\n",
      "Epoch 318/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1027.0644\n",
      "Epoch 00318: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1030.5572 - val_loss: 1081.5969\n",
      "Epoch 319/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1037.1573\n",
      "Epoch 00319: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1036.6000 - val_loss: 1078.3641\n",
      "Epoch 320/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1027.5816\n",
      "Epoch 00320: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1027.2038 - val_loss: 1078.9817\n",
      "Epoch 321/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1026.1786\n",
      "Epoch 00321: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.3148 - val_loss: 1083.0583\n",
      "Epoch 322/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1026.5522\n",
      "Epoch 00322: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1026.3562 - val_loss: 1084.4266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1034.2490\n",
      "Epoch 00323: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1033.2681 - val_loss: 1081.3848\n",
      "Epoch 324/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1031.6314\n",
      "Epoch 00324: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1028.7673 - val_loss: 1085.1642\n",
      "Epoch 325/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1023.4163\n",
      "Epoch 00325: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1025.3554 - val_loss: 1083.1537\n",
      "Epoch 326/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1026.4405\n",
      "Epoch 00326: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.3639 - val_loss: 1079.1644\n",
      "Epoch 327/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1032.1082\n",
      "Epoch 00327: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1031.5612 - val_loss: 1080.1886\n",
      "Epoch 328/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1027.3570\n",
      "Epoch 00328: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1029.8696 - val_loss: 1083.1040\n",
      "Epoch 329/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1025.5675\n",
      "Epoch 00329: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1024.4549 - val_loss: 1084.1621\n",
      "Epoch 330/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1031.4956\n",
      "Epoch 00330: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1027.5361 - val_loss: 1079.8328\n",
      "Epoch 331/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1030.1648\n",
      "Epoch 00331: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1030.3116 - val_loss: 1083.5734\n",
      "Epoch 332/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1034.4980\n",
      "Epoch 00332: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1035.5056 - val_loss: 1082.1995\n",
      "Epoch 333/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1026.3658\n",
      "Epoch 00333: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1028.4001 - val_loss: 1079.4852\n",
      "Epoch 334/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1013.5404\n",
      "Epoch 00334: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1012.9361 - val_loss: 1079.8200\n",
      "Epoch 335/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1020.9028\n",
      "Epoch 00335: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1020.5283 - val_loss: 1082.0446\n",
      "Epoch 336/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1036.6664\n",
      "Epoch 00336: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1035.7853 - val_loss: 1088.4149\n",
      "Epoch 337/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1025.0142\n",
      "Epoch 00337: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.9273 - val_loss: 1081.0524\n",
      "Epoch 338/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1026.5968\n",
      "Epoch 00338: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1028.7956 - val_loss: 1079.9802\n",
      "Epoch 339/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1019.2113\n",
      "Epoch 00339: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1020.3084 - val_loss: 1082.0621\n",
      "Epoch 340/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1022.9826\n",
      "Epoch 00340: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1024.6393 - val_loss: 1081.1866\n",
      "Epoch 341/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1022.4113\n",
      "Epoch 00341: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1022.9456 - val_loss: 1078.9793\n",
      "Epoch 342/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1031.6622\n",
      "Epoch 00342: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1030.9568 - val_loss: 1078.1919\n",
      "Epoch 343/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1019.8773\n",
      "Epoch 00343: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1021.1994 - val_loss: 1078.8886\n",
      "Epoch 344/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1033.9347\n",
      "Epoch 00344: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1034.6045 - val_loss: 1078.9348\n",
      "Epoch 345/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1024.0405\n",
      "Epoch 00345: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1023.9674 - val_loss: 1080.1823\n",
      "Epoch 346/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1028.4235\n",
      "Epoch 00346: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1028.7701 - val_loss: 1083.6889\n",
      "Epoch 347/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1018.8080\n",
      "Epoch 00347: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1017.3988 - val_loss: 1078.8776\n",
      "Epoch 348/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.6202\n",
      "Epoch 00348: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1014.5627 - val_loss: 1077.0312\n",
      "Epoch 349/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.3331\n",
      "Epoch 00349: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1018.6888 - val_loss: 1077.9279\n",
      "Epoch 350/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1026.2625\n",
      "Epoch 00350: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1024.6499 - val_loss: 1079.1528\n",
      "Epoch 351/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1022.0924\n",
      "Epoch 00351: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1023.3832 - val_loss: 1082.6021\n",
      "Epoch 352/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1015.0575\n",
      "Epoch 00352: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1017.0941 - val_loss: 1080.5226\n",
      "Epoch 353/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1027.8419\n",
      "Epoch 00353: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1025.3683 - val_loss: 1077.7902\n",
      "Epoch 354/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1026.6983\n",
      "Epoch 00354: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1024.6220 - val_loss: 1077.2335\n",
      "Epoch 355/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1031.3643\n",
      "Epoch 00355: val_loss did not improve from 1076.52419\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1032.8687 - val_loss: 1083.1160\n",
      "Epoch 356/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1015.0766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00356: val_loss improved from 1076.52419 to 1076.40146, saving model to Weights\\Weights-356--1076.40146.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1019.3267 - val_loss: 1076.4015\n",
      "Epoch 357/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1015.9570\n",
      "Epoch 00357: val_loss improved from 1076.40146 to 1075.11338, saving model to Weights\\Weights-357--1075.11338.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1015.6694 - val_loss: 1075.1134\n",
      "Epoch 358/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1018.1465\n",
      "Epoch 00358: val_loss did not improve from 1075.11338\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1016.8924 - val_loss: 1080.1381\n",
      "Epoch 359/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1021.8635\n",
      "Epoch 00359: val_loss did not improve from 1075.11338\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1022.2132 - val_loss: 1077.7239\n",
      "Epoch 360/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1019.3940\n",
      "Epoch 00360: val_loss did not improve from 1075.11338\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1019.0279 - val_loss: 1077.1998\n",
      "Epoch 361/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1027.7895\n",
      "Epoch 00361: val_loss improved from 1075.11338 to 1075.03348, saving model to Weights\\Weights-361--1075.03348.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1023.2228 - val_loss: 1075.0335\n",
      "Epoch 362/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1010.9132\n",
      "Epoch 00362: val_loss did not improve from 1075.03348\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1018.9326 - val_loss: 1076.0978\n",
      "Epoch 363/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1009.4004\n",
      "Epoch 00363: val_loss did not improve from 1075.03348\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1017.4262 - val_loss: 1077.0705\n",
      "Epoch 364/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1013.7719\n",
      "Epoch 00364: val_loss improved from 1075.03348 to 1074.77632, saving model to Weights\\Weights-364--1074.77632.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1013.8179 - val_loss: 1074.7763\n",
      "Epoch 365/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1021.6566\n",
      "Epoch 00365: val_loss improved from 1074.77632 to 1071.37012, saving model to Weights\\Weights-365--1071.37012.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1020.7638 - val_loss: 1071.3701\n",
      "Epoch 366/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1012.1250\n",
      "Epoch 00366: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1019.3767 - val_loss: 1078.7381\n",
      "Epoch 367/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1022.4983\n",
      "Epoch 00367: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1025.2080 - val_loss: 1082.2120\n",
      "Epoch 368/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1020.4485\n",
      "Epoch 00368: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1019.9602 - val_loss: 1075.4289\n",
      "Epoch 369/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1019.0072 ETA: 0s - loss: 102\n",
      "Epoch 00369: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1013.7749 - val_loss: 1077.3973\n",
      "Epoch 370/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1012.7869\n",
      "Epoch 00370: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1011.3969 - val_loss: 1076.9711\n",
      "Epoch 371/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1022.3512\n",
      "Epoch 00371: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1021.2053 - val_loss: 1076.2091\n",
      "Epoch 372/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1013.2817\n",
      "Epoch 00372: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1014.7927 - val_loss: 1077.9071\n",
      "Epoch 373/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1012.4856\n",
      "Epoch 00373: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1014.9101 - val_loss: 1083.0915\n",
      "Epoch 374/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1009.0151\n",
      "Epoch 00374: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1014.1908 - val_loss: 1076.5944\n",
      "Epoch 375/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1015.2033\n",
      "Epoch 00375: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1015.1421 - val_loss: 1075.8957\n",
      "Epoch 376/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1018.4590\n",
      "Epoch 00376: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1015.0884 - val_loss: 1077.7094\n",
      "Epoch 377/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1019.1625\n",
      "Epoch 00377: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1018.2686 - val_loss: 1074.4115\n",
      "Epoch 378/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1019.1457\n",
      "Epoch 00378: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1016.7579 - val_loss: 1081.5819\n",
      "Epoch 379/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1007.2774\n",
      "Epoch 00379: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1009.7229 - val_loss: 1073.1945\n",
      "Epoch 380/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1014.1473\n",
      "Epoch 00380: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1012.9511 - val_loss: 1075.9024\n",
      "Epoch 381/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1012.9873\n",
      "Epoch 00381: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1015.2676 - val_loss: 1076.6158\n",
      "Epoch 382/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1015.5127\n",
      "Epoch 00382: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1014.0950 - val_loss: 1080.6798\n",
      "Epoch 383/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1019.8785\n",
      "Epoch 00383: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1019.1992 - val_loss: 1076.4282\n",
      "Epoch 384/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1013.7321\n",
      "Epoch 00384: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1014.9361 - val_loss: 1074.9434\n",
      "Epoch 385/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1009.8750\n",
      "Epoch 00385: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1009.5271 - val_loss: 1082.3957\n",
      "Epoch 386/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1018.4176\n",
      "Epoch 00386: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1013.7258 - val_loss: 1078.1487\n",
      "Epoch 387/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1012.3714\n",
      "Epoch 00387: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1016.6018 - val_loss: 1077.3585\n",
      "Epoch 388/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1010.4460\n",
      "Epoch 00388: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1011.4403 - val_loss: 1076.3591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 389/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1011.1725\n",
      "Epoch 00389: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1012.7996 - val_loss: 1077.5653\n",
      "Epoch 390/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1009.6122\n",
      "Epoch 00390: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1007.8196 - val_loss: 1073.5157\n",
      "Epoch 391/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1013.2810\n",
      "Epoch 00391: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1013.7762 - val_loss: 1076.5000\n",
      "Epoch 392/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1013.1235\n",
      "Epoch 00392: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1011.7934 - val_loss: 1077.7070\n",
      "Epoch 393/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1009.1973\n",
      "Epoch 00393: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1008.1069 - val_loss: 1075.2081\n",
      "Epoch 394/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1019.0098\n",
      "Epoch 00394: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1017.9446 - val_loss: 1076.5401\n",
      "Epoch 395/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1014.8268\n",
      "Epoch 00395: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1014.4324 - val_loss: 1075.6801\n",
      "Epoch 396/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1012.8310\n",
      "Epoch 00396: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1011.5920 - val_loss: 1074.4612\n",
      "Epoch 397/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1015.3129\n",
      "Epoch 00397: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1014.6875 - val_loss: 1076.9525\n",
      "Epoch 398/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1021.9729\n",
      "Epoch 00398: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1019.0836 - val_loss: 1071.4948\n",
      "Epoch 399/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1012.0412\n",
      "Epoch 00399: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1010.0354 - val_loss: 1071.6056\n",
      "Epoch 400/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1008.2963\n",
      "Epoch 00400: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1004.6738 - val_loss: 1078.5860\n",
      "Epoch 401/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1010.7335\n",
      "Epoch 00401: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1013.4660 - val_loss: 1073.3253\n",
      "Epoch 402/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1006.2128\n",
      "Epoch 00402: val_loss did not improve from 1071.37012\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1005.7694 - val_loss: 1073.3708\n",
      "Epoch 403/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 999.8882 \n",
      "Epoch 00403: val_loss improved from 1071.37012 to 1070.95263, saving model to Weights\\Weights-403--1070.95263.hdf5\n",
      "19948/19948 [==============================] - 1s 41us/sample - loss: 1001.8679 - val_loss: 1070.9526\n",
      "Epoch 404/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1004.3700\n",
      "Epoch 00404: val_loss did not improve from 1070.95263\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1005.1436 - val_loss: 1072.1254\n",
      "Epoch 405/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1007.3623\n",
      "Epoch 00405: val_loss did not improve from 1070.95263\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1008.5504 - val_loss: 1074.3754\n",
      "Epoch 406/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1008.7279\n",
      "Epoch 00406: val_loss did not improve from 1070.95263\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1008.3590 - val_loss: 1073.7298\n",
      "Epoch 407/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1011.7509 ETA: 0s - loss: 101\n",
      "Epoch 00407: val_loss did not improve from 1070.95263\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1012.3164 - val_loss: 1073.9327\n",
      "Epoch 408/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1003.7770\n",
      "Epoch 00408: val_loss improved from 1070.95263 to 1068.13698, saving model to Weights\\Weights-408--1068.13698.hdf5\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 1008.0001 - val_loss: 1068.1370\n",
      "Epoch 409/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1011.3413\n",
      "Epoch 00409: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1011.2981 - val_loss: 1069.5791\n",
      "Epoch 410/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1008.9769\n",
      "Epoch 00410: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1009.5039 - val_loss: 1074.8799\n",
      "Epoch 411/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1004.4310\n",
      "Epoch 00411: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1002.7707 - val_loss: 1073.8060\n",
      "Epoch 412/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1002.5027\n",
      "Epoch 00412: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1003.6556 - val_loss: 1072.7171\n",
      "Epoch 413/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.4128\n",
      "Epoch 00413: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1007.4167 - val_loss: 1071.8904\n",
      "Epoch 414/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1012.0576\n",
      "Epoch 00414: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1008.6365 - val_loss: 1073.4662\n",
      "Epoch 415/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 998.3759\n",
      "Epoch 00415: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1004.6989 - val_loss: 1073.5247\n",
      "Epoch 416/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.8648\n",
      "Epoch 00416: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 997.3159 - val_loss: 1076.3055\n",
      "Epoch 417/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.2909\n",
      "Epoch 00417: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1014.1619 - val_loss: 1075.6193\n",
      "Epoch 418/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.8312\n",
      "Epoch 00418: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1001.0589 - val_loss: 1072.2902\n",
      "Epoch 419/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 993.2158 \n",
      "Epoch 00419: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1000.4328 - val_loss: 1074.9806\n",
      "Epoch 420/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1013.7235\n",
      "Epoch 00420: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1013.2151 - val_loss: 1074.6989\n",
      "Epoch 421/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1008.7675\n",
      "Epoch 00421: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1008.0673 - val_loss: 1076.2747\n",
      "Epoch 422/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1010.4676\n",
      "Epoch 00422: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1008.3025 - val_loss: 1071.9440\n",
      "Epoch 423/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1011.5462\n",
      "Epoch 00423: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1009.3935 - val_loss: 1075.7726\n",
      "Epoch 424/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1008.1476\n",
      "Epoch 00424: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1008.4260 - val_loss: 1074.5454\n",
      "Epoch 425/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.9310\n",
      "Epoch 00425: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1005.3234 - val_loss: 1078.4584\n",
      "Epoch 426/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1008.1380\n",
      "Epoch 00426: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1009.0003 - val_loss: 1071.7294\n",
      "Epoch 427/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1002.7684\n",
      "Epoch 00427: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1004.9054 - val_loss: 1073.9989\n",
      "Epoch 428/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1003.9142\n",
      "Epoch 00428: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1003.6211 - val_loss: 1072.0087\n",
      "Epoch 429/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.3695\n",
      "Epoch 00429: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 997.6807 - val_loss: 1073.7745\n",
      "Epoch 430/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1004.2862\n",
      "Epoch 00430: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1003.0435 - val_loss: 1071.5772\n",
      "Epoch 431/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.8594\n",
      "Epoch 00431: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 998.9487 - val_loss: 1069.5561\n",
      "Epoch 432/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1001.1788\n",
      "Epoch 00432: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1001.7128 - val_loss: 1073.6680\n",
      "Epoch 433/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 999.0676\n",
      "Epoch 00433: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 998.8079 - val_loss: 1070.1482\n",
      "Epoch 434/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 999.0417\n",
      "Epoch 00434: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 999.2873 - val_loss: 1074.2673\n",
      "Epoch 435/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.1428\n",
      "Epoch 00435: val_loss did not improve from 1068.13698\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1000.6966 - val_loss: 1078.4385\n",
      "Epoch 436/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1004.5136\n",
      "Epoch 00436: val_loss improved from 1068.13698 to 1067.48982, saving model to Weights\\Weights-436--1067.48982.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1004.6758 - val_loss: 1067.4898\n",
      "Epoch 437/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1010.3990\n",
      "Epoch 00437: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1004.8193 - val_loss: 1068.0232\n",
      "Epoch 438/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1001.4826\n",
      "Epoch 00438: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1002.5582 - val_loss: 1071.3108\n",
      "Epoch 439/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 998.1981\n",
      "Epoch 00439: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1001.7214 - val_loss: 1073.8364\n",
      "Epoch 440/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.3927\n",
      "Epoch 00440: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1005.6416 - val_loss: 1073.2578\n",
      "Epoch 441/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1005.0078\n",
      "Epoch 00441: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1006.6626 - val_loss: 1072.6129\n",
      "Epoch 442/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1003.8581\n",
      "Epoch 00442: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1005.5968 - val_loss: 1067.9844\n",
      "Epoch 443/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.0271\n",
      "Epoch 00443: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1006.8686 - val_loss: 1070.4005\n",
      "Epoch 444/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.4483\n",
      "Epoch 00444: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 999.4486 - val_loss: 1072.9084\n",
      "Epoch 445/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1000.7870\n",
      "Epoch 00445: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 999.1727 - val_loss: 1070.0890\n",
      "Epoch 446/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 993.7029 \n",
      "Epoch 00446: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 997.4909 - val_loss: 1070.6078\n",
      "Epoch 447/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 988.2996\n",
      "Epoch 00447: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 991.4172 - val_loss: 1070.9139\n",
      "Epoch 448/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 999.3071\n",
      "Epoch 00448: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 999.9213 - val_loss: 1069.2674\n",
      "Epoch 449/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.7020\n",
      "Epoch 00449: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 999.5301 - val_loss: 1071.3121\n",
      "Epoch 450/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.8067\n",
      "Epoch 00450: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 997.2420 - val_loss: 1069.4707\n",
      "Epoch 451/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1008.0384\n",
      "Epoch 00451: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1002.4973 - val_loss: 1068.8987\n",
      "Epoch 452/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 992.1658\n",
      "Epoch 00452: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 994.6034 - val_loss: 1068.8366\n",
      "Epoch 453/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 999.7987\n",
      "Epoch 00453: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 998.9126 - val_loss: 1070.7305\n",
      "Epoch 454/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 996.7431\n",
      "Epoch 00454: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 993.7801 - val_loss: 1070.4036\n",
      "Epoch 455/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.4520\n",
      "Epoch 00455: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 998.7217 - val_loss: 1075.5655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 998.0949\n",
      "Epoch 00456: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 997.2907 - val_loss: 1069.7882\n",
      "Epoch 457/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 991.8743\n",
      "Epoch 00457: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 991.4254 - val_loss: 1068.9737\n",
      "Epoch 458/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.0663\n",
      "Epoch 00458: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 994.1642 - val_loss: 1068.8588\n",
      "Epoch 459/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 995.9641\n",
      "Epoch 00459: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 996.7141 - val_loss: 1069.0852\n",
      "Epoch 460/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 994.2164\n",
      "Epoch 00460: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 993.9086 - val_loss: 1071.6320\n",
      "Epoch 461/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 999.3817 \n",
      "Epoch 00461: val_loss did not improve from 1067.48982\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 998.2527 - val_loss: 1071.1212\n",
      "Epoch 462/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 993.9664\n",
      "Epoch 00462: val_loss improved from 1067.48982 to 1067.10052, saving model to Weights\\Weights-462--1067.10052.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 998.8732 - val_loss: 1067.1005\n",
      "Epoch 463/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 989.3116\n",
      "Epoch 00463: val_loss did not improve from 1067.10052\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 993.2533 - val_loss: 1074.9452\n",
      "Epoch 464/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 999.1124 \n",
      "Epoch 00464: val_loss improved from 1067.10052 to 1066.94494, saving model to Weights\\Weights-464--1066.94494.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 996.1546 - val_loss: 1066.9449\n",
      "Epoch 465/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 990.8316\n",
      "Epoch 00465: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 992.8770 - val_loss: 1070.7750\n",
      "Epoch 466/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 998.7433\n",
      "Epoch 00466: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 997.5420 - val_loss: 1069.1360\n",
      "Epoch 467/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 986.2053\n",
      "Epoch 00467: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 989.2040 - val_loss: 1068.9779\n",
      "Epoch 468/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 990.0736\n",
      "Epoch 00468: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 990.4070 - val_loss: 1070.2677\n",
      "Epoch 469/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.7597\n",
      "Epoch 00469: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 999.2459 - val_loss: 1067.4904\n",
      "Epoch 470/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1003.3797\n",
      "Epoch 00470: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1000.0855 - val_loss: 1067.0230\n",
      "Epoch 471/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.1988\n",
      "Epoch 00471: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 995.4735 - val_loss: 1070.1446\n",
      "Epoch 472/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 996.5555\n",
      "Epoch 00472: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 998.5323 - val_loss: 1066.9939\n",
      "Epoch 473/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1001.7907\n",
      "Epoch 00473: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 999.6690 - val_loss: 1067.8620\n",
      "Epoch 474/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 985.5773\n",
      "Epoch 00474: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 988.7256 - val_loss: 1071.4314\n",
      "Epoch 475/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 992.0223\n",
      "Epoch 00475: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 992.2920 - val_loss: 1071.0970\n",
      "Epoch 476/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 989.6466\n",
      "Epoch 00476: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 991.8155 - val_loss: 1076.5586\n",
      "Epoch 477/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.6659\n",
      "Epoch 00477: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 993.4139 - val_loss: 1068.4481\n",
      "Epoch 478/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1000.0434\n",
      "Epoch 00478: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 997.6678 - val_loss: 1068.1598\n",
      "Epoch 479/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 994.3946\n",
      "Epoch 00479: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 991.7095 - val_loss: 1068.6004\n",
      "Epoch 480/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 990.3833- ETA: 0s - loss: 990.81\n",
      "Epoch 00480: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 990.7968 - val_loss: 1074.0281\n",
      "Epoch 481/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.3274\n",
      "Epoch 00481: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 994.3616 - val_loss: 1070.5470\n",
      "Epoch 482/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 991.4649\n",
      "Epoch 00482: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 990.9129 - val_loss: 1068.3519\n",
      "Epoch 483/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 993.4751\n",
      "Epoch 00483: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 991.6609 - val_loss: 1068.1081\n",
      "Epoch 484/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.6466\n",
      "Epoch 00484: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 996.0530 - val_loss: 1071.6612\n",
      "Epoch 485/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1004.8404\n",
      "Epoch 00485: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1003.7049 - val_loss: 1073.6809\n",
      "Epoch 486/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.1350\n",
      "Epoch 00486: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 991.0919 - val_loss: 1069.6844\n",
      "Epoch 487/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 991.5963\n",
      "Epoch 00487: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 992.0792 - val_loss: 1072.7958\n",
      "Epoch 488/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 992.7880\n",
      "Epoch 00488: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 994.8451 - val_loss: 1067.8087\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17408/19948 [=========================>....] - ETA: 0s - loss: 992.1124\n",
      "Epoch 00489: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 994.7424 - val_loss: 1071.5131\n",
      "Epoch 490/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 994.4136\n",
      "Epoch 00490: val_loss did not improve from 1066.94494\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 993.3577 - val_loss: 1067.8464\n",
      "Epoch 491/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 988.0432\n",
      "Epoch 00491: val_loss improved from 1066.94494 to 1066.71914, saving model to Weights\\Weights-491--1066.71914.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 987.2654 - val_loss: 1066.7191\n",
      "Epoch 492/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 998.4959 \n",
      "Epoch 00492: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 997.6327 - val_loss: 1068.0487\n",
      "Epoch 493/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 990.6413\n",
      "Epoch 00493: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 990.5831 - val_loss: 1071.7696\n",
      "Epoch 494/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 992.9699\n",
      "Epoch 00494: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 995.4985 - val_loss: 1070.0351\n",
      "Epoch 495/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 995.9446\n",
      "Epoch 00495: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 995.6133 - val_loss: 1068.7415\n",
      "Epoch 496/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 988.1310\n",
      "Epoch 00496: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 985.1455 - val_loss: 1066.8374\n",
      "Epoch 497/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 989.9009\n",
      "Epoch 00497: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 992.1394 - val_loss: 1066.9932\n",
      "Epoch 498/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 993.5512- ETA: 0s - loss: 987.580\n",
      "Epoch 00498: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 993.9687 - val_loss: 1068.6314\n",
      "Epoch 499/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 976.2414\n",
      "Epoch 00499: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 977.2361 - val_loss: 1070.7418\n",
      "Epoch 500/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 985.6436\n",
      "Epoch 00500: val_loss did not improve from 1066.71914\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 985.7961 - val_loss: 1067.6574\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.005, decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=500, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=callbacks_list, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1479.1710955809324,
          1478.4765967443086,
          1456.2947744702633,
          1432.9945094305742,
          1421.847750097519,
          1410.290778191266,
          1396.0418759918366,
          1389.1999084093252,
          1361.762865873145,
          1360.6112916532202,
          1354.8874023388544,
          1360.5650164548922,
          1339.8866414004538,
          1336.1306244869472,
          1324.523360713441,
          1317.5453727543174,
          1320.4018576472656,
          1298.7646605539637,
          1301.1991368720783,
          1299.2807323210268,
          1295.396088129919,
          1291.136228804266,
          1295.494122144333,
          1290.045049208019,
          1294.4529196565368,
          1272.3308547536094,
          1281.6214635346823,
          1273.1153185538258,
          1268.3522913970855,
          1270.2735619691275,
          1267.5290281587597,
          1259.0840361698229,
          1254.4662873226248,
          1251.3791520061534,
          1254.2196390057509,
          1247.0684915301267,
          1240.1169982628662,
          1242.8749860232308,
          1231.3602239602262,
          1238.5944600002977,
          1233.9732434946095,
          1227.7401464510854,
          1227.9789916673976,
          1219.171756454477,
          1222.5838438729759,
          1232.1347204636352,
          1227.678565956056,
          1217.648140512011,
          1218.4487247409672,
          1214.796293116008,
          1211.6522578822126,
          1219.9214451470307,
          1212.6485268680992,
          1205.722408290853,
          1205.710633388999,
          1206.700134656748,
          1188.5234247226383,
          1201.2698575133238,
          1185.7337772055,
          1187.9081130470474,
          1197.1926130497889,
          1191.7076775837882,
          1183.5439175058277,
          1188.4704472595545,
          1184.7849556552114,
          1185.5762871405107,
          1182.2232258706622,
          1181.0266127233542,
          1184.9111751344512,
          1184.3600037343185,
          1183.7278782990072,
          1168.8306421157902,
          1173.6044097710687,
          1177.2677338276785,
          1171.9387939893725,
          1166.3347866562438,
          1173.0599087657206,
          1168.8544990902126,
          1170.353598139342,
          1163.2526067041888,
          1168.193666961835,
          1165.9743121667116,
          1159.0288745122084,
          1155.832406689032,
          1157.992686796217,
          1157.167956143982,
          1157.1595088881481,
          1162.1003235854628,
          1151.857571654074,
          1156.6043502902469,
          1153.0556809031607,
          1150.765787703302,
          1151.0065345924795,
          1147.4658169835322,
          1147.5010897718835,
          1146.5686546005943,
          1141.8779126020622,
          1148.5282067114733,
          1143.7406452087928,
          1143.068599525759,
          1141.0193783867353,
          1151.314724019756,
          1146.6681926230856,
          1139.5911303226278,
          1145.6138258299704,
          1143.6042146348084,
          1139.2620236687653,
          1129.196576528662,
          1131.972090349949,
          1137.2371000538901,
          1136.4237280111884,
          1141.601487206581,
          1129.5207852672809,
          1132.015892492355,
          1121.8817909719567,
          1131.1014389610252,
          1130.9879228229725,
          1123.885937372716,
          1121.977614037709,
          1113.4779801507357,
          1127.5247451479315,
          1122.6983185334605,
          1111.3560622364241,
          1122.8158385385989,
          1122.2848156182638,
          1115.7641737658537,
          1122.8533867009303,
          1113.7282863668192,
          1121.2803188191642,
          1116.6992455286088,
          1118.2465614699781,
          1109.558185584277,
          1111.9094846062653,
          1120.0928339463856,
          1109.8108381352063,
          1110.3895727034821,
          1114.149719764552,
          1102.2724122513457,
          1104.2352459779215,
          1106.5157588930438,
          1110.6225551423936,
          1099.0906156544124,
          1098.957407374408,
          1105.777536365057,
          1101.7314611006195,
          1107.6616159779098,
          1110.6572047039099,
          1103.4593726158716,
          1100.7220872412804,
          1097.9554091859538,
          1103.2484763608038,
          1096.8034222474528,
          1098.6486193938224,
          1095.3617385181942,
          1097.6424013959734,
          1099.7918988540125,
          1101.3706232640411,
          1099.0456500867099,
          1093.6811197149698,
          1097.1899235620033,
          1095.6071182290752,
          1092.163282028391,
          1090.5527540305968,
          1087.647130635355,
          1083.3099654316065,
          1088.7746009008188,
          1081.9558765632441,
          1084.553932558637,
          1096.6743850417336,
          1080.9061797000327,
          1089.6535506721773,
          1087.6543102643043,
          1089.2386911781177,
          1082.6464459205263,
          1077.2208532467619,
          1083.835680435147,
          1089.1141632551273,
          1085.7231255365514,
          1085.545887006412,
          1086.0896644508143,
          1090.6210979356874,
          1086.6635386036899,
          1083.0333331995218,
          1087.631673405542,
          1076.7153299996005,
          1072.7645704515335,
          1074.1802804723923,
          1077.649906480482,
          1073.0229086347158,
          1075.2931641555153,
          1077.4007233749155,
          1078.1730087827962,
          1076.0336253409255,
          1081.6388991749643,
          1075.6303818394622,
          1073.3042334875363,
          1074.8630758575812,
          1067.6237846330146,
          1079.4711325373705,
          1066.869694433065,
          1072.903041291146,
          1070.9637401971688,
          1067.0354647425102,
          1069.2705670975004,
          1074.2362869045455,
          1065.1269013791327,
          1068.7415971467221,
          1067.5111321662887,
          1071.967360131459,
          1058.1720676762511,
          1058.5400574207783,
          1064.786223355529,
          1069.293123424615,
          1078.1324037223344,
          1069.4379532781325,
          1067.9501632222293,
          1064.9377645550305,
          1061.5893967626375,
          1059.7535897532805,
          1066.5150415494347,
          1058.752818509766,
          1057.8209747687738,
          1060.3680741069359,
          1062.3010327339364,
          1055.2285463445191,
          1056.9254402388592,
          1052.8486186643868,
          1055.7096891370688,
          1068.2795827378725,
          1066.2490892825251,
          1051.78999672831,
          1053.2671552236989,
          1058.7505101887955,
          1054.3449556493367,
          1044.1197369753156,
          1056.3029200873443,
          1058.3870866376603,
          1051.4091137812804,
          1054.4109052922363,
          1050.877394849656,
          1058.0308079816116,
          1064.3575855574102,
          1054.7333237070682,
          1050.4844280064692,
          1049.441353549502,
          1051.001821900026,
          1044.5004591283039,
          1053.0864471013306,
          1055.8172466430296,
          1047.7582030907313,
          1042.3705942579613,
          1042.541837023331,
          1041.4830783915133,
          1044.1705834339205,
          1046.8112754293977,
          1043.7739622530298,
          1044.9787284219244,
          1043.3509731258303,
          1040.8289751596337,
          1047.7379949832455,
          1049.6030983780486,
          1035.0928104233114,
          1052.5637548740006,
          1039.2675312501958,
          1047.7176484543406,
          1050.284692152722,
          1050.6969286037252,
          1038.0868626103652,
          1042.2230064280411,
          1044.7782610642162,
          1041.615070594679,
          1042.671694305585,
          1038.3686839322277,
          1045.947847753769,
          1036.6282968041126,
          1039.670145723608,
          1041.4607371255106,
          1039.5285581378773,
          1041.760441380989,
          1039.5341315888106,
          1041.662766160768,
          1038.427169784506,
          1044.4983877270356,
          1039.5947357905945,
          1038.6514308295318,
          1038.511969352739,
          1029.474300123681,
          1036.122518413598,
          1042.8723836148793,
          1045.0998731712218,
          1041.44438530903,
          1031.273298907237,
          1029.0365994187425,
          1036.8133149117393,
          1040.8759545080882,
          1039.320374795758,
          1027.7661231212871,
          1029.8932788972031,
          1027.7949204063377,
          1031.8900637017573,
          1028.0448350036265,
          1038.5903687600457,
          1031.0560984928,
          1031.6166352462692,
          1028.9808826679837,
          1028.3475518281125,
          1029.7385226246445,
          1030.5572124976109,
          1036.60004977786,
          1027.20375030744,
          1023.3148182833962,
          1026.3561884557075,
          1033.2681001977212,
          1028.767263635452,
          1025.3554305158254,
          1023.3639424539937,
          1031.561162415836,
          1029.8695758385866,
          1024.4548564793854,
          1027.53613100115,
          1030.3116196838341,
          1035.5055500012925,
          1028.4000784853117,
          1012.9361311210906,
          1020.5283115617207,
          1035.7853170562798,
          1023.9272829938895,
          1028.7956260251262,
          1020.3084309487872,
          1024.6392683721817,
          1022.9456315668785,
          1030.9567939141768,
          1021.199425611081,
          1034.6044853826982,
          1023.9674423398296,
          1028.770064521271,
          1017.3987643507886,
          1014.5626683576518,
          1018.6887773445333,
          1024.6499170793281,
          1023.3831544535323,
          1017.0940801552787,
          1025.3682556091148,
          1024.6219818500758,
          1032.8687426909573,
          1019.3266760178026,
          1015.6693592060059,
          1016.89236645405,
          1022.2132283438895,
          1019.0278899944543,
          1023.2228385109879,
          1018.9325660780529,
          1017.4261728687948,
          1013.8179214199105,
          1020.7637844234854,
          1019.3767160829093,
          1025.208039303067,
          1019.9602436794651,
          1013.7748722361735,
          1011.396922281134,
          1021.2052875072845,
          1014.7927210601079,
          1014.9101015300915,
          1014.1908331045483,
          1015.1420745084681,
          1015.0883956734776,
          1018.268640347227,
          1016.7579232983296,
          1009.722869989316,
          1012.9511340447076,
          1015.2676338240167,
          1014.0950380412902,
          1019.1992319801993,
          1014.9360640277025,
          1009.5271288847097,
          1013.7257729838792,
          1016.6018259290562,
          1011.4402990499909,
          1012.799562872257,
          1007.819596074305,
          1013.7762476628687,
          1011.7933881265118,
          1008.1068554486783,
          1017.9445540950417,
          1014.4323551903896,
          1011.591984178395,
          1014.6875272926405,
          1019.0836270005138,
          1010.0353933900016,
          1004.6738435826704,
          1013.4659557476392,
          1005.7694120435789,
          1001.8679323565489,
          1005.1436250853783,
          1008.5503862777597,
          1008.359009572349,
          1012.3163743800287,
          1008.0001358267824,
          1011.2981479140687,
          1009.5039027252105,
          1002.7706774062758,
          1003.6556011205306,
          1007.4167463701523,
          1008.6364664093439,
          1004.6988516823428,
          997.315929975064,
          1014.1619237116112,
          1001.058932668297,
          1000.432825370377,
          1013.215073558929,
          1008.0673358887404,
          1008.3024655853266,
          1009.3935387525144,
          1008.4260263452554,
          1005.32335991302,
          1009.0002603203877,
          1004.9053665262103,
          1003.6211460466158,
          997.6806638422007,
          1003.0434907248103,
          998.9487045590997,
          1001.7128440561671,
          998.8078986566244,
          999.2872536098739,
          1000.6965984239687,
          1004.6757881527127,
          1004.8193130998017,
          1002.5581846174076,
          1001.7214379212123,
          1005.6415771582285,
          1006.6626131995923,
          1005.5968023697551,
          1006.8685621434088,
          999.4486020097567,
          999.1727386713267,
          997.4909353185469,
          991.4171943366229,
          999.9213083656178,
          999.530057788923,
          997.2419937467298,
          1002.4972874419781,
          994.6033563485767,
          998.9125631426866,
          993.780129863524,
          998.7216636056481,
          997.2907149775941,
          991.4253805989322,
          994.1642120098193,
          996.7141032015271,
          993.9085906560182,
          998.2526724757807,
          998.8732498686037,
          993.2532948458962,
          996.1545659706447,
          992.876982033169,
          997.5419532801886,
          989.2039724426086,
          990.4069637943419,
          999.2458536359574,
          1000.0855166425871,
          995.4734851752252,
          998.5323163467202,
          999.6690326856664,
          988.7256114922235,
          992.2919821149246,
          991.8154857903988,
          993.4138618929725,
          997.6677925350051,
          991.7094930706555,
          990.7968035618193,
          994.3615546172,
          990.9129299288502,
          991.6609070546311,
          996.0530211680251,
          1003.7048812753002,
          991.0919293116203,
          992.0792413791482,
          994.8451442867666,
          994.7424146041426,
          993.3577243092783,
          987.2653823035596,
          997.6326555367197,
          990.5831362837322,
          995.4984891136878,
          995.6133239880721,
          985.1455190355275,
          992.1394364483399,
          993.9687420937014,
          977.2361183755498,
          985.7961252846268
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          2775.1356840774292,
          2201.127991714003,
          1955.9487683357588,
          1604.2625012287808,
          1446.1908639709338,
          1419.4898753115522,
          1291.174309111894,
          1366.0733655796084,
          1300.245912982152,
          1254.1384557858244,
          1310.5605882423206,
          1320.130831372506,
          1269.0961764993278,
          1277.5867690565783,
          1214.160863068196,
          1263.319678502975,
          1239.7793187825455,
          1211.0601255157942,
          1200.4196176171326,
          1190.781071973655,
          1204.6287338780046,
          1204.9877476115635,
          1197.4475817790317,
          1219.0026660626222,
          1209.3863825242506,
          1192.774026751781,
          1184.3886916529852,
          1190.4662691356903,
          1202.560816864081,
          1193.3278842128204,
          1206.127285997304,
          1177.99021613911,
          1187.1225469423625,
          1183.0422531687857,
          1182.1080589072603,
          1196.1985853404399,
          1176.4162639268347,
          1171.1804615584504,
          1174.0241396429544,
          1160.681657269839,
          1178.782575345312,
          1170.3437927870275,
          1167.489429964196,
          1178.8715265892492,
          1169.379618869412,
          1175.7766987869634,
          1172.056096191896,
          1157.3867852559233,
          1161.9171132787044,
          1161.4154783393856,
          1161.039147413157,
          1162.8586268634388,
          1152.3171550719371,
          1160.649280267367,
          1168.8807380145408,
          1153.2549814331715,
          1151.754746691985,
          1156.9638833917404,
          1156.2140083014667,
          1160.1411063051041,
          1178.381439692419,
          1153.5182578953325,
          1155.9389364006574,
          1140.2511180191545,
          1158.3255266427086,
          1166.444814452146,
          1159.5967996037746,
          1157.1223357242502,
          1143.5432405993865,
          1151.7218355370449,
          1149.243541998258,
          1152.6051057211641,
          1135.9035819546837,
          1144.225540457925,
          1139.810216915543,
          1142.5649412104283,
          1141.6020004550894,
          1143.0736718691253,
          1145.5783076036757,
          1160.546603689123,
          1159.446987778576,
          1140.6247554187764,
          1140.6411603763536,
          1135.6246516333092,
          1139.6050810231602,
          1130.005322970583,
          1131.1228612360887,
          1132.8138582923023,
          1136.8036354237815,
          1138.4573045680488,
          1142.2812363903963,
          1132.1475408327276,
          1140.7461984166646,
          1130.6594929776404,
          1135.651593606267,
          1129.7735814533803,
          1132.5498820125686,
          1140.698033417158,
          1130.5441409627924,
          1121.5448023503689,
          1138.4845753500115,
          1131.4020010131812,
          1130.6999596411608,
          1137.8379634706487,
          1142.8231713735008,
          1126.9143855812222,
          1121.4554700130495,
          1121.092016244188,
          1129.3134326984534,
          1121.9676253473876,
          1131.1529437474935,
          1121.677810672005,
          1119.283656035967,
          1119.2594416381028,
          1127.1118231620965,
          1122.4961142867942,
          1120.9249356921746,
          1120.0508924766893,
          1122.5373195160935,
          1115.3147805387757,
          1110.8653118714126,
          1123.6418084725365,
          1122.6787703693667,
          1123.7864112219113,
          1121.8384280574808,
          1110.3243686759401,
          1144.0008198807134,
          1128.5046670904899,
          1116.907532998883,
          1114.7911623933164,
          1120.0425867264653,
          1126.5262079110767,
          1123.4447048165073,
          1114.9842184650795,
          1129.6525760189384,
          1113.6377238045482,
          1109.6797720949469,
          1114.841482238586,
          1105.2459223081576,
          1115.3967103042128,
          1112.6483031663574,
          1121.540114282486,
          1107.8093487647964,
          1114.0474134605442,
          1121.4122928353954,
          1111.609789138283,
          1111.924245322996,
          1126.54849510583,
          1120.6938566640454,
          1127.7281348987838,
          1108.9670548944835,
          1109.9129888344462,
          1114.6808936633686,
          1112.0130758673724,
          1115.3762334070536,
          1122.8053401000805,
          1110.8407503552205,
          1105.1812871669465,
          1116.961959052521,
          1116.9696309525862,
          1133.0690373829377,
          1108.035130719754,
          1107.6365069689004,
          1106.5418422615599,
          1108.8337910011342,
          1110.739855044013,
          1108.5650541750347,
          1108.3691562907309,
          1104.6765550391956,
          1105.8852417653086,
          1111.1337422856066,
          1107.3345347806642,
          1111.3343021445603,
          1104.8101885948388,
          1106.7544988552268,
          1109.7660694661588,
          1104.2630991946248,
          1104.7324727151922,
          1101.9914556411122,
          1103.8049260352307,
          1108.0743404286693,
          1100.735310562345,
          1101.4884158284194,
          1109.1951109750587,
          1101.1904026396364,
          1098.5477984458812,
          1098.2803844928455,
          1098.26635352992,
          1099.9442682566469,
          1112.187904028991,
          1098.4684459379544,
          1098.4069697669017,
          1098.7325238980725,
          1102.6929258258974,
          1102.6332666348521,
          1105.200603649567,
          1098.2805976202187,
          1098.3543813524539,
          1095.5475510007661,
          1101.2222636276192,
          1095.3616899299507,
          1093.5400081226815,
          1099.7601373678988,
          1096.937728817582,
          1094.2795055719089,
          1097.7921458095736,
          1101.658923528322,
          1095.487911587706,
          1094.182168339832,
          1091.6906509659489,
          1095.858563466184,
          1091.0426907567096,
          1089.868854235857,
          1097.0610501121275,
          1098.688084968183,
          1108.6430992308517,
          1098.7127428041424,
          1093.4873159961994,
          1102.3357570258845,
          1094.3974170489757,
          1104.35658532497,
          1103.2685529006276,
          1101.4018352746439,
          1094.4443437703655,
          1090.1832464848057,
          1095.6190233278398,
          1093.4624986635174,
          1094.9649683415162,
          1098.146030142237,
          1107.4749836880576,
          1094.7263812181202,
          1098.8889932917382,
          1091.985066593064,
          1091.6775785254551,
          1091.0884143499661,
          1092.2367863721065,
          1093.9583847936276,
          1092.8235314650121,
          1092.6962458593514,
          1100.6263268384519,
          1098.1593438594643,
          1091.6416355375538,
          1092.0530353038996,
          1096.5838279869458,
          1091.6123242941412,
          1089.7996213739707,
          1091.662813867814,
          1090.6756800836472,
          1094.3439375726498,
          1091.5072532089291,
          1090.5375202087928,
          1087.1787713729525,
          1099.1590559771812,
          1085.3403774863473,
          1088.3107779200532,
          1087.6713652273254,
          1086.7578731067965,
          1093.403732712866,
          1081.7130698017972,
          1085.6874876387592,
          1085.1302589858633,
          1091.5108983797327,
          1087.5815143298357,
          1084.3427682237489,
          1090.363680481361,
          1085.6041276900019,
          1088.3299023750815,
          1082.776851332018,
          1084.930508139522,
          1085.3799179135283,
          1085.9659977513802,
          1082.7707015535314,
          1082.128515487925,
          1081.9293689471533,
          1089.919609392624,
          1090.0302877080017,
          1081.2379126402475,
          1086.162085435805,
          1082.4302157778193,
          1081.702941417216,
          1082.6575060489304,
          1079.0048025990623,
          1077.9541753383014,
          1081.423833608006,
          1076.5241866401473,
          1078.8822971464087,
          1084.856091487281,
          1080.9754766935048,
          1088.6721276588662,
          1085.9334296514687,
          1081.248087042913,
          1084.45895238264,
          1086.9080062508224,
          1079.3073910869432,
          1084.5152819204932,
          1077.7144487288042,
          1081.3765001161223,
          1084.0579526789184,
          1082.6189742989977,
          1084.4171160936403,
          1082.649124270956,
          1080.009234850455,
          1081.6440675198876,
          1086.986063104893,
          1079.3563087513865,
          1081.4351777512156,
          1084.677548809522,
          1081.5969096310564,
          1078.3640554797944,
          1078.981688376106,
          1083.0582803374712,
          1084.4266267099144,
          1081.3847817313297,
          1085.1641593582767,
          1083.1536541883897,
          1079.1643587291803,
          1080.1886071020983,
          1083.1039860620033,
          1084.1620929994156,
          1079.8328240828498,
          1083.5733696184298,
          1082.1995111606584,
          1079.4852266038183,
          1079.8199819775557,
          1082.0445975698926,
          1088.4149059037873,
          1081.0524022125494,
          1079.9801770491952,
          1082.0621431395457,
          1081.1866437943731,
          1078.9792503967346,
          1078.1919021927715,
          1078.888624584075,
          1078.9347736487916,
          1080.18230602692,
          1083.6888850709108,
          1078.8776154916845,
          1077.0311793573446,
          1077.9278557354592,
          1079.1528498265411,
          1082.6021484717687,
          1080.5225565474138,
          1077.7901596151949,
          1077.2335129442017,
          1083.1159832948479,
          1076.4014645108532,
          1075.113377178124,
          1080.1381487864542,
          1077.7239345736414,
          1077.199800071005,
          1075.0334844472582,
          1076.0977693859504,
          1077.070508664324,
          1074.776324694205,
          1071.3701231845375,
          1078.7381243968694,
          1082.2120274921829,
          1075.4289482488455,
          1077.3973316952201,
          1076.9711283272054,
          1076.209144581576,
          1077.9070603588862,
          1083.0915057127047,
          1076.5943998830553,
          1075.8956862304883,
          1077.7094336632667,
          1074.4115478319804,
          1081.581929692004,
          1073.1945409137977,
          1075.9023720462264,
          1076.6158444817854,
          1080.6797961320526,
          1076.428163591549,
          1074.943359962465,
          1082.3957380008335,
          1078.1486598065126,
          1077.3585389640018,
          1076.3590760782722,
          1077.5652647077713,
          1073.515726557996,
          1076.4999755712508,
          1077.706972846197,
          1075.2080505505721,
          1076.5400771498082,
          1075.6801362252795,
          1074.4611681044544,
          1076.9524964370253,
          1071.4948151081485,
          1071.6055642864808,
          1078.5860300257232,
          1073.3252761134027,
          1073.3708045703986,
          1070.952632582018,
          1072.125379330987,
          1074.3753630777912,
          1073.729804596443,
          1073.9327276798583,
          1068.1369825217441,
          1069.5791323554522,
          1074.879878161736,
          1073.8060107157517,
          1072.7171002271923,
          1071.8904374028725,
          1073.4661688015794,
          1073.524733594455,
          1076.30552852945,
          1075.6193207074723,
          1072.2902359122,
          1074.9806428354345,
          1074.6988586829664,
          1076.2746989340253,
          1071.944038043836,
          1075.772647090443,
          1074.545436739037,
          1078.458445400424,
          1071.7293751595946,
          1073.9989453781002,
          1072.0086686321938,
          1073.7744578531135,
          1071.5772407429238,
          1069.5561264463386,
          1073.6679521296387,
          1070.1482425840388,
          1074.2672647614265,
          1078.4385045405163,
          1067.4898236880733,
          1068.0231790643957,
          1071.3108036461206,
          1073.8364056116216,
          1073.257761145776,
          1072.612947227244,
          1067.9844127201427,
          1070.4005305395592,
          1072.9083723393715,
          1070.0890221803252,
          1070.6078473709379,
          1070.9138591637084,
          1069.2674152748318,
          1071.3120835608129,
          1069.4706762974358,
          1068.8986784585234,
          1068.8366405241518,
          1070.7304691416432,
          1070.4036263572398,
          1075.5654585467137,
          1069.7881684273643,
          1068.9736722705597,
          1068.8587649666476,
          1069.0852160441366,
          1071.632032689289,
          1071.1212179498727,
          1067.1005168369402,
          1074.945202741973,
          1066.9449358615605,
          1070.775049650576,
          1069.1359792295907,
          1068.9779073295647,
          1070.2676778471682,
          1067.490421433618,
          1067.0230175115457,
          1070.1445921750067,
          1066.993873206078,
          1067.8619723781833,
          1071.431351078233,
          1071.0969859525392,
          1076.558622658169,
          1068.4480732913387,
          1068.1598096457037,
          1068.6004185344786,
          1074.0281170496417,
          1070.5470392943528,
          1068.3518549596138,
          1068.10811031044,
          1071.661203234856,
          1073.6808646083332,
          1069.6844208026807,
          1072.7957817278048,
          1067.8086677216231,
          1071.5131362783472,
          1067.8464441875046,
          1066.7191393423682,
          1068.0486851752055,
          1071.769559252494,
          1070.035134954397,
          1068.7415468939946,
          1066.8374440782363,
          1066.9932479475933,
          1068.6314204529433,
          1070.7417819533914,
          1067.6574206217415
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849,
          1068.4816212092849
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          499
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"32be5b9c-dea4-4912-a0ac-b8dee9555c37\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"32be5b9c-dea4-4912-a0ac-b8dee9555c37\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '32be5b9c-dea4-4912-a0ac-b8dee9555c37',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [1479.1710955809324, 1478.4765967443086, 1456.2947744702633, 1432.9945094305742, 1421.847750097519, 1410.290778191266, 1396.0418759918366, 1389.1999084093252, 1361.762865873145, 1360.6112916532202, 1354.8874023388544, 1360.5650164548922, 1339.8866414004538, 1336.1306244869472, 1324.523360713441, 1317.5453727543174, 1320.4018576472656, 1298.7646605539637, 1301.1991368720783, 1299.2807323210268, 1295.396088129919, 1291.136228804266, 1295.494122144333, 1290.045049208019, 1294.4529196565368, 1272.3308547536094, 1281.6214635346823, 1273.1153185538258, 1268.3522913970855, 1270.2735619691275, 1267.5290281587597, 1259.0840361698229, 1254.4662873226248, 1251.3791520061534, 1254.2196390057509, 1247.0684915301267, 1240.1169982628662, 1242.8749860232308, 1231.3602239602262, 1238.5944600002977, 1233.9732434946095, 1227.7401464510854, 1227.9789916673976, 1219.171756454477, 1222.5838438729759, 1232.1347204636352, 1227.678565956056, 1217.648140512011, 1218.4487247409672, 1214.796293116008, 1211.6522578822126, 1219.9214451470307, 1212.6485268680992, 1205.722408290853, 1205.710633388999, 1206.700134656748, 1188.5234247226383, 1201.2698575133238, 1185.7337772055, 1187.9081130470474, 1197.1926130497889, 1191.7076775837882, 1183.5439175058277, 1188.4704472595545, 1184.7849556552114, 1185.5762871405107, 1182.2232258706622, 1181.0266127233542, 1184.9111751344512, 1184.3600037343185, 1183.7278782990072, 1168.8306421157902, 1173.6044097710687, 1177.2677338276785, 1171.9387939893725, 1166.3347866562438, 1173.0599087657206, 1168.8544990902126, 1170.353598139342, 1163.2526067041888, 1168.193666961835, 1165.9743121667116, 1159.0288745122084, 1155.832406689032, 1157.992686796217, 1157.167956143982, 1157.1595088881481, 1162.1003235854628, 1151.857571654074, 1156.6043502902469, 1153.0556809031607, 1150.765787703302, 1151.0065345924795, 1147.4658169835322, 1147.5010897718835, 1146.5686546005943, 1141.8779126020622, 1148.5282067114733, 1143.7406452087928, 1143.068599525759, 1141.0193783867353, 1151.314724019756, 1146.6681926230856, 1139.5911303226278, 1145.6138258299704, 1143.6042146348084, 1139.2620236687653, 1129.196576528662, 1131.972090349949, 1137.2371000538901, 1136.4237280111884, 1141.601487206581, 1129.5207852672809, 1132.015892492355, 1121.8817909719567, 1131.1014389610252, 1130.9879228229725, 1123.885937372716, 1121.977614037709, 1113.4779801507357, 1127.5247451479315, 1122.6983185334605, 1111.3560622364241, 1122.8158385385989, 1122.2848156182638, 1115.7641737658537, 1122.8533867009303, 1113.7282863668192, 1121.2803188191642, 1116.6992455286088, 1118.2465614699781, 1109.558185584277, 1111.9094846062653, 1120.0928339463856, 1109.8108381352063, 1110.3895727034821, 1114.149719764552, 1102.2724122513457, 1104.2352459779215, 1106.5157588930438, 1110.6225551423936, 1099.0906156544124, 1098.957407374408, 1105.777536365057, 1101.7314611006195, 1107.6616159779098, 1110.6572047039099, 1103.4593726158716, 1100.7220872412804, 1097.9554091859538, 1103.2484763608038, 1096.8034222474528, 1098.6486193938224, 1095.3617385181942, 1097.6424013959734, 1099.7918988540125, 1101.3706232640411, 1099.0456500867099, 1093.6811197149698, 1097.1899235620033, 1095.6071182290752, 1092.163282028391, 1090.5527540305968, 1087.647130635355, 1083.3099654316065, 1088.7746009008188, 1081.9558765632441, 1084.553932558637, 1096.6743850417336, 1080.9061797000327, 1089.6535506721773, 1087.6543102643043, 1089.2386911781177, 1082.6464459205263, 1077.2208532467619, 1083.835680435147, 1089.1141632551273, 1085.7231255365514, 1085.545887006412, 1086.0896644508143, 1090.6210979356874, 1086.6635386036899, 1083.0333331995218, 1087.631673405542, 1076.7153299996005, 1072.7645704515335, 1074.1802804723923, 1077.649906480482, 1073.0229086347158, 1075.2931641555153, 1077.4007233749155, 1078.1730087827962, 1076.0336253409255, 1081.6388991749643, 1075.6303818394622, 1073.3042334875363, 1074.8630758575812, 1067.6237846330146, 1079.4711325373705, 1066.869694433065, 1072.903041291146, 1070.9637401971688, 1067.0354647425102, 1069.2705670975004, 1074.2362869045455, 1065.1269013791327, 1068.7415971467221, 1067.5111321662887, 1071.967360131459, 1058.1720676762511, 1058.5400574207783, 1064.786223355529, 1069.293123424615, 1078.1324037223344, 1069.4379532781325, 1067.9501632222293, 1064.9377645550305, 1061.5893967626375, 1059.7535897532805, 1066.5150415494347, 1058.752818509766, 1057.8209747687738, 1060.3680741069359, 1062.3010327339364, 1055.2285463445191, 1056.9254402388592, 1052.8486186643868, 1055.7096891370688, 1068.2795827378725, 1066.2490892825251, 1051.78999672831, 1053.2671552236989, 1058.7505101887955, 1054.3449556493367, 1044.1197369753156, 1056.3029200873443, 1058.3870866376603, 1051.4091137812804, 1054.4109052922363, 1050.877394849656, 1058.0308079816116, 1064.3575855574102, 1054.7333237070682, 1050.4844280064692, 1049.441353549502, 1051.001821900026, 1044.5004591283039, 1053.0864471013306, 1055.8172466430296, 1047.7582030907313, 1042.3705942579613, 1042.541837023331, 1041.4830783915133, 1044.1705834339205, 1046.8112754293977, 1043.7739622530298, 1044.9787284219244, 1043.3509731258303, 1040.8289751596337, 1047.7379949832455, 1049.6030983780486, 1035.0928104233114, 1052.5637548740006, 1039.2675312501958, 1047.7176484543406, 1050.284692152722, 1050.6969286037252, 1038.0868626103652, 1042.2230064280411, 1044.7782610642162, 1041.615070594679, 1042.671694305585, 1038.3686839322277, 1045.947847753769, 1036.6282968041126, 1039.670145723608, 1041.4607371255106, 1039.5285581378773, 1041.760441380989, 1039.5341315888106, 1041.662766160768, 1038.427169784506, 1044.4983877270356, 1039.5947357905945, 1038.6514308295318, 1038.511969352739, 1029.474300123681, 1036.122518413598, 1042.8723836148793, 1045.0998731712218, 1041.44438530903, 1031.273298907237, 1029.0365994187425, 1036.8133149117393, 1040.8759545080882, 1039.320374795758, 1027.7661231212871, 1029.8932788972031, 1027.7949204063377, 1031.8900637017573, 1028.0448350036265, 1038.5903687600457, 1031.0560984928, 1031.6166352462692, 1028.9808826679837, 1028.3475518281125, 1029.7385226246445, 1030.5572124976109, 1036.60004977786, 1027.20375030744, 1023.3148182833962, 1026.3561884557075, 1033.2681001977212, 1028.767263635452, 1025.3554305158254, 1023.3639424539937, 1031.561162415836, 1029.8695758385866, 1024.4548564793854, 1027.53613100115, 1030.3116196838341, 1035.5055500012925, 1028.4000784853117, 1012.9361311210906, 1020.5283115617207, 1035.7853170562798, 1023.9272829938895, 1028.7956260251262, 1020.3084309487872, 1024.6392683721817, 1022.9456315668785, 1030.9567939141768, 1021.199425611081, 1034.6044853826982, 1023.9674423398296, 1028.770064521271, 1017.3987643507886, 1014.5626683576518, 1018.6887773445333, 1024.6499170793281, 1023.3831544535323, 1017.0940801552787, 1025.3682556091148, 1024.6219818500758, 1032.8687426909573, 1019.3266760178026, 1015.6693592060059, 1016.89236645405, 1022.2132283438895, 1019.0278899944543, 1023.2228385109879, 1018.9325660780529, 1017.4261728687948, 1013.8179214199105, 1020.7637844234854, 1019.3767160829093, 1025.208039303067, 1019.9602436794651, 1013.7748722361735, 1011.396922281134, 1021.2052875072845, 1014.7927210601079, 1014.9101015300915, 1014.1908331045483, 1015.1420745084681, 1015.0883956734776, 1018.268640347227, 1016.7579232983296, 1009.722869989316, 1012.9511340447076, 1015.2676338240167, 1014.0950380412902, 1019.1992319801993, 1014.9360640277025, 1009.5271288847097, 1013.7257729838792, 1016.6018259290562, 1011.4402990499909, 1012.799562872257, 1007.819596074305, 1013.7762476628687, 1011.7933881265118, 1008.1068554486783, 1017.9445540950417, 1014.4323551903896, 1011.591984178395, 1014.6875272926405, 1019.0836270005138, 1010.0353933900016, 1004.6738435826704, 1013.4659557476392, 1005.7694120435789, 1001.8679323565489, 1005.1436250853783, 1008.5503862777597, 1008.359009572349, 1012.3163743800287, 1008.0001358267824, 1011.2981479140687, 1009.5039027252105, 1002.7706774062758, 1003.6556011205306, 1007.4167463701523, 1008.6364664093439, 1004.6988516823428, 997.315929975064, 1014.1619237116112, 1001.058932668297, 1000.432825370377, 1013.215073558929, 1008.0673358887404, 1008.3024655853266, 1009.3935387525144, 1008.4260263452554, 1005.32335991302, 1009.0002603203877, 1004.9053665262103, 1003.6211460466158, 997.6806638422007, 1003.0434907248103, 998.9487045590997, 1001.7128440561671, 998.8078986566244, 999.2872536098739, 1000.6965984239687, 1004.6757881527127, 1004.8193130998017, 1002.5581846174076, 1001.7214379212123, 1005.6415771582285, 1006.6626131995923, 1005.5968023697551, 1006.8685621434088, 999.4486020097567, 999.1727386713267, 997.4909353185469, 991.4171943366229, 999.9213083656178, 999.530057788923, 997.2419937467298, 1002.4972874419781, 994.6033563485767, 998.9125631426866, 993.780129863524, 998.7216636056481, 997.2907149775941, 991.4253805989322, 994.1642120098193, 996.7141032015271, 993.9085906560182, 998.2526724757807, 998.8732498686037, 993.2532948458962, 996.1545659706447, 992.876982033169, 997.5419532801886, 989.2039724426086, 990.4069637943419, 999.2458536359574, 1000.0855166425871, 995.4734851752252, 998.5323163467202, 999.6690326856664, 988.7256114922235, 992.2919821149246, 991.8154857903988, 993.4138618929725, 997.6677925350051, 991.7094930706555, 990.7968035618193, 994.3615546172, 990.9129299288502, 991.6609070546311, 996.0530211680251, 1003.7048812753002, 991.0919293116203, 992.0792413791482, 994.8451442867666, 994.7424146041426, 993.3577243092783, 987.2653823035596, 997.6326555367197, 990.5831362837322, 995.4984891136878, 995.6133239880721, 985.1455190355275, 992.1394364483399, 993.9687420937014, 977.2361183755498, 985.7961252846268]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [2775.1356840774292, 2201.127991714003, 1955.9487683357588, 1604.2625012287808, 1446.1908639709338, 1419.4898753115522, 1291.174309111894, 1366.0733655796084, 1300.245912982152, 1254.1384557858244, 1310.5605882423206, 1320.130831372506, 1269.0961764993278, 1277.5867690565783, 1214.160863068196, 1263.319678502975, 1239.7793187825455, 1211.0601255157942, 1200.4196176171326, 1190.781071973655, 1204.6287338780046, 1204.9877476115635, 1197.4475817790317, 1219.0026660626222, 1209.3863825242506, 1192.774026751781, 1184.3886916529852, 1190.4662691356903, 1202.560816864081, 1193.3278842128204, 1206.127285997304, 1177.99021613911, 1187.1225469423625, 1183.0422531687857, 1182.1080589072603, 1196.1985853404399, 1176.4162639268347, 1171.1804615584504, 1174.0241396429544, 1160.681657269839, 1178.782575345312, 1170.3437927870275, 1167.489429964196, 1178.8715265892492, 1169.379618869412, 1175.7766987869634, 1172.056096191896, 1157.3867852559233, 1161.9171132787044, 1161.4154783393856, 1161.039147413157, 1162.8586268634388, 1152.3171550719371, 1160.649280267367, 1168.8807380145408, 1153.2549814331715, 1151.754746691985, 1156.9638833917404, 1156.2140083014667, 1160.1411063051041, 1178.381439692419, 1153.5182578953325, 1155.9389364006574, 1140.2511180191545, 1158.3255266427086, 1166.444814452146, 1159.5967996037746, 1157.1223357242502, 1143.5432405993865, 1151.7218355370449, 1149.243541998258, 1152.6051057211641, 1135.9035819546837, 1144.225540457925, 1139.810216915543, 1142.5649412104283, 1141.6020004550894, 1143.0736718691253, 1145.5783076036757, 1160.546603689123, 1159.446987778576, 1140.6247554187764, 1140.6411603763536, 1135.6246516333092, 1139.6050810231602, 1130.005322970583, 1131.1228612360887, 1132.8138582923023, 1136.8036354237815, 1138.4573045680488, 1142.2812363903963, 1132.1475408327276, 1140.7461984166646, 1130.6594929776404, 1135.651593606267, 1129.7735814533803, 1132.5498820125686, 1140.698033417158, 1130.5441409627924, 1121.5448023503689, 1138.4845753500115, 1131.4020010131812, 1130.6999596411608, 1137.8379634706487, 1142.8231713735008, 1126.9143855812222, 1121.4554700130495, 1121.092016244188, 1129.3134326984534, 1121.9676253473876, 1131.1529437474935, 1121.677810672005, 1119.283656035967, 1119.2594416381028, 1127.1118231620965, 1122.4961142867942, 1120.9249356921746, 1120.0508924766893, 1122.5373195160935, 1115.3147805387757, 1110.8653118714126, 1123.6418084725365, 1122.6787703693667, 1123.7864112219113, 1121.8384280574808, 1110.3243686759401, 1144.0008198807134, 1128.5046670904899, 1116.907532998883, 1114.7911623933164, 1120.0425867264653, 1126.5262079110767, 1123.4447048165073, 1114.9842184650795, 1129.6525760189384, 1113.6377238045482, 1109.6797720949469, 1114.841482238586, 1105.2459223081576, 1115.3967103042128, 1112.6483031663574, 1121.540114282486, 1107.8093487647964, 1114.0474134605442, 1121.4122928353954, 1111.609789138283, 1111.924245322996, 1126.54849510583, 1120.6938566640454, 1127.7281348987838, 1108.9670548944835, 1109.9129888344462, 1114.6808936633686, 1112.0130758673724, 1115.3762334070536, 1122.8053401000805, 1110.8407503552205, 1105.1812871669465, 1116.961959052521, 1116.9696309525862, 1133.0690373829377, 1108.035130719754, 1107.6365069689004, 1106.5418422615599, 1108.8337910011342, 1110.739855044013, 1108.5650541750347, 1108.3691562907309, 1104.6765550391956, 1105.8852417653086, 1111.1337422856066, 1107.3345347806642, 1111.3343021445603, 1104.8101885948388, 1106.7544988552268, 1109.7660694661588, 1104.2630991946248, 1104.7324727151922, 1101.9914556411122, 1103.8049260352307, 1108.0743404286693, 1100.735310562345, 1101.4884158284194, 1109.1951109750587, 1101.1904026396364, 1098.5477984458812, 1098.2803844928455, 1098.26635352992, 1099.9442682566469, 1112.187904028991, 1098.4684459379544, 1098.4069697669017, 1098.7325238980725, 1102.6929258258974, 1102.6332666348521, 1105.200603649567, 1098.2805976202187, 1098.3543813524539, 1095.5475510007661, 1101.2222636276192, 1095.3616899299507, 1093.5400081226815, 1099.7601373678988, 1096.937728817582, 1094.2795055719089, 1097.7921458095736, 1101.658923528322, 1095.487911587706, 1094.182168339832, 1091.6906509659489, 1095.858563466184, 1091.0426907567096, 1089.868854235857, 1097.0610501121275, 1098.688084968183, 1108.6430992308517, 1098.7127428041424, 1093.4873159961994, 1102.3357570258845, 1094.3974170489757, 1104.35658532497, 1103.2685529006276, 1101.4018352746439, 1094.4443437703655, 1090.1832464848057, 1095.6190233278398, 1093.4624986635174, 1094.9649683415162, 1098.146030142237, 1107.4749836880576, 1094.7263812181202, 1098.8889932917382, 1091.985066593064, 1091.6775785254551, 1091.0884143499661, 1092.2367863721065, 1093.9583847936276, 1092.8235314650121, 1092.6962458593514, 1100.6263268384519, 1098.1593438594643, 1091.6416355375538, 1092.0530353038996, 1096.5838279869458, 1091.6123242941412, 1089.7996213739707, 1091.662813867814, 1090.6756800836472, 1094.3439375726498, 1091.5072532089291, 1090.5375202087928, 1087.1787713729525, 1099.1590559771812, 1085.3403774863473, 1088.3107779200532, 1087.6713652273254, 1086.7578731067965, 1093.403732712866, 1081.7130698017972, 1085.6874876387592, 1085.1302589858633, 1091.5108983797327, 1087.5815143298357, 1084.3427682237489, 1090.363680481361, 1085.6041276900019, 1088.3299023750815, 1082.776851332018, 1084.930508139522, 1085.3799179135283, 1085.9659977513802, 1082.7707015535314, 1082.128515487925, 1081.9293689471533, 1089.919609392624, 1090.0302877080017, 1081.2379126402475, 1086.162085435805, 1082.4302157778193, 1081.702941417216, 1082.6575060489304, 1079.0048025990623, 1077.9541753383014, 1081.423833608006, 1076.5241866401473, 1078.8822971464087, 1084.856091487281, 1080.9754766935048, 1088.6721276588662, 1085.9334296514687, 1081.248087042913, 1084.45895238264, 1086.9080062508224, 1079.3073910869432, 1084.5152819204932, 1077.7144487288042, 1081.3765001161223, 1084.0579526789184, 1082.6189742989977, 1084.4171160936403, 1082.649124270956, 1080.009234850455, 1081.6440675198876, 1086.986063104893, 1079.3563087513865, 1081.4351777512156, 1084.677548809522, 1081.5969096310564, 1078.3640554797944, 1078.981688376106, 1083.0582803374712, 1084.4266267099144, 1081.3847817313297, 1085.1641593582767, 1083.1536541883897, 1079.1643587291803, 1080.1886071020983, 1083.1039860620033, 1084.1620929994156, 1079.8328240828498, 1083.5733696184298, 1082.1995111606584, 1079.4852266038183, 1079.8199819775557, 1082.0445975698926, 1088.4149059037873, 1081.0524022125494, 1079.9801770491952, 1082.0621431395457, 1081.1866437943731, 1078.9792503967346, 1078.1919021927715, 1078.888624584075, 1078.9347736487916, 1080.18230602692, 1083.6888850709108, 1078.8776154916845, 1077.0311793573446, 1077.9278557354592, 1079.1528498265411, 1082.6021484717687, 1080.5225565474138, 1077.7901596151949, 1077.2335129442017, 1083.1159832948479, 1076.4014645108532, 1075.113377178124, 1080.1381487864542, 1077.7239345736414, 1077.199800071005, 1075.0334844472582, 1076.0977693859504, 1077.070508664324, 1074.776324694205, 1071.3701231845375, 1078.7381243968694, 1082.2120274921829, 1075.4289482488455, 1077.3973316952201, 1076.9711283272054, 1076.209144581576, 1077.9070603588862, 1083.0915057127047, 1076.5943998830553, 1075.8956862304883, 1077.7094336632667, 1074.4115478319804, 1081.581929692004, 1073.1945409137977, 1075.9023720462264, 1076.6158444817854, 1080.6797961320526, 1076.428163591549, 1074.943359962465, 1082.3957380008335, 1078.1486598065126, 1077.3585389640018, 1076.3590760782722, 1077.5652647077713, 1073.515726557996, 1076.4999755712508, 1077.706972846197, 1075.2080505505721, 1076.5400771498082, 1075.6801362252795, 1074.4611681044544, 1076.9524964370253, 1071.4948151081485, 1071.6055642864808, 1078.5860300257232, 1073.3252761134027, 1073.3708045703986, 1070.952632582018, 1072.125379330987, 1074.3753630777912, 1073.729804596443, 1073.9327276798583, 1068.1369825217441, 1069.5791323554522, 1074.879878161736, 1073.8060107157517, 1072.7171002271923, 1071.8904374028725, 1073.4661688015794, 1073.524733594455, 1076.30552852945, 1075.6193207074723, 1072.2902359122, 1074.9806428354345, 1074.6988586829664, 1076.2746989340253, 1071.944038043836, 1075.772647090443, 1074.545436739037, 1078.458445400424, 1071.7293751595946, 1073.9989453781002, 1072.0086686321938, 1073.7744578531135, 1071.5772407429238, 1069.5561264463386, 1073.6679521296387, 1070.1482425840388, 1074.2672647614265, 1078.4385045405163, 1067.4898236880733, 1068.0231790643957, 1071.3108036461206, 1073.8364056116216, 1073.257761145776, 1072.612947227244, 1067.9844127201427, 1070.4005305395592, 1072.9083723393715, 1070.0890221803252, 1070.6078473709379, 1070.9138591637084, 1069.2674152748318, 1071.3120835608129, 1069.4706762974358, 1068.8986784585234, 1068.8366405241518, 1070.7304691416432, 1070.4036263572398, 1075.5654585467137, 1069.7881684273643, 1068.9736722705597, 1068.8587649666476, 1069.0852160441366, 1071.632032689289, 1071.1212179498727, 1067.1005168369402, 1074.945202741973, 1066.9449358615605, 1070.775049650576, 1069.1359792295907, 1068.9779073295647, 1070.2676778471682, 1067.490421433618, 1067.0230175115457, 1070.1445921750067, 1066.993873206078, 1067.8619723781833, 1071.431351078233, 1071.0969859525392, 1076.558622658169, 1068.4480732913387, 1068.1598096457037, 1068.6004185344786, 1074.0281170496417, 1070.5470392943528, 1068.3518549596138, 1068.10811031044, 1071.661203234856, 1073.6808646083332, 1069.6844208026807, 1072.7957817278048, 1067.8086677216231, 1071.5131362783472, 1067.8464441875046, 1066.7191393423682, 1068.0486851752055, 1071.769559252494, 1070.035134954397, 1068.7415468939946, 1066.8374440782363, 1066.9932479475933, 1068.6314204529433, 1070.7417819533914, 1067.6574206217415]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849, 1068.4816212092849]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 499], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('32be5b9c-dea4-4912-a0ac-b8dee9555c37');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing most recent weights file - as only best epochs are saved it will \n",
    "# be equivalent to lowest error\n",
    "list_of_files = glob.glob('Weights/*') \n",
    "latest_file = max(list_of_files, key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading best weights and compiling model\n",
    "weights_file =  latest_file \n",
    "model.load_weights(weights_file) \n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.87% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1067   : Mean absolute error \n",
      "\n",
      "9.13% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing model performance as reference for tracking progress, evaluating key KPIs\n",
    "previous_val_loss.append(np.asarray(history.history[\"val_loss\"]).min())\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining validation loss with model names\n",
    "models=[\"basic_model\",\"dropout\",\"batch_norm\",\"leakyRELU\",\"1024_layer\",\"lr_decay\",\"callbacks\"]\n",
    "df_comparison=pd.DataFrame(previous_val_loss,columns=[\"val_loss\"])\n",
    "df_comparison[\"model\"]=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "model=%{x}<br>val_loss=%{y}",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "basic_model",
          "dropout",
          "batch_norm",
          "leakyRELU",
          "1024_layer",
          "lr_decay",
          "callbacks"
         ],
         "xaxis": "x",
         "y": [
          1189.305595892093,
          1152.263653981798,
          1112.9759580230866,
          1113.1894470300515,
          1090.821307445256,
          1068.4816212092849,
          1066.7191393423682
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "val_loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"84f04a30-a832-444a-9bf6-8577ad6918b5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"84f04a30-a832-444a-9bf6-8577ad6918b5\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '84f04a30-a832-444a-9bf6-8577ad6918b5',\n",
       "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"model=%{x}<br>val_loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"basic_model\", \"dropout\", \"batch_norm\", \"leakyRELU\", \"1024_layer\", \"lr_decay\", \"callbacks\"], \"xaxis\": \"x\", \"y\": [1189.305595892093, 1152.263653981798, 1112.9759580230866, 1113.1894470300515, 1090.821307445256, 1068.4816212092849, 1066.7191393423682], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"model\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"val_loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('84f04a30-a832-444a-9bf6-8577ad6918b5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting model evolution\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_comparison, x=\"model\", y=\"val_loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "528px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
