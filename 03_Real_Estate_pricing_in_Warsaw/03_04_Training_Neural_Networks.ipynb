{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import timeit \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cufflinks wrapper on plotly\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "\n",
    "# Set global theme\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geojsoncontour\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_summary(model, X_test, y_test ):\n",
    "    \n",
    "    y_hat=model.predict(X_test)\n",
    "    \n",
    "    df_summary=pd.DataFrame(y_hat, columns=[\"y_hat\"])\n",
    "    df_summary[\"y_true\"]=y_test\n",
    "    df_summary[\"abs_error\"]=np.abs(df_summary.y_true-df_summary.y_hat)\n",
    "    df_summary[\"error\"]=df_summary.y_hat-df_summary.y_true\n",
    "    df_summary[\"relative_error\"]= df_summary[\"error\"]/df_summary.y_true\n",
    "    df_summary[\"relative_abs_error\"]= df_summary[\"abs_error\"]/df_summary.y_true\n",
    "    \n",
    "    share_within_5pct=(df_summary.query(\"relative_abs_error<0.05\").shape[0]/df_summary.shape[0])*100\n",
    "    \n",
    "    print(\"{:.2f}% : Share of forecasts within 5% absolute error\\n\".format(share_within_5pct))\n",
    "    print(\"{:.0f}   : Mean absolute error \\n\".format(df_summary.abs_error.mean()))\n",
    "    print(\"{:.2f}% : Mean absolute percentage error\\n\".format(df_summary.relative_abs_error.mean()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_plot_loss(history, starting_epoch,previous_val_loss):\n",
    "\n",
    "        trace0=go.Scatter(\n",
    "                y=history.history['loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"blue\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Training Loss\"\n",
    "            )\n",
    "\n",
    "\n",
    "        trace1=go.Scatter(\n",
    "                y=history.history['val_loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"red\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Validation Loss\"\n",
    "            )\n",
    "        \n",
    "        trace2=go.Scatter(\n",
    "                y=list(np.ones([len(history.epoch[starting_epoch:])])*np.asarray(previous_val_loss).min()),\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"grey\",\n",
    "                size=5,\n",
    "\n",
    "                ),\n",
    "                name=\"Lowest error from previous models\"\n",
    "            )\n",
    "\n",
    "        data=[trace0, trace1,trace2]\n",
    "        figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning curve\",\n",
    "                yaxis=dict(title=\"Loss\",range=(900,1500)),\n",
    "                xaxis=dict(title=\"Epoch\",range=(starting_epoch,history.epoch[-1])),\n",
    "                legend=dict(\n",
    "                    x=1,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None\n",
    "\n",
    "\n",
    "            )))\n",
    "        iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a central city point to center all graphs around - Swietokrzyska Subway \n",
    "center_coors=52.235176, 21.008393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24935, 46)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"https://raw.githubusercontent.com/Jan-Majewski/Project_Portfolio/master/03_Real_Estate_pricing_in_Warsaw/top_features_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.unit_price\n",
    "X=df.drop(columns=[\"unit_price\",\"lat_mod\",\"lon_mod\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx=np.asarray(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_coors=df[[ \"unit_price\",\"lat_mod\",\"lon_mod\"]].iloc[test_idx]\n",
    "X_test_coors.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['build_year', 'building_floors_num', 'rooms_num',\n",
       "       'Equipment_types_dishwasher', 'Equipment_types_fridge',\n",
       "       'Equipment_types_furniture', 'Equipment_types_tv',\n",
       "       'Equipment_types_washing_machine', 'Extras_types_air_conditioning',\n",
       "       'Extras_types_balcony', 'Extras_types_garden', 'Extras_types_lift',\n",
       "       'floor_num', 'east_bank', 'distance_driving', 'distance_transit',\n",
       "       'time_driving', 'time_transit', 'restaurant_price_level',\n",
       "       'restaurant_mean_rating', 'restaurant_mean_popularity',\n",
       "       'restaurant_count', 'restaurant_ratings_count', 'district_Bemowo',\n",
       "       'district_Bialoleka', 'district_Downtown', 'district_Subburbs',\n",
       "       'district_Targowek', 'district_Wawer', 'district_Wola',\n",
       "       'district_Zoliborz', 'market_primary', 'Building_material_brick',\n",
       "       'Building_ownership_full_ownership', 'Building_type_apartment',\n",
       "       'Building_type_block', 'Building_type_tenement',\n",
       "       'Construction_status_ready_to_use', 'Construction_status_to_completion',\n",
       "       'Heating_urban', 'Windows_type_aluminium', 'Windows_type_plastic',\n",
       "       'Windows_type_wooden', 'unit_price', 'lat_mod', 'lon_mod'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and scaling data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19948, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np.asarray(y_train).reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11195.887256867856"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=np.asarray(y_test).reshape(-1,1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11128.766392620813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19948, 43)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 43)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Initial_model\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Initial_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 3s 153us/sample - loss: 11118.3712 - val_loss: 10586.6312\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 7066.7532 - val_loss: 3763.7940\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 2550.48 - 0s 18us/sample - loss: 2464.6214 - val_loss: 1684.2396\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1628.4329 - val_loss: 1496.9288\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1463.1655 - val_loss: 1407.6184\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1387.9317 - val_loss: 1362.7423\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1347.6675 - val_loss: 1336.8411\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1319.9526 - val_loss: 1317.9871\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1298.9918 - val_loss: 1306.9242\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1286.6853 - val_loss: 1288.8674\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1266.8067 - val_loss: 1279.4219\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1256.6964 - val_loss: 1272.4301\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1246.2823 - val_loss: 1266.6113\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1237.2830 - val_loss: 1258.3193\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1230.6447 - val_loss: 1257.2213\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1223.1365 - val_loss: 1250.9842\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1216.7042 - val_loss: 1255.9366\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1211.3147 - val_loss: 1244.1626\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1206.3111 - val_loss: 1248.4659\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1202.4708 - val_loss: 1239.4952\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1196.9696 - val_loss: 1248.7253\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1197.1793 - val_loss: 1238.1981\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1193.1343 - val_loss: 1231.5101\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1185.8137 - val_loss: 1229.5338\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1180.6904 - val_loss: 1227.2803\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1177.5097 - val_loss: 1233.0450\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1178.1076 - val_loss: 1236.4010\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1170.7666 - val_loss: 1223.6072\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1166.2315 - val_loss: 1222.0867\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1162.8833 - val_loss: 1220.1124\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1160.5255 - val_loss: 1218.5866\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1155.9453 - val_loss: 1218.4465\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1156.1949 - val_loss: 1217.2312\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1152.5997 - val_loss: 1213.6029\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1148.6161 - val_loss: 1220.6182\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1153.4612 - val_loss: 1214.0547\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1145.6265 - val_loss: 1242.1274\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1156.7491 - val_loss: 1215.2261\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1143.4673 - val_loss: 1222.1613\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1139.4550 - val_loss: 1208.8892\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1133.9171 - val_loss: 1208.6677\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1132.8796 - val_loss: 1206.0362\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1130.0576 - val_loss: 1208.6641\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1130.9783 - val_loss: 1203.3641\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1129.3228 - val_loss: 1208.0661\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1124.1213 - val_loss: 1204.5232\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1121.1197 - val_loss: 1202.6409\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1116.3776 - val_loss: 1205.9033\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1115.1090 - val_loss: 1200.7536\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1112.1373 - val_loss: 1196.8280\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1111.4558 - val_loss: 1201.8723\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1112.7550 - val_loss: 1198.8129\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1105.0217 - val_loss: 1207.2609\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1109.6441 - val_loss: 1199.1299\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1099.7231 - val_loss: 1198.5406\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1097.0701 - val_loss: 1194.7596\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1098.9161 - val_loss: 1205.2827\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1098.2303 - val_loss: 1189.7387\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1085.3657 - val_loss: 1194.0971\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1087.3683 - val_loss: 1191.2243\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1082.3032 - val_loss: 1186.1028\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1084.3062 - val_loss: 1202.0115\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1091.1825 - val_loss: 1204.4265\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1076.9177 - val_loss: 1188.5765\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1065.0981 - val_loss: 1181.2816\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1065.3543 - val_loss: 1185.4537\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1061.9579 - val_loss: 1179.5669\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1061.0261 - val_loss: 1183.6878\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1055.9787 - val_loss: 1181.0440\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1049.4482 - val_loss: 1180.4160\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1050.4536 - val_loss: 1181.8914\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1041.5853 - val_loss: 1177.2874\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1038.3217 - val_loss: 1182.3154\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1035.1507 - val_loss: 1193.8523\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1040.1218 - val_loss: 1183.0535\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1032.4921 - val_loss: 1186.0599\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1021.8529 - val_loss: 1172.1018\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1012.7016 - val_loss: 1173.8347\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1014.0081 - val_loss: 1191.0554\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1014.2976 - val_loss: 1170.2613\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1011.9299 - val_loss: 1186.7581\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1019.7613 - val_loss: 1188.1704\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1008.0780 - val_loss: 1171.4328\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1001.3345 - val_loss: 1196.8587\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 994.1019 - val_loss: 1175.2893\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 987.3112 - val_loss: 1163.3847\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 980.4431 - val_loss: 1192.7050\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1000.1788 - val_loss: 1167.8774\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 987.2779 - val_loss: 1172.0597\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 978.8411 - val_loss: 1173.3214\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 971.9390 - val_loss: 1178.4457\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 972.3974 - val_loss: 1174.0351\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 968.5074 - val_loss: 1170.1270\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 975.2903 - val_loss: 1201.0608\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 975.0878 - val_loss: 1173.1109\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 952.1126 - val_loss: 1174.6473\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 969.9683 - val_loss: 1200.1404\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 957.7118 - val_loss: 1190.9284\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 944.8714 - val_loss: 1169.6952\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 948.7823 - val_loss: 1171.2743\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 940.6787 - val_loss: 1188.9244\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 944.1065 - val_loss: 1180.7162\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 931.4105 - val_loss: 1173.3912\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 922.6698 - val_loss: 1169.8859\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 914.0831 - val_loss: 1158.1489\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 912.8027 - val_loss: 1164.9452\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 912.0500 - val_loss: 1186.7738\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 909.4973 - val_loss: 1172.3863\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 909.2467 - val_loss: 1170.1474\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 899.1171 - val_loss: 1181.1836\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 912.4480 - val_loss: 1172.2081\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 891.8297 - val_loss: 1172.1553\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 907.0121 - val_loss: 1183.3680\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 890.1119 - val_loss: 1168.1826\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 890.0562 - val_loss: 1178.9255\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 882.7776 - val_loss: 1176.4514\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 890.6592 - val_loss: 1183.5731\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 875.0344 - val_loss: 1174.1515\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 878.8147 - val_loss: 1178.9799\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 875.5967 - val_loss: 1165.2429\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 862.9883 - val_loss: 1170.5057\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 870.0373 - val_loss: 1185.3070\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 850.6343 - val_loss: 1167.2157\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 855.2705 - val_loss: 1174.3231\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 864.7072 - val_loss: 1176.0756\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 849.4837 - val_loss: 1182.9686\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 852.3675 - val_loss: 1180.5704\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 846.9860 - val_loss: 1205.0612\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 855.2829 - val_loss: 1174.1754\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 838.8879 - val_loss: 1194.9680\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 839.5845 - val_loss: 1193.6322\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 841.3837 - val_loss: 1218.7171\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 846.8096 - val_loss: 1181.7108\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 831.6812 - val_loss: 1180.7450\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 825.4580 - val_loss: 1178.0169\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 827.1971 - val_loss: 1175.6190\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 821.9884 - val_loss: 1178.7540\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 808.7685 - val_loss: 1179.8893\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 807.0219 - val_loss: 1179.1867\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 807.3701 - val_loss: 1184.7008\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 813.8104 - val_loss: 1192.4862\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 805.8432 - val_loss: 1191.6409\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 802.3203 - val_loss: 1178.7827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 799.8053 - val_loss: 1178.5600\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 803.7892 - val_loss: 1181.5303\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 785.4978 - val_loss: 1191.8789\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 780.3535 - val_loss: 1180.4938\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 792.4177 - val_loss: 1195.4573\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 813.9468 - val_loss: 1200.4485\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 800.3382 - val_loss: 1185.5327\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 789.7976 - val_loss: 1180.2982\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 789.8404 - val_loss: 1182.2539\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 767.8329 - val_loss: 1179.8586\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 775.2201 - val_loss: 1180.2238\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 762.2565 - val_loss: 1178.3734\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 760.6915 - val_loss: 1190.8613\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 763.0366 - val_loss: 1222.9257\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 775.5654 - val_loss: 1185.2536\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 775.1434 - val_loss: 1181.3857\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 782.2500 - val_loss: 1258.2621\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 827.1841 - val_loss: 1198.0467\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 793.3800 - val_loss: 1181.4904\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 759.2697 - val_loss: 1205.6129\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 765.8176 - val_loss: 1186.9915\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 764.2975 - val_loss: 1181.5929\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 759.3735 - val_loss: 1184.1196\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 756.2271 - val_loss: 1181.3891\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 737.2195 - val_loss: 1182.6574\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 736.4565 - val_loss: 1181.4695\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 728.3792 - val_loss: 1183.9658\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 725.3777 - val_loss: 1179.7625\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 719.4076 - val_loss: 1189.6516\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 724.2060 - val_loss: 1185.8147\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 725.4163 - val_loss: 1188.2294\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 727.1411 - val_loss: 1198.5365\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 721.2924 - val_loss: 1195.7513\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 732.8086 - val_loss: 1196.9849\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 715.7472 - val_loss: 1184.3692\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 716.8895 - val_loss: 1186.2994\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 726.1431 - val_loss: 1210.9987\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 760.1792 - val_loss: 1275.4871\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 773.8848 - val_loss: 1243.3323\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 737.0054 - val_loss: 1193.2940\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 732.4947 - val_loss: 1194.4968\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 717.2285 - val_loss: 1182.2175\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 701.1899 - val_loss: 1187.5057\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 705.1546 - val_loss: 1192.3180\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 704.0331 - val_loss: 1207.4441\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 702.9418 - val_loss: 1196.4641\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 696.8276 - val_loss: 1198.5180\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 697.3914 - val_loss: 1196.9992\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 691.4541 - val_loss: 1194.8951\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 695.9164 - val_loss: 1198.3758\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 703.4133 - val_loss: 1252.9899\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 741.7173 - val_loss: 1224.1827\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 709.1251 - val_loss: 1210.0897\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 690.0927 - val_loss: 1194.1337\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 683.2354 - val_loss: 1195.4847\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 689.5171 - val_loss: 1197.1003\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 688.4638 - val_loss: 1190.6140\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1266.806673498557,
          1256.6963538794616,
          1246.2823334321579,
          1237.2830432611117,
          1230.6446730982977,
          1223.136516123562,
          1216.704225737895,
          1211.314654111432,
          1206.3111273882407,
          1202.4707678440507,
          1196.9696495066862,
          1197.179255468515,
          1193.134318025695,
          1185.8136979143821,
          1180.6904070945786,
          1177.5096632837174,
          1178.107604307332,
          1170.7666139482185,
          1166.231498747329,
          1162.8832743208513,
          1160.525480996685,
          1155.9452931870912,
          1156.194921982702,
          1152.5996979842905,
          1148.6161124147393,
          1153.4611763534408,
          1145.6265372488,
          1156.7491146659047,
          1143.467257153756,
          1139.4549846936018,
          1133.9171238775505,
          1132.8795723617734,
          1130.0575849993186,
          1130.978327244586,
          1129.3228071207795,
          1124.1212570652447,
          1121.1196537755977,
          1116.377574295708,
          1115.1090446733774,
          1112.137250185443,
          1111.4557810022857,
          1112.7550351127777,
          1105.0216561840082,
          1109.644149593631,
          1099.7230977543566,
          1097.0700975367988,
          1098.9160617899508,
          1098.2302988012975,
          1085.3657411858724,
          1087.3682869038603,
          1082.3032387625797,
          1084.3061745939833,
          1091.1825280641779,
          1076.917660477789,
          1065.0980882447684,
          1065.3542927186859,
          1061.9579183954454,
          1061.0260643003842,
          1055.978719928161,
          1049.4482097790192,
          1050.4536025844932,
          1041.5853094510571,
          1038.3217454737787,
          1035.1507169372792,
          1040.1217515638316,
          1032.4920707168717,
          1021.8528591819198,
          1012.7016250796994,
          1014.0080523692656,
          1014.2975948892903,
          1011.9299099289011,
          1019.7612658145554,
          1008.0779785498938,
          1001.3345373385843,
          994.1018554002125,
          987.3111676663035,
          980.443110020624,
          1000.1788329343794,
          987.2779145823085,
          978.8410611975903,
          971.938995000282,
          972.3974401559288,
          968.507412534308,
          975.2902932634615,
          975.0878136426193,
          952.112636311441,
          969.9683020947236,
          957.7117602935601,
          944.8713555512888,
          948.7823466256407,
          940.6786830451557,
          944.1065290752049,
          931.4104733709011,
          922.6697941992932,
          914.0831026758439,
          912.8026519585688,
          912.0499587256946,
          909.497333007225,
          909.2467012376711,
          899.1171070785018,
          912.4479643165981,
          891.8296834832087,
          907.0121367190437,
          890.111922370233,
          890.0561659900703,
          882.7775990157417,
          890.6592490573147,
          875.0343885924693,
          878.8147121358305,
          875.5966815600444,
          862.988284811506,
          870.0373462702245,
          850.6343430051924,
          855.2704696762363,
          864.7071810780294,
          849.4837186915472,
          852.367497669233,
          846.9859571164324,
          855.2828914381494,
          838.8878878908404,
          839.5845010626261,
          841.3837395073872,
          846.8096169425271,
          831.6811742023401,
          825.4579644624853,
          827.1970732424812,
          821.9884163424513,
          808.7684685504578,
          807.021919392765,
          807.3701393153449,
          813.8104366518965,
          805.8432367514914,
          802.3203016074215,
          799.8053268331647,
          803.7891924521333,
          785.4978437590078,
          780.353452913121,
          792.4177080991633,
          813.9468001692095,
          800.3382279155296,
          789.7975930289847,
          789.8403781962007,
          767.8329280886163,
          775.2201442745278,
          762.2565047541577,
          760.6914677624715,
          763.0365770828372,
          775.5654016360506,
          775.1434214308765,
          782.2500443903172,
          827.1841444616354,
          793.3799909672375,
          759.2696859246149,
          765.8176284707426,
          764.2975043829778,
          759.3734730440748,
          756.2271294036369,
          737.2194828135183,
          736.4565215018031,
          728.3792233464234,
          725.3776750459398,
          719.4075789190567,
          724.2060189622903,
          725.4163306897739,
          727.1411403780689,
          721.2924294816914,
          732.8086315802924,
          715.7472114142278,
          716.8894533991503,
          726.1431024251922,
          760.1792365227717,
          773.884759321991,
          737.0054365838485,
          732.4947492998398,
          717.2285381200105,
          701.1899204166184,
          705.1546420228727,
          704.0330541414513,
          702.9418173040922,
          696.8275707146197,
          697.3913744338796,
          691.454108355063,
          695.9163916637359,
          703.4132956012782,
          741.7172837610209,
          709.1251497301187,
          690.0926522973402,
          683.2354485309265,
          689.5170502021075,
          688.463839270678
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1279.4218688805738,
          1272.4300734889227,
          1266.6112870514119,
          1258.319251097776,
          1257.221348381769,
          1250.9841788846313,
          1255.9365569474776,
          1244.1626018664151,
          1248.4659386622013,
          1239.4952348518257,
          1248.725321015304,
          1238.1980991642724,
          1231.510116072296,
          1229.533780456139,
          1227.2803074125538,
          1233.0450362348356,
          1236.4010495158507,
          1223.6072074992637,
          1222.086693787637,
          1220.1124423256326,
          1218.5865670420828,
          1218.4464938429762,
          1217.2312263594329,
          1213.6028907092034,
          1220.6182407511483,
          1214.0547337628616,
          1242.1274050079035,
          1215.2260975460024,
          1222.1612501830932,
          1208.8891529108494,
          1208.6677171436752,
          1206.036228667191,
          1208.6640957407228,
          1203.3640643162457,
          1208.0660823120581,
          1204.5231526284747,
          1202.6408768266242,
          1205.9032969362922,
          1200.7536068387183,
          1196.8280462552245,
          1201.8723085539982,
          1198.812912473799,
          1207.260932893296,
          1199.1298846972832,
          1198.540640088057,
          1194.7595670863386,
          1205.2827431644541,
          1189.7386631021907,
          1194.097056032207,
          1191.2242596375577,
          1186.1028430755903,
          1202.011470301299,
          1204.4265043703472,
          1188.5764830111023,
          1181.2815669862737,
          1185.4536637542767,
          1179.5668947270717,
          1183.6877905748304,
          1181.0439869980307,
          1180.4160484496017,
          1181.891358988448,
          1177.2874001701298,
          1182.3153893971983,
          1193.852270223871,
          1183.0534902220381,
          1186.0598821584558,
          1172.1017869115951,
          1173.8346879131836,
          1191.0553685431732,
          1170.2612723990578,
          1186.758077446674,
          1188.1704233986882,
          1171.4327813349867,
          1196.858715154519,
          1175.2892698261574,
          1163.3846939298035,
          1192.7050030274024,
          1167.8774009446045,
          1172.0596558375996,
          1173.3214451078663,
          1178.4457385344474,
          1174.035071508187,
          1170.1269502121531,
          1201.0607644817933,
          1173.110927743187,
          1174.6472575277753,
          1200.1404393610887,
          1190.9283896431502,
          1169.6951907478976,
          1171.2743073152305,
          1188.924379705594,
          1180.7162319051017,
          1173.3912271270538,
          1169.885935292111,
          1158.148884609751,
          1164.9451833066755,
          1186.7737795024877,
          1172.3862808928213,
          1170.1474291654395,
          1181.1836042019797,
          1172.208147310938,
          1172.1552762524361,
          1183.3680378065,
          1168.1825646828238,
          1178.9254588786314,
          1176.4514311428463,
          1183.5731461125097,
          1174.1514791729983,
          1178.979865080851,
          1165.242887611305,
          1170.5057077600982,
          1185.3070382408325,
          1167.2157307730567,
          1174.3231159315,
          1176.0756109684007,
          1182.9685783623358,
          1180.5704249505745,
          1205.061168706606,
          1174.1753906103133,
          1194.9679691367478,
          1193.632227042263,
          1218.7171219878717,
          1181.710772716093,
          1180.7449890956723,
          1178.0169044496158,
          1175.6190299368202,
          1178.7539541773456,
          1179.8892928988416,
          1179.18666014548,
          1184.7008075733233,
          1192.4861815280276,
          1191.640926638753,
          1178.782695775618,
          1178.5600226115243,
          1181.5303354150478,
          1191.8789400047544,
          1180.49381901244,
          1195.4573168313789,
          1200.4484585459304,
          1185.5327066681966,
          1180.2981552084248,
          1182.2539445086522,
          1179.8586272306043,
          1180.2237909139935,
          1178.373370543687,
          1190.8613079553716,
          1222.9256799367809,
          1185.2535921765732,
          1181.3856817765252,
          1258.2621122780558,
          1198.0467242662955,
          1181.4903510112622,
          1205.612928844488,
          1186.9915321339388,
          1181.5929135723584,
          1184.1196347564214,
          1181.3890842019327,
          1182.6573815063696,
          1181.4695112321333,
          1183.9657795326443,
          1179.7624877170879,
          1189.6515601942003,
          1185.8147040949045,
          1188.2294257794877,
          1198.5365184342572,
          1195.751250516969,
          1196.9849275352244,
          1184.3691651271822,
          1186.299405309273,
          1210.9987032201693,
          1275.4871072013798,
          1243.332340672663,
          1193.294019039346,
          1194.4968430859687,
          1182.2174895695605,
          1187.5056742745983,
          1192.3180174459453,
          1207.4441483403725,
          1196.4640715763328,
          1198.5180185768154,
          1196.9992039850486,
          1194.895110661744,
          1198.3758090370902,
          1252.9899420867303,
          1224.1826681256032,
          1210.08974052752,
          1194.1336926448218,
          1195.4846828805676,
          1197.100325812934,
          1190.6139926210492
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"0f86a097-a2f6-4d9b-afc2-02b8d422e037\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"0f86a097-a2f6-4d9b-afc2-02b8d422e037\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '0f86a097-a2f6-4d9b-afc2-02b8d422e037',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1266.806673498557, 1256.6963538794616, 1246.2823334321579, 1237.2830432611117, 1230.6446730982977, 1223.136516123562, 1216.704225737895, 1211.314654111432, 1206.3111273882407, 1202.4707678440507, 1196.9696495066862, 1197.179255468515, 1193.134318025695, 1185.8136979143821, 1180.6904070945786, 1177.5096632837174, 1178.107604307332, 1170.7666139482185, 1166.231498747329, 1162.8832743208513, 1160.525480996685, 1155.9452931870912, 1156.194921982702, 1152.5996979842905, 1148.6161124147393, 1153.4611763534408, 1145.6265372488, 1156.7491146659047, 1143.467257153756, 1139.4549846936018, 1133.9171238775505, 1132.8795723617734, 1130.0575849993186, 1130.978327244586, 1129.3228071207795, 1124.1212570652447, 1121.1196537755977, 1116.377574295708, 1115.1090446733774, 1112.137250185443, 1111.4557810022857, 1112.7550351127777, 1105.0216561840082, 1109.644149593631, 1099.7230977543566, 1097.0700975367988, 1098.9160617899508, 1098.2302988012975, 1085.3657411858724, 1087.3682869038603, 1082.3032387625797, 1084.3061745939833, 1091.1825280641779, 1076.917660477789, 1065.0980882447684, 1065.3542927186859, 1061.9579183954454, 1061.0260643003842, 1055.978719928161, 1049.4482097790192, 1050.4536025844932, 1041.5853094510571, 1038.3217454737787, 1035.1507169372792, 1040.1217515638316, 1032.4920707168717, 1021.8528591819198, 1012.7016250796994, 1014.0080523692656, 1014.2975948892903, 1011.9299099289011, 1019.7612658145554, 1008.0779785498938, 1001.3345373385843, 994.1018554002125, 987.3111676663035, 980.443110020624, 1000.1788329343794, 987.2779145823085, 978.8410611975903, 971.938995000282, 972.3974401559288, 968.507412534308, 975.2902932634615, 975.0878136426193, 952.112636311441, 969.9683020947236, 957.7117602935601, 944.8713555512888, 948.7823466256407, 940.6786830451557, 944.1065290752049, 931.4104733709011, 922.6697941992932, 914.0831026758439, 912.8026519585688, 912.0499587256946, 909.497333007225, 909.2467012376711, 899.1171070785018, 912.4479643165981, 891.8296834832087, 907.0121367190437, 890.111922370233, 890.0561659900703, 882.7775990157417, 890.6592490573147, 875.0343885924693, 878.8147121358305, 875.5966815600444, 862.988284811506, 870.0373462702245, 850.6343430051924, 855.2704696762363, 864.7071810780294, 849.4837186915472, 852.367497669233, 846.9859571164324, 855.2828914381494, 838.8878878908404, 839.5845010626261, 841.3837395073872, 846.8096169425271, 831.6811742023401, 825.4579644624853, 827.1970732424812, 821.9884163424513, 808.7684685504578, 807.021919392765, 807.3701393153449, 813.8104366518965, 805.8432367514914, 802.3203016074215, 799.8053268331647, 803.7891924521333, 785.4978437590078, 780.353452913121, 792.4177080991633, 813.9468001692095, 800.3382279155296, 789.7975930289847, 789.8403781962007, 767.8329280886163, 775.2201442745278, 762.2565047541577, 760.6914677624715, 763.0365770828372, 775.5654016360506, 775.1434214308765, 782.2500443903172, 827.1841444616354, 793.3799909672375, 759.2696859246149, 765.8176284707426, 764.2975043829778, 759.3734730440748, 756.2271294036369, 737.2194828135183, 736.4565215018031, 728.3792233464234, 725.3776750459398, 719.4075789190567, 724.2060189622903, 725.4163306897739, 727.1411403780689, 721.2924294816914, 732.8086315802924, 715.7472114142278, 716.8894533991503, 726.1431024251922, 760.1792365227717, 773.884759321991, 737.0054365838485, 732.4947492998398, 717.2285381200105, 701.1899204166184, 705.1546420228727, 704.0330541414513, 702.9418173040922, 696.8275707146197, 697.3913744338796, 691.454108355063, 695.9163916637359, 703.4132956012782, 741.7172837610209, 709.1251497301187, 690.0926522973402, 683.2354485309265, 689.5170502021075, 688.463839270678]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1279.4218688805738, 1272.4300734889227, 1266.6112870514119, 1258.319251097776, 1257.221348381769, 1250.9841788846313, 1255.9365569474776, 1244.1626018664151, 1248.4659386622013, 1239.4952348518257, 1248.725321015304, 1238.1980991642724, 1231.510116072296, 1229.533780456139, 1227.2803074125538, 1233.0450362348356, 1236.4010495158507, 1223.6072074992637, 1222.086693787637, 1220.1124423256326, 1218.5865670420828, 1218.4464938429762, 1217.2312263594329, 1213.6028907092034, 1220.6182407511483, 1214.0547337628616, 1242.1274050079035, 1215.2260975460024, 1222.1612501830932, 1208.8891529108494, 1208.6677171436752, 1206.036228667191, 1208.6640957407228, 1203.3640643162457, 1208.0660823120581, 1204.5231526284747, 1202.6408768266242, 1205.9032969362922, 1200.7536068387183, 1196.8280462552245, 1201.8723085539982, 1198.812912473799, 1207.260932893296, 1199.1298846972832, 1198.540640088057, 1194.7595670863386, 1205.2827431644541, 1189.7386631021907, 1194.097056032207, 1191.2242596375577, 1186.1028430755903, 1202.011470301299, 1204.4265043703472, 1188.5764830111023, 1181.2815669862737, 1185.4536637542767, 1179.5668947270717, 1183.6877905748304, 1181.0439869980307, 1180.4160484496017, 1181.891358988448, 1177.2874001701298, 1182.3153893971983, 1193.852270223871, 1183.0534902220381, 1186.0598821584558, 1172.1017869115951, 1173.8346879131836, 1191.0553685431732, 1170.2612723990578, 1186.758077446674, 1188.1704233986882, 1171.4327813349867, 1196.858715154519, 1175.2892698261574, 1163.3846939298035, 1192.7050030274024, 1167.8774009446045, 1172.0596558375996, 1173.3214451078663, 1178.4457385344474, 1174.035071508187, 1170.1269502121531, 1201.0607644817933, 1173.110927743187, 1174.6472575277753, 1200.1404393610887, 1190.9283896431502, 1169.6951907478976, 1171.2743073152305, 1188.924379705594, 1180.7162319051017, 1173.3912271270538, 1169.885935292111, 1158.148884609751, 1164.9451833066755, 1186.7737795024877, 1172.3862808928213, 1170.1474291654395, 1181.1836042019797, 1172.208147310938, 1172.1552762524361, 1183.3680378065, 1168.1825646828238, 1178.9254588786314, 1176.4514311428463, 1183.5731461125097, 1174.1514791729983, 1178.979865080851, 1165.242887611305, 1170.5057077600982, 1185.3070382408325, 1167.2157307730567, 1174.3231159315, 1176.0756109684007, 1182.9685783623358, 1180.5704249505745, 1205.061168706606, 1174.1753906103133, 1194.9679691367478, 1193.632227042263, 1218.7171219878717, 1181.710772716093, 1180.7449890956723, 1178.0169044496158, 1175.6190299368202, 1178.7539541773456, 1179.8892928988416, 1179.18666014548, 1184.7008075733233, 1192.4861815280276, 1191.640926638753, 1178.782695775618, 1178.5600226115243, 1181.5303354150478, 1191.8789400047544, 1180.49381901244, 1195.4573168313789, 1200.4484585459304, 1185.5327066681966, 1180.2981552084248, 1182.2539445086522, 1179.8586272306043, 1180.2237909139935, 1178.373370543687, 1190.8613079553716, 1222.9256799367809, 1185.2535921765732, 1181.3856817765252, 1258.2621122780558, 1198.0467242662955, 1181.4903510112622, 1205.612928844488, 1186.9915321339388, 1181.5929135723584, 1184.1196347564214, 1181.3890842019327, 1182.6573815063696, 1181.4695112321333, 1183.9657795326443, 1179.7624877170879, 1189.6515601942003, 1185.8147040949045, 1188.2294257794877, 1198.5365184342572, 1195.751250516969, 1196.9849275352244, 1184.3691651271822, 1186.299405309273, 1210.9987032201693, 1275.4871072013798, 1243.332340672663, 1193.294019039346, 1194.4968430859687, 1182.2174895695605, 1187.5056742745983, 1192.3180174459453, 1207.4441483403725, 1196.4640715763328, 1198.5180185768154, 1196.9992039850486, 1194.895110661744, 1198.3758090370902, 1252.9899420867303, 1224.1826681256032, 1210.08974052752, 1194.1336926448218, 1195.4846828805676, 1197.100325812934, 1190.6139926210492]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0f86a097-a2f6-4d9b-afc2-02b8d422e037');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.46% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1191   : Mean absolute error \n",
      "\n",
      "10.21% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss=[history.history[\"val_loss\"][-1]]\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Dropout\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Dropout\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 1s 46us/sample - loss: 11104.9790 - val_loss: 10453.7489\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 6566.8866 - val_loss: 2844.8541\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 2426.2825 - val_loss: 1743.4334\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1758.0013 - val_loss: 1499.9645\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1648.7797 - val_loss: 1414.9995\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1603.1394 - val_loss: 1372.3611\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1572.5695 - val_loss: 1345.1777\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1534.9278 - val_loss: 1323.9774\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1533.2602 - val_loss: 1315.9346\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1520.0386 - val_loss: 1303.2823\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1498.2691 - val_loss: 1295.0026\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1486.9505 - val_loss: 1276.5579\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1484.9824 - val_loss: 1285.6039\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1479.5547 - val_loss: 1271.8723\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1481.0780 - val_loss: 1266.4461\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1463.6835 - val_loss: 1276.5045\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1468.3619 - val_loss: 1259.7092\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1455.7245 - val_loss: 1260.6952\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1455.0857 - val_loss: 1253.8644\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1446.3882 - val_loss: 1249.1545\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1449.4839 - val_loss: 1249.3484\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1439.8112 - val_loss: 1253.2121\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1431.1293 - val_loss: 1244.6248\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1433.0326 - val_loss: 1242.1062\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1424.8367 - val_loss: 1249.6552\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1429.1097 - val_loss: 1255.3233\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1418.1374 - val_loss: 1239.7926\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1430.7523 - val_loss: 1236.2560\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1421.9824 - val_loss: 1235.4790\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1416.3785 - val_loss: 1232.1296\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1416.4073 - val_loss: 1234.0316\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1411.0418 - val_loss: 1228.1314\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1423.0476 - val_loss: 1235.6925\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1417.6300 - val_loss: 1234.6620\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1398.5295 - val_loss: 1221.2850\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1407.5659 - val_loss: 1226.7668\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1409.1974 - val_loss: 1226.2345\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1396.6081 - val_loss: 1235.1055\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1410.6686 - val_loss: 1230.5604\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1397.8216 - val_loss: 1227.3995\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1399.0705 - val_loss: 1218.0990\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1384.3026 - val_loss: 1236.1609\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1391.5235 - val_loss: 1241.2732\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1396.8885 - val_loss: 1220.1060\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1395.0467 - val_loss: 1218.7439\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1389.7114 - val_loss: 1234.7977\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1394.6826 - val_loss: 1213.1495\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1379.9505 - val_loss: 1230.1325\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1386.5194 - val_loss: 1226.4799\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1390.4921 - val_loss: 1226.0493\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1381.2682 - val_loss: 1213.0930\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1375.7231 - val_loss: 1214.7484\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1384.3462 - val_loss: 1227.4507\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1379.8381 - val_loss: 1208.9488\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1375.0296 - val_loss: 1220.5301\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1369.7695 - val_loss: 1212.3966\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1378.5514 - val_loss: 1204.9719\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1363.3762 - val_loss: 1209.4419\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1371.3662 - val_loss: 1203.6224\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1378.9811 - val_loss: 1210.3077\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1365.0974 - val_loss: 1205.0243\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1370.7701 - val_loss: 1210.4479\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1373.9342 - val_loss: 1216.3004\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1372.0939 - val_loss: 1213.1279\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1363.3639 - val_loss: 1215.5323\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1362.8074 - val_loss: 1210.2485\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1371.0554 - val_loss: 1204.3341\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1362.1915 - val_loss: 1205.0737\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1373.7385 - val_loss: 1206.5199\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1357.9881 - val_loss: 1202.9004\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1357.9216 - val_loss: 1206.0526\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1344.8349 - val_loss: 1201.0028\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1356.0631 - val_loss: 1210.5179\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1358.5342 - val_loss: 1191.1674\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1356.5611 - val_loss: 1194.8789\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1369.2569 - val_loss: 1205.6773\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1351.4459 - val_loss: 1221.7578\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1357.5692 - val_loss: 1198.2149\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1346.9332 - val_loss: 1200.3566\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1359.4658 - val_loss: 1203.2201\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1354.7571 - val_loss: 1191.7564\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1347.4137 - val_loss: 1194.1099\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1353.0742 - val_loss: 1191.9421\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1336.1823 - val_loss: 1192.9367\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1358.6952 - val_loss: 1223.3159\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1355.6675 - val_loss: 1194.1964\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1354.1660 - val_loss: 1193.7986\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1339.1187 - val_loss: 1194.5937\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1337.8079 - val_loss: 1183.4346\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1332.8833 - val_loss: 1185.5196\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1343.0567 - val_loss: 1187.1379\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1349.4933 - val_loss: 1192.8638\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1345.4300 - val_loss: 1192.4829\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1346.2609 - val_loss: 1185.7354\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1345.8784 - val_loss: 1187.5511\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1335.6502 - val_loss: 1183.3535\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1338.8773 - val_loss: 1235.5991\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1333.6505 - val_loss: 1184.2226\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1336.0900 - val_loss: 1190.2915\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1341.6809 - val_loss: 1192.1814\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1322.7243 - val_loss: 1182.3013\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1328.9614 - val_loss: 1187.9929\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1332.5932 - val_loss: 1180.8304\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1334.0951 - val_loss: 1174.6040\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1332.4966 - val_loss: 1195.2677\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1340.9671 - val_loss: 1182.8520\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1333.8999 - val_loss: 1188.5450\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1323.8695 - val_loss: 1173.7903\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1324.1858 - val_loss: 1188.0595\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1318.5289 - val_loss: 1172.1680\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1336.3477 - val_loss: 1184.1048\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1328.3952 - val_loss: 1181.3624\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1325.1412 - val_loss: 1180.1289\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1320.9592 - val_loss: 1177.6077\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1318.1484 - val_loss: 1176.2340\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1318.5477 - val_loss: 1175.0114\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1323.7549 - val_loss: 1215.8718\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1324.0972 - val_loss: 1189.8609\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1319.1356 - val_loss: 1178.0120\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1316.6289 - val_loss: 1177.6650\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1317.9182 - val_loss: 1188.2310\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1327.9278 - val_loss: 1202.3930\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1328.5366 - val_loss: 1172.2888\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1324.3886 - val_loss: 1168.3066\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1315.5559 - val_loss: 1185.8326\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1310.0864 - val_loss: 1177.3065\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1310.1863 - val_loss: 1167.4447\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1303.3894 - val_loss: 1174.9422\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1315.0777 - val_loss: 1158.6044\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1322.0044 - val_loss: 1174.7006\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1300.0727 - val_loss: 1187.8838\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1314.1777 - val_loss: 1170.1615\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1315.2307 - val_loss: 1161.0333\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1303.3316 - val_loss: 1158.6266\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1315.6300 - val_loss: 1175.1507\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1313.3998 - val_loss: 1173.5085\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1311.4862 - val_loss: 1175.2420\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1306.2935 - val_loss: 1181.5403\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1316.9318 - val_loss: 1170.6863\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1305.4697 - val_loss: 1170.7188\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1301.9149 - val_loss: 1164.9126\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1303.9845 - val_loss: 1196.2192\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1324.2511 - val_loss: 1164.2917\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1296.6610 - val_loss: 1158.4953\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1301.7553 - val_loss: 1174.9954\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1317.4221 - val_loss: 1191.8962\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1298.6759 - val_loss: 1164.5919\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1306.2641 - val_loss: 1184.8389\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1295.3704 - val_loss: 1168.0473\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1305.6053 - val_loss: 1166.7980\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1291.1350 - val_loss: 1157.0641\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1294.0071 - val_loss: 1162.0362\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1285.7233 - val_loss: 1149.8968\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1299.0759 - val_loss: 1163.2808\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1294.9863 - val_loss: 1153.5720\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1301.7018 - val_loss: 1176.7258\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1292.4254 - val_loss: 1193.6376\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1299.4175 - val_loss: 1181.4504\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1302.9836 - val_loss: 1157.0768\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1296.1790 - val_loss: 1158.9824\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1301.9092 - val_loss: 1164.1733\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1298.4372 - val_loss: 1149.8123\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1283.8941 - val_loss: 1157.9814\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1289.2757 - val_loss: 1160.4414\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1286.1074 - val_loss: 1160.6215\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1283.5961 - val_loss: 1158.2213\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1288.4565 - val_loss: 1170.6296\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1288.6247 - val_loss: 1151.9440\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1281.3546 - val_loss: 1155.5403\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1273.6500 - val_loss: 1166.4112\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1283.4619 - val_loss: 1167.3589\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1316.0265 - val_loss: 1185.7758\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1274.53 - 0s 9us/sample - loss: 1275.8741 - val_loss: 1152.0363\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1286.1234 - val_loss: 1149.2832\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1280.5084 - val_loss: 1156.9669\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1289.7659 - val_loss: 1166.8618\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1284.8172 - val_loss: 1172.7220\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1278.2859 - val_loss: 1158.0088\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1286.2579 - val_loss: 1167.8999\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1287.8056 - val_loss: 1170.4314\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1285.6579 - val_loss: 1161.8145\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1278.5046 - val_loss: 1153.1661\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1272.8081 - val_loss: 1154.2239\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1270.2810 - val_loss: 1157.2879\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1286.3445 - val_loss: 1159.5525\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1296.11 - 0s 9us/sample - loss: 1286.4361 - val_loss: 1178.9769\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1258.1694 - val_loss: 1161.9494\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1279.0675 - val_loss: 1152.3942\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1276.6826 - val_loss: 1151.8233\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1265.2350 - val_loss: 1182.0305\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1277.3773 - val_loss: 1146.0519\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1274.2709 - val_loss: 1175.3354\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1273.8111 - val_loss: 1152.8986\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1266.6822 - val_loss: 1151.1407\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1278.1330 - val_loss: 1153.2450\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1264.6318 - val_loss: 1157.3770\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1268.7676 - val_loss: 1153.4707\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1274.0863 - val_loss: 1197.2682\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1270.0642 - val_loss: 1169.4254\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1262.8991 - val_loss: 1156.7920\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1498.269117283061,
          1486.9505386857393,
          1484.982372160782,
          1479.5547435294657,
          1481.078042657002,
          1463.683472634318,
          1468.3619313290728,
          1455.7244777094272,
          1455.085722928442,
          1446.388227653618,
          1449.4838808685786,
          1439.8111894636993,
          1431.1292839165033,
          1433.0326321335863,
          1424.8366510250871,
          1429.1096907379108,
          1418.137379011602,
          1430.7522584109308,
          1421.9823572049047,
          1416.3784669731144,
          1416.407295736493,
          1411.0418256411983,
          1423.0476449951514,
          1417.6299580080083,
          1398.529450717373,
          1407.565891214619,
          1409.1974040219413,
          1396.6081457296784,
          1410.6685719540728,
          1397.821622669135,
          1399.0704955197969,
          1384.3025948520842,
          1391.5234974214206,
          1396.8885071645261,
          1395.0467339594666,
          1389.7114406881722,
          1394.6826315559126,
          1379.95053878365,
          1386.5194104280504,
          1390.4921133080775,
          1381.2681758215892,
          1375.7231168959215,
          1384.3461912104283,
          1379.8381436755094,
          1375.0296247048968,
          1369.7695051078115,
          1378.5514421725002,
          1363.3761962009428,
          1371.3662254527787,
          1378.981055945654,
          1365.097442755461,
          1370.7701251770227,
          1373.9342345763046,
          1372.0939112835952,
          1363.3639311330553,
          1362.8073531465013,
          1371.055416862162,
          1362.1915452344062,
          1373.7385236282303,
          1357.9881441993323,
          1357.9215723087057,
          1344.8349085062569,
          1356.063101441678,
          1358.5342056828222,
          1356.5610629874,
          1369.2568508933775,
          1351.445854387423,
          1357.569214332264,
          1346.9331642397185,
          1359.4657576250986,
          1354.757124235904,
          1347.4136723498675,
          1353.0742046018868,
          1336.1822555294154,
          1358.6952099873733,
          1355.6675339525343,
          1354.1659887974358,
          1339.1186628446853,
          1337.8078957682553,
          1332.8833237168592,
          1343.056717656336,
          1349.493330829101,
          1345.430035621914,
          1346.2608734369517,
          1345.8783727094744,
          1335.6501626151824,
          1338.877271065904,
          1333.6504752444246,
          1336.089992452055,
          1341.6808734447845,
          1322.7243015189101,
          1328.9613933972469,
          1332.593174406778,
          1334.0950746599362,
          1332.4966465789569,
          1340.9671436016847,
          1333.8999118166216,
          1323.869516064032,
          1324.1857760352698,
          1318.528889541519,
          1336.3477257911586,
          1328.395165932401,
          1325.1412427193516,
          1320.9591568693838,
          1318.148393440132,
          1318.5476581396788,
          1323.7548864841556,
          1324.0972346460171,
          1319.1355995803933,
          1316.6289397599776,
          1317.9181574486242,
          1327.927805629598,
          1328.5366303708,
          1324.3886272031891,
          1315.5559229631808,
          1310.0863745983697,
          1310.1863110934132,
          1303.3894037583655,
          1315.0777173483086,
          1322.0043970769314,
          1300.072746611894,
          1314.1776536230527,
          1315.2307367074316,
          1303.3315598338884,
          1315.6299687537205,
          1313.3997731504255,
          1311.4862093346999,
          1306.2934630282875,
          1316.9318297152674,
          1305.4697438192818,
          1301.9149372352492,
          1303.984508379012,
          1324.2511059271685,
          1296.6610187708745,
          1301.755322309685,
          1317.4221434469855,
          1298.6758822939644,
          1306.2640514018087,
          1295.3703514146546,
          1305.6052831110887,
          1291.1349503582362,
          1294.0070989993906,
          1285.7232556845063,
          1299.075872488196,
          1294.9862922896405,
          1301.7017966439305,
          1292.42536388066,
          1299.417452711033,
          1302.9836199117706,
          1296.1790281195954,
          1301.9092473928308,
          1298.437219754761,
          1283.894134270588,
          1289.2757203886354,
          1286.1073714754064,
          1283.5961197600088,
          1288.456514794912,
          1288.6247212234232,
          1281.354553675494,
          1273.650037778889,
          1283.4619443414206,
          1316.0264892578125,
          1275.8741212504074,
          1286.1234373433426,
          1280.5084372199751,
          1289.7658981956604,
          1284.817155488058,
          1278.285881377519,
          1286.257917460397,
          1287.8056354235857,
          1285.6579070182083,
          1278.5046285870608,
          1272.8081035594892,
          1270.2810107059604,
          1286.3445244745715,
          1286.4360794119398,
          1258.1693650316995,
          1279.0674503709645,
          1276.6825557729394,
          1265.2350288484436,
          1277.3773209025105,
          1274.270869592973,
          1273.8111234228525,
          1266.6821899291674,
          1278.1329603453353,
          1264.6318473196327,
          1268.7676470297383,
          1274.08634079466,
          1270.0641871013072,
          1262.8991302190382
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1295.002641389096,
          1276.557855037355,
          1285.6038930712039,
          1271.8722836846505,
          1266.4461046474742,
          1276.5045163567854,
          1259.7091924599663,
          1260.6952047736224,
          1253.864407518141,
          1249.154511936112,
          1249.348434078017,
          1253.2121224656764,
          1244.6248414579077,
          1242.1061740995337,
          1249.6552457042608,
          1255.3232882300622,
          1239.7925713789446,
          1236.2559501872838,
          1235.4790128895675,
          1232.1295803170274,
          1234.0316228688732,
          1228.1313842504183,
          1235.692455633671,
          1234.6620389771217,
          1221.2849861299535,
          1226.7667507785868,
          1226.2344813311486,
          1235.105508599703,
          1230.560386570513,
          1227.3994953284791,
          1218.0989883511584,
          1236.1608602532601,
          1241.273150278615,
          1220.10600490964,
          1218.743851837042,
          1234.7976588250544,
          1213.1495320939912,
          1230.132471843786,
          1226.4799057138403,
          1226.0493359149805,
          1213.0929927577326,
          1214.7483625149998,
          1227.4506736802796,
          1208.9487735250323,
          1220.5301489928502,
          1212.3965836565696,
          1204.9719212090106,
          1209.4418573251392,
          1203.6223664437694,
          1210.307653879579,
          1205.0242559854842,
          1210.4479460562306,
          1216.3004483532184,
          1213.1279315233278,
          1215.5322509422938,
          1210.248467646741,
          1204.3341011581283,
          1205.073732035323,
          1206.5198617724443,
          1202.9004171833094,
          1206.0526287292273,
          1201.0027724916815,
          1210.5179162844881,
          1191.1673503942282,
          1194.8789144989864,
          1205.6773352170721,
          1221.7578406248824,
          1198.2149132911586,
          1200.3565990324844,
          1203.2200687363315,
          1191.7564215789178,
          1194.1099187966838,
          1191.942140336359,
          1192.9366616141422,
          1223.3159034534713,
          1194.1964453281657,
          1193.798599380851,
          1194.5937204064553,
          1183.4345651966598,
          1185.519602578031,
          1187.1378519453312,
          1192.8637883790825,
          1192.4828604665097,
          1185.7353937865403,
          1187.551115199323,
          1183.353536333138,
          1235.599092503791,
          1184.2225625738247,
          1190.2914663084957,
          1192.181381161332,
          1182.3012917080503,
          1187.992946431229,
          1180.8303851733256,
          1174.6040498998568,
          1195.2676649474179,
          1182.8519850586133,
          1188.5450236532954,
          1173.7903070933646,
          1188.0595399356687,
          1172.167961553555,
          1184.1048167129068,
          1181.362359444125,
          1180.128943945665,
          1177.607657081263,
          1176.2339740796774,
          1175.0114005643188,
          1215.8717833848475,
          1189.860943971905,
          1178.0119794865086,
          1177.6650458673018,
          1188.2309774211778,
          1202.393044033628,
          1172.2887717048702,
          1168.3065638873964,
          1185.8325615349909,
          1177.3065073683765,
          1167.4447001412657,
          1174.9421697830376,
          1158.6043946193697,
          1174.7005508511584,
          1187.8837980213398,
          1170.1614994150807,
          1161.033287059049,
          1158.6266441429357,
          1175.150697232727,
          1173.508492735409,
          1175.2420275382008,
          1181.5403123560711,
          1170.686251539158,
          1170.7187903147794,
          1164.912587375614,
          1196.219204648884,
          1164.2917382557932,
          1158.4953161422825,
          1174.9954055838148,
          1191.896155149874,
          1164.591865143318,
          1184.838945809887,
          1168.047287400366,
          1166.7980442754679,
          1157.0641405936685,
          1162.0361726132476,
          1149.8967739707223,
          1163.280810502815,
          1153.5719656996864,
          1176.725817080464,
          1193.6375678326149,
          1181.4503724821254,
          1157.076779950136,
          1158.9824246654582,
          1164.1733459142208,
          1149.8123330375774,
          1157.9814458020542,
          1160.44139398667,
          1160.6215000309398,
          1158.2212658429494,
          1170.6295875869057,
          1151.9440449465485,
          1155.540300019308,
          1166.4111528352623,
          1167.3588831694829,
          1185.7758334511527,
          1152.036326578009,
          1149.283243390824,
          1156.9668806866603,
          1166.861808450996,
          1172.7219777769858,
          1158.0088187784334,
          1167.8999332835685,
          1170.431418098188,
          1161.814497111435,
          1153.1661038916025,
          1154.2238818976214,
          1157.2878669599552,
          1159.552540438147,
          1178.9769136326715,
          1161.9493883804923,
          1152.394196395002,
          1151.8232758688214,
          1182.0305455561413,
          1146.0519103330769,
          1175.335386506871,
          1152.898641301368,
          1151.140651093233,
          1153.2449841756536,
          1157.3769540551527,
          1153.4707082408402,
          1197.268163411393,
          1169.425375727869,
          1156.7920102030905
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492,
          1190.6139926210492
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"cd34bcac-1510-446e-9c6b-7bfeb20faf52\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"cd34bcac-1510-446e-9c6b-7bfeb20faf52\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'cd34bcac-1510-446e-9c6b-7bfeb20faf52',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1498.269117283061, 1486.9505386857393, 1484.982372160782, 1479.5547435294657, 1481.078042657002, 1463.683472634318, 1468.3619313290728, 1455.7244777094272, 1455.085722928442, 1446.388227653618, 1449.4838808685786, 1439.8111894636993, 1431.1292839165033, 1433.0326321335863, 1424.8366510250871, 1429.1096907379108, 1418.137379011602, 1430.7522584109308, 1421.9823572049047, 1416.3784669731144, 1416.407295736493, 1411.0418256411983, 1423.0476449951514, 1417.6299580080083, 1398.529450717373, 1407.565891214619, 1409.1974040219413, 1396.6081457296784, 1410.6685719540728, 1397.821622669135, 1399.0704955197969, 1384.3025948520842, 1391.5234974214206, 1396.8885071645261, 1395.0467339594666, 1389.7114406881722, 1394.6826315559126, 1379.95053878365, 1386.5194104280504, 1390.4921133080775, 1381.2681758215892, 1375.7231168959215, 1384.3461912104283, 1379.8381436755094, 1375.0296247048968, 1369.7695051078115, 1378.5514421725002, 1363.3761962009428, 1371.3662254527787, 1378.981055945654, 1365.097442755461, 1370.7701251770227, 1373.9342345763046, 1372.0939112835952, 1363.3639311330553, 1362.8073531465013, 1371.055416862162, 1362.1915452344062, 1373.7385236282303, 1357.9881441993323, 1357.9215723087057, 1344.8349085062569, 1356.063101441678, 1358.5342056828222, 1356.5610629874, 1369.2568508933775, 1351.445854387423, 1357.569214332264, 1346.9331642397185, 1359.4657576250986, 1354.757124235904, 1347.4136723498675, 1353.0742046018868, 1336.1822555294154, 1358.6952099873733, 1355.6675339525343, 1354.1659887974358, 1339.1186628446853, 1337.8078957682553, 1332.8833237168592, 1343.056717656336, 1349.493330829101, 1345.430035621914, 1346.2608734369517, 1345.8783727094744, 1335.6501626151824, 1338.877271065904, 1333.6504752444246, 1336.089992452055, 1341.6808734447845, 1322.7243015189101, 1328.9613933972469, 1332.593174406778, 1334.0950746599362, 1332.4966465789569, 1340.9671436016847, 1333.8999118166216, 1323.869516064032, 1324.1857760352698, 1318.528889541519, 1336.3477257911586, 1328.395165932401, 1325.1412427193516, 1320.9591568693838, 1318.148393440132, 1318.5476581396788, 1323.7548864841556, 1324.0972346460171, 1319.1355995803933, 1316.6289397599776, 1317.9181574486242, 1327.927805629598, 1328.5366303708, 1324.3886272031891, 1315.5559229631808, 1310.0863745983697, 1310.1863110934132, 1303.3894037583655, 1315.0777173483086, 1322.0043970769314, 1300.072746611894, 1314.1776536230527, 1315.2307367074316, 1303.3315598338884, 1315.6299687537205, 1313.3997731504255, 1311.4862093346999, 1306.2934630282875, 1316.9318297152674, 1305.4697438192818, 1301.9149372352492, 1303.984508379012, 1324.2511059271685, 1296.6610187708745, 1301.755322309685, 1317.4221434469855, 1298.6758822939644, 1306.2640514018087, 1295.3703514146546, 1305.6052831110887, 1291.1349503582362, 1294.0070989993906, 1285.7232556845063, 1299.075872488196, 1294.9862922896405, 1301.7017966439305, 1292.42536388066, 1299.417452711033, 1302.9836199117706, 1296.1790281195954, 1301.9092473928308, 1298.437219754761, 1283.894134270588, 1289.2757203886354, 1286.1073714754064, 1283.5961197600088, 1288.456514794912, 1288.6247212234232, 1281.354553675494, 1273.650037778889, 1283.4619443414206, 1316.0264892578125, 1275.8741212504074, 1286.1234373433426, 1280.5084372199751, 1289.7658981956604, 1284.817155488058, 1278.285881377519, 1286.257917460397, 1287.8056354235857, 1285.6579070182083, 1278.5046285870608, 1272.8081035594892, 1270.2810107059604, 1286.3445244745715, 1286.4360794119398, 1258.1693650316995, 1279.0674503709645, 1276.6825557729394, 1265.2350288484436, 1277.3773209025105, 1274.270869592973, 1273.8111234228525, 1266.6821899291674, 1278.1329603453353, 1264.6318473196327, 1268.7676470297383, 1274.08634079466, 1270.0641871013072, 1262.8991302190382]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1295.002641389096, 1276.557855037355, 1285.6038930712039, 1271.8722836846505, 1266.4461046474742, 1276.5045163567854, 1259.7091924599663, 1260.6952047736224, 1253.864407518141, 1249.154511936112, 1249.348434078017, 1253.2121224656764, 1244.6248414579077, 1242.1061740995337, 1249.6552457042608, 1255.3232882300622, 1239.7925713789446, 1236.2559501872838, 1235.4790128895675, 1232.1295803170274, 1234.0316228688732, 1228.1313842504183, 1235.692455633671, 1234.6620389771217, 1221.2849861299535, 1226.7667507785868, 1226.2344813311486, 1235.105508599703, 1230.560386570513, 1227.3994953284791, 1218.0989883511584, 1236.1608602532601, 1241.273150278615, 1220.10600490964, 1218.743851837042, 1234.7976588250544, 1213.1495320939912, 1230.132471843786, 1226.4799057138403, 1226.0493359149805, 1213.0929927577326, 1214.7483625149998, 1227.4506736802796, 1208.9487735250323, 1220.5301489928502, 1212.3965836565696, 1204.9719212090106, 1209.4418573251392, 1203.6223664437694, 1210.307653879579, 1205.0242559854842, 1210.4479460562306, 1216.3004483532184, 1213.1279315233278, 1215.5322509422938, 1210.248467646741, 1204.3341011581283, 1205.073732035323, 1206.5198617724443, 1202.9004171833094, 1206.0526287292273, 1201.0027724916815, 1210.5179162844881, 1191.1673503942282, 1194.8789144989864, 1205.6773352170721, 1221.7578406248824, 1198.2149132911586, 1200.3565990324844, 1203.2200687363315, 1191.7564215789178, 1194.1099187966838, 1191.942140336359, 1192.9366616141422, 1223.3159034534713, 1194.1964453281657, 1193.798599380851, 1194.5937204064553, 1183.4345651966598, 1185.519602578031, 1187.1378519453312, 1192.8637883790825, 1192.4828604665097, 1185.7353937865403, 1187.551115199323, 1183.353536333138, 1235.599092503791, 1184.2225625738247, 1190.2914663084957, 1192.181381161332, 1182.3012917080503, 1187.992946431229, 1180.8303851733256, 1174.6040498998568, 1195.2676649474179, 1182.8519850586133, 1188.5450236532954, 1173.7903070933646, 1188.0595399356687, 1172.167961553555, 1184.1048167129068, 1181.362359444125, 1180.128943945665, 1177.607657081263, 1176.2339740796774, 1175.0114005643188, 1215.8717833848475, 1189.860943971905, 1178.0119794865086, 1177.6650458673018, 1188.2309774211778, 1202.393044033628, 1172.2887717048702, 1168.3065638873964, 1185.8325615349909, 1177.3065073683765, 1167.4447001412657, 1174.9421697830376, 1158.6043946193697, 1174.7005508511584, 1187.8837980213398, 1170.1614994150807, 1161.033287059049, 1158.6266441429357, 1175.150697232727, 1173.508492735409, 1175.2420275382008, 1181.5403123560711, 1170.686251539158, 1170.7187903147794, 1164.912587375614, 1196.219204648884, 1164.2917382557932, 1158.4953161422825, 1174.9954055838148, 1191.896155149874, 1164.591865143318, 1184.838945809887, 1168.047287400366, 1166.7980442754679, 1157.0641405936685, 1162.0361726132476, 1149.8967739707223, 1163.280810502815, 1153.5719656996864, 1176.725817080464, 1193.6375678326149, 1181.4503724821254, 1157.076779950136, 1158.9824246654582, 1164.1733459142208, 1149.8123330375774, 1157.9814458020542, 1160.44139398667, 1160.6215000309398, 1158.2212658429494, 1170.6295875869057, 1151.9440449465485, 1155.540300019308, 1166.4111528352623, 1167.3588831694829, 1185.7758334511527, 1152.036326578009, 1149.283243390824, 1156.9668806866603, 1166.861808450996, 1172.7219777769858, 1158.0088187784334, 1167.8999332835685, 1170.431418098188, 1161.814497111435, 1153.1661038916025, 1154.2238818976214, 1157.2878669599552, 1159.552540438147, 1178.9769136326715, 1161.9493883804923, 1152.394196395002, 1151.8232758688214, 1182.0305455561413, 1146.0519103330769, 1175.335386506871, 1152.898641301368, 1151.140651093233, 1153.2449841756536, 1157.3769540551527, 1153.4707082408402, 1197.268163411393, 1169.425375727869, 1156.7920102030905]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492, 1190.6139926210492]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('cd34bcac-1510-446e-9c6b-7bfeb20faf52');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.24% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1157   : Mean absolute error \n",
      "\n",
      "9.87% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Batchnorm\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Batchnorm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 2s 120us/sample - loss: 11193.2647 - val_loss: 11118.9971\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11176.2128 - val_loss: 11081.1694\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 11140.8975 - val_loss: 11036.2493\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 11087.0119 - val_loss: 10963.5245\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11010.1089 - val_loss: 10843.1981\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10903.1804 - val_loss: 10675.0540\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10760.0586 - val_loss: 10465.3191\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10573.3401 - val_loss: 10204.3620\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10335.7768 - val_loss: 9890.0583\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10037.1593 - val_loss: 9517.0301\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 9670.9130 - val_loss: 9071.2507\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 9225.1616 - val_loss: 8560.9366\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 8692.5583 - val_loss: 7953.1172\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 8063.0853 - val_loss: 7253.1635\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 7331.7243 - val_loss: 6433.1704\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 6492.9189 - val_loss: 5572.8495\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 5587.9371 - val_loss: 4715.2481\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 4745.8364 - val_loss: 4698.7706\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 4231.4139 - val_loss: 3855.7663\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 3934.7812 - val_loss: 3344.4070\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 3602.0063 - val_loss: 3028.1273\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 3248.9126 - val_loss: 2372.9591\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 2842.0228 - val_loss: 1938.4514\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 2419.7683 - val_loss: 1622.4962\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1992.8929 - val_loss: 1575.8434\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1641.0187 - val_loss: 1831.8637\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1466.3565 - val_loss: 2105.9923\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1407.2197 - val_loss: 1874.4142\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1397.1362 - val_loss: 1902.7148\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1375.1422 - val_loss: 1657.1879\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1378.0069 - val_loss: 1563.9027\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1348.9617 - val_loss: 1492.5223\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1354.6084 - val_loss: 1428.3651\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1346.9471 - val_loss: 1381.0074\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1343.8434 - val_loss: 1305.1219\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1330.5094 - val_loss: 1302.0024\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1330.2965 - val_loss: 1315.6098\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1292.1865 - val_loss: 1221.6448\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1324.0397 - val_loss: 1204.3531\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1284.1425 - val_loss: 1213.9876\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1271.7471 - val_loss: 1180.1906\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1282.03 - 0s 15us/sample - loss: 1280.6227 - val_loss: 1192.8597\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1258.6368 - val_loss: 1166.4785\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1290.3554 - val_loss: 1172.8904\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1294.8082 - val_loss: 1177.1777\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1255.8786 - val_loss: 1164.3527\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1259.8740 - val_loss: 1160.9768\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1274.0084 - val_loss: 1162.0273\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1244.2323 - val_loss: 1159.8698\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1245.9131 - val_loss: 1161.1819\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1286.5658 - val_loss: 1163.8020\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1230.3753 - val_loss: 1146.5269\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1256.1946 - val_loss: 1154.4610\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1224.4600 - val_loss: 1135.6088\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1232.5170 - val_loss: 1135.5245\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1250.2506 - val_loss: 1144.1641\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1227.9808 - val_loss: 1141.6249\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1197.1354 - val_loss: 1131.3039\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1205.8982 - val_loss: 1137.3158\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1226.2921 - val_loss: 1150.1144\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1197.4965 - val_loss: 1136.8092\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1222.4823 - val_loss: 1140.1697\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1179.2658 - val_loss: 1134.4565\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1207.2794 - val_loss: 1133.3826\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1201.5225 - val_loss: 1128.6067\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1189.6054 - val_loss: 1122.7445\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1170.86 - 0s 19us/sample - loss: 1171.4485 - val_loss: 1123.6841\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1164.0543 - val_loss: 1130.6397\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1154.8399 - val_loss: 1124.3568\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1177.3712 - val_loss: 1127.0466\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1188.5822 - val_loss: 1129.8644\n",
      "Epoch 72/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1188.0753 - val_loss: 1122.6812\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1166.5342 - val_loss: 1128.6818\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1201.1976 - val_loss: 1125.9392\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1171.3940 - val_loss: 1140.2794\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1151.4845 - val_loss: 1127.8106\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1160.4606 - val_loss: 1132.2761\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1169.6768 - val_loss: 1125.9864\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1159.0885 - val_loss: 1126.1226\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1139.2909 - val_loss: 1127.3988\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1142.6321 - val_loss: 1116.6416\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.3002 - val_loss: 1113.7631\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1170.5006 - val_loss: 1120.3890\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1178.0556 - val_loss: 1107.1405\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1149.0698 - val_loss: 1123.1816\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1140.4614 - val_loss: 1132.4769\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1140.8688 - val_loss: 1117.9462\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1141.9382 - val_loss: 1118.1651\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1121.7573 - val_loss: 1108.9468\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1151.1097 - val_loss: 1117.0323\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1125.7087 - val_loss: 1108.1048\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1097.3956 - val_loss: 1117.7990\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1112.2266 - val_loss: 1117.4689\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1144.7727 - val_loss: 1133.4383\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1116.8892 - val_loss: 1105.4733\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1100.4599 - val_loss: 1100.3310\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1116.6752 - val_loss: 1134.1766\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1094.5436 - val_loss: 1106.7269\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1082.6479 - val_loss: 1112.0960\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1124.6109 - val_loss: 1104.9969\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1129.6224 - val_loss: 1103.8074\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1107.2844 - val_loss: 1099.5315\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1097.4997 - val_loss: 1128.3406\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1089.0982 - val_loss: 1109.0539\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1121.8736 - val_loss: 1122.1176\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1091.1660 - val_loss: 1107.5300\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1093.6232 - val_loss: 1110.1117\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1090.7862 - val_loss: 1099.8834\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1076.3553 - val_loss: 1101.8465\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1065.5934 - val_loss: 1112.1890\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1054.2182 - val_loss: 1118.0413\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1066.2517 - val_loss: 1096.1615\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1086.3617 - val_loss: 1128.3516\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1078.3012 - val_loss: 1096.2587\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1069.3654 - val_loss: 1100.8275\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1080.2316 - val_loss: 1109.2173\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1068.2020 - val_loss: 1100.4154\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1066.7314 - val_loss: 1093.7244\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1052.1456 - val_loss: 1103.3359\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1069.5297 - val_loss: 1096.5013\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1072.6756 - val_loss: 1108.6039\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1085.9711 - val_loss: 1089.1017\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1047.1688 - val_loss: 1115.1976\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1081.4895 - val_loss: 1089.5555\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1044.2642 - val_loss: 1090.3099\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1033.8700 - val_loss: 1095.0009\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1033.8339 - val_loss: 1101.2091\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.9242 - val_loss: 1088.4621\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1068.3252 - val_loss: 1114.8222\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1027.2120 - val_loss: 1098.0760\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1035.8361 - val_loss: 1095.1967\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1031.3536 - val_loss: 1094.7311\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1055.4304 - val_loss: 1087.9799\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1051.6044 - val_loss: 1091.6263\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1029.4581 - val_loss: 1089.3390\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1021.2301 - val_loss: 1095.4837\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1013.1725 - val_loss: 1090.7476\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1024.4482 - val_loss: 1095.1908\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1018.7806 - val_loss: 1104.1510\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1042.2239 - val_loss: 1084.4705\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1044.7968 - val_loss: 1088.7474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1027.0748 - val_loss: 1092.4569\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1005.4187 - val_loss: 1105.6217\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1015.4161 - val_loss: 1090.5424\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1029.2523 - val_loss: 1086.4205\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1003.4030 - val_loss: 1091.2997\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1019.5471 - val_loss: 1090.0725\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1005.0308 - val_loss: 1095.7604\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1077.5319 - val_loss: 1088.8389\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1028.9360 - val_loss: 1095.2153\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1020.3849 - val_loss: 1092.9372\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1000.5441 - val_loss: 1088.7955\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1011.0064 - val_loss: 1091.5896\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1036.1131 - val_loss: 1087.5512\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1014.7569 - val_loss: 1094.5105\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1025.3762 - val_loss: 1093.5482\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 992.7932 - val_loss: 1095.5456\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1012.9903 - val_loss: 1085.8091\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1018.4058 - val_loss: 1089.4413\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1004.0057 - val_loss: 1099.4199\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1018.8784 - val_loss: 1084.7083\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 991.7420 - val_loss: 1089.3172\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 975.8299 - val_loss: 1095.6891\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 990.3401 - val_loss: 1097.1418\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 995.6189 - val_loss: 1087.7055\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1007.1032 - val_loss: 1096.4041\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 976.4319 - val_loss: 1086.3784\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 999.7779 - val_loss: 1084.9816\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1016.3167 - val_loss: 1078.7747\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1002.6336 - val_loss: 1081.2125\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 982.3568 - val_loss: 1086.6730\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 995.3235 - val_loss: 1092.7093\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 975.5057 - val_loss: 1083.7080\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 982.3565 - val_loss: 1082.4932\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 989.3612 - val_loss: 1087.5215\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 975.9218 - val_loss: 1083.2459\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 987.2999 - val_loss: 1085.4744\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 993.1720 - val_loss: 1081.8045\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 966.2327 - val_loss: 1085.4723\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 981.4232 - val_loss: 1084.9512\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 984.0311 - val_loss: 1084.7494\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 977.4360 - val_loss: 1089.5342\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 978.7300 - val_loss: 1087.3371\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 956.8462 - val_loss: 1097.7459\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 962.9183 - val_loss: 1084.9430\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 957.6327 - val_loss: 1095.7773\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 971.4231 - val_loss: 1080.1120\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 969.5737 - val_loss: 1089.2939\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 972.5167 - val_loss: 1087.1733\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 965.6096 - val_loss: 1093.9244\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 950.2777 - val_loss: 1089.8688\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 955.9127 - val_loss: 1090.0825\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 954.5039 - val_loss: 1078.6874\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 953.6976 - val_loss: 1098.5626\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 965.3139 - val_loss: 1080.8228\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 965.2549 - val_loss: 1080.5744\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 947.3068 - val_loss: 1084.7954\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 951.5229 - val_loss: 1083.5271\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 959.8999 - val_loss: 1073.3963\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 947.1427 - val_loss: 1081.8032\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1156.7920102030905"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_val_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9670.913028170118,
          9225.16158751034,
          8692.55831607492,
          8063.085313514356,
          7331.724335126798,
          6492.918900567256,
          5587.937127155605,
          4745.83636723058,
          4231.413860901625,
          3934.781235411288,
          3602.006280538384,
          3248.912584878888,
          2842.0227544251775,
          2419.768313386289,
          1992.89294164339,
          1641.0186711524182,
          1466.3564945126861,
          1407.2196843384597,
          1397.1361942417473,
          1375.1422410670086,
          1378.006891771144,
          1348.9617415681162,
          1354.6083867861125,
          1346.9470918088202,
          1343.8434071407926,
          1330.5094368894281,
          1330.296514957444,
          1292.186496340681,
          1324.0396585565832,
          1284.1424828332963,
          1271.7471349581176,
          1280.6227400469736,
          1258.6368017783739,
          1290.3553962470392,
          1294.8082088380963,
          1255.8785992016744,
          1259.87404348474,
          1274.0084295094982,
          1244.2322568218383,
          1245.9130808706152,
          1286.5658069378821,
          1230.3753077826568,
          1256.194570629731,
          1224.4600197574239,
          1232.5170107807644,
          1250.2505579937524,
          1227.9808167495253,
          1197.1354105146036,
          1205.8982221696333,
          1226.2920729394473,
          1197.4964511489638,
          1222.4822921186883,
          1179.2658464008375,
          1207.2794204629301,
          1201.5225395843647,
          1189.6054238089346,
          1171.4485259575285,
          1164.0543377136023,
          1154.8398737596658,
          1177.371156878,
          1188.5822302537379,
          1188.0752977761713,
          1166.5341606683235,
          1201.1976207230596,
          1171.3939728401265,
          1151.484522306826,
          1160.460603061123,
          1169.6768192515383,
          1159.0884711627184,
          1139.2908664333909,
          1142.6321204663375,
          1159.3002167941336,
          1170.500644449005,
          1178.0556081799004,
          1149.0698322229593,
          1140.4613771195734,
          1140.8688288035219,
          1141.9382207949654,
          1121.7572745534876,
          1151.1096859158029,
          1125.708683250279,
          1097.3956083669102,
          1112.2265764522915,
          1144.7727066936534,
          1116.8892037755195,
          1100.4598886058832,
          1116.6752304526926,
          1094.5436112652662,
          1082.6478987212456,
          1124.610937265014,
          1129.622367471833,
          1107.2843963886182,
          1097.499746753669,
          1089.098176499132,
          1121.8736206079166,
          1091.165979593819,
          1093.6232496972598,
          1090.7862154737081,
          1076.3553300103706,
          1065.5933500955023,
          1054.2182023847943,
          1066.251739887477,
          1086.3617207473808,
          1078.3011815828577,
          1069.365391032309,
          1080.2316240731761,
          1068.2019951336365,
          1066.731394692607,
          1052.1455886134027,
          1069.5296788985347,
          1072.6755971042287,
          1085.9710962614124,
          1047.1688355299952,
          1081.489454490856,
          1044.2641706082297,
          1033.8699731606866,
          1033.8338969406393,
          1060.9241578641577,
          1068.3251965363852,
          1027.2120396331243,
          1035.8361482285584,
          1031.3536164976204,
          1055.4303534037128,
          1051.6044081310627,
          1029.4581324774492,
          1021.2300926500699,
          1013.1725239529026,
          1024.4482168041204,
          1018.780630077655,
          1042.2238786665644,
          1044.7968147358915,
          1027.074766928193,
          1005.4186840130042,
          1015.4160626711481,
          1029.2523440951356,
          1003.4029896309506,
          1019.547126679758,
          1005.0308468522063,
          1077.5318756256502,
          1028.9359627512,
          1020.3848990774451,
          1000.544149603422,
          1011.006444991842,
          1036.1131031502218,
          1014.7569052338815,
          1025.3761944630257,
          992.7932355080239,
          1012.9903155430683,
          1018.4057791958311,
          1004.0057432772475,
          1018.878449202301,
          991.7419879332749,
          975.8298992541742,
          990.3401302492927,
          995.6189157483285,
          1007.1031548481129,
          976.4318693691489,
          999.7778872407125,
          1016.3166764593803,
          1002.6335671231531,
          982.356793165159,
          995.3235194464593,
          975.5056998905161,
          982.3565419382386,
          989.3611964594273,
          975.9217608849415,
          987.2999372587477,
          993.1720212910011,
          966.2327276137685,
          981.4231503863169,
          984.0311471446855,
          977.4360213508246,
          978.7300439394379,
          956.8461710407998,
          962.9183416923063,
          957.6326629534642,
          971.4231215760586,
          969.5737256466422,
          972.5166873323767,
          965.6096438019123,
          950.2777316481837,
          955.9126555704989,
          954.5038678689593,
          953.6976450294202,
          965.3138569337896,
          965.2548702799153,
          947.3067949079716,
          951.5228642933541,
          959.8999076798896,
          947.1426853128681
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9071.250697320847,
          8560.936606270052,
          7953.117237924072,
          7253.1634956942735,
          6433.170413583129,
          5572.849548474471,
          4715.24814860434,
          4698.77059642179,
          3855.7663364689597,
          3344.4070248172593,
          3028.127285091629,
          2372.9591470538244,
          1938.4513635843819,
          1622.4961995181613,
          1575.8433928213356,
          1831.8636541923063,
          2105.992251190987,
          1874.4142030509795,
          1902.7148290633772,
          1657.187866651536,
          1563.9026911620604,
          1492.5223033010827,
          1428.3650664824247,
          1381.0073705060893,
          1305.1219437627676,
          1302.0024462773133,
          1315.6097616742984,
          1221.6447867727577,
          1204.3531203345494,
          1213.9876091656668,
          1180.1906152040226,
          1192.8596782298036,
          1166.4785444352583,
          1172.8903886677626,
          1177.177715992244,
          1164.3527249217104,
          1160.9767629234448,
          1162.0272527418945,
          1159.8698311410449,
          1161.1819447604787,
          1163.8020100307674,
          1146.5269457425243,
          1154.4609748529772,
          1135.6087922592883,
          1135.5245410773089,
          1144.1640990941683,
          1141.62493537886,
          1131.3039428392726,
          1137.3157879187058,
          1150.1144123402487,
          1136.809239520801,
          1140.1696712477833,
          1134.4564941357294,
          1133.382648230125,
          1128.6066605939113,
          1122.7445183404586,
          1123.6841263114175,
          1130.6397375118668,
          1124.356788024841,
          1127.0466384229858,
          1129.8644247749226,
          1122.681172146213,
          1128.6818465314504,
          1125.9392020082291,
          1140.2793692310945,
          1127.8105615616228,
          1132.2761340128866,
          1125.98640650261,
          1126.1226207671195,
          1127.3988304161132,
          1116.6416193822688,
          1113.7631496431738,
          1120.3890148321182,
          1107.140548776428,
          1123.181571793695,
          1132.476913094162,
          1117.9461695868242,
          1118.1651206956446,
          1108.9468072799827,
          1117.032333774846,
          1108.1048393303056,
          1117.7989761172016,
          1117.4688606392244,
          1133.438309208434,
          1105.4732589889963,
          1100.3309624555093,
          1134.1765827479571,
          1106.72693710679,
          1112.096033500578,
          1104.9969039375424,
          1103.8074186733163,
          1099.5315418476712,
          1128.3405687061752,
          1109.05387131542,
          1122.1175572602046,
          1107.530034094505,
          1110.1117413820857,
          1099.8834155327288,
          1101.8464647242988,
          1112.1889996755235,
          1118.0413161867339,
          1096.1615182384355,
          1128.3516068291228,
          1096.2587443662114,
          1100.8274982238977,
          1109.2172790613015,
          1100.4153908805472,
          1093.724361831974,
          1103.3358543737154,
          1096.5013109524216,
          1108.603894466433,
          1089.1017312248173,
          1115.1975622703012,
          1089.555450396617,
          1090.3099209311397,
          1095.0009209491552,
          1101.2091259295653,
          1088.4620583585681,
          1114.8222290797871,
          1098.0760445175033,
          1095.1967433686962,
          1094.731078049021,
          1087.9799395175503,
          1091.6262953846015,
          1089.3389836524182,
          1095.4837266835177,
          1090.7476266662463,
          1095.1907515693147,
          1104.1509720928711,
          1084.4705242419352,
          1088.747440048227,
          1092.456859171737,
          1105.6217390557247,
          1090.542382141811,
          1086.4204628812256,
          1091.299696496046,
          1090.0724682759158,
          1095.7604418215876,
          1088.8389110025912,
          1095.2152512058697,
          1092.9371890841971,
          1088.7954748753007,
          1091.5896407074408,
          1087.5512265728785,
          1094.510510065428,
          1093.548204800254,
          1095.5456458520669,
          1085.809141267295,
          1089.441306674698,
          1099.4198696395786,
          1084.708327417888,
          1089.3172458107877,
          1095.68912159897,
          1097.1418041644604,
          1087.7054536374653,
          1096.4040751559523,
          1086.3784054361652,
          1084.9816081626682,
          1078.774745686441,
          1081.2124805010606,
          1086.6730176192475,
          1092.7092551228898,
          1083.7080301361666,
          1082.4931802667404,
          1087.5214538757803,
          1083.2458567813424,
          1085.4743750499345,
          1081.8044934897139,
          1085.4722849232223,
          1084.9512083467798,
          1084.74943483428,
          1089.5342166733114,
          1087.337143198292,
          1097.7458740626018,
          1084.9430346048164,
          1095.7772988823676,
          1080.1120469127936,
          1089.2939288879602,
          1087.1732906925192,
          1093.9244425643392,
          1089.8687769107883,
          1090.0825227867847,
          1078.6874454391966,
          1098.562616734173,
          1080.8228022741941,
          1080.5744255131701,
          1084.7953775764254,
          1083.5271275139582,
          1073.396307425674,
          1081.8032113475506
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905,
          1156.7920102030905
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"21628112-8235-428d-b905-fd546bfd233a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"21628112-8235-428d-b905-fd546bfd233a\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '21628112-8235-428d-b905-fd546bfd233a',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9670.913028170118, 9225.16158751034, 8692.55831607492, 8063.085313514356, 7331.724335126798, 6492.918900567256, 5587.937127155605, 4745.83636723058, 4231.413860901625, 3934.781235411288, 3602.006280538384, 3248.912584878888, 2842.0227544251775, 2419.768313386289, 1992.89294164339, 1641.0186711524182, 1466.3564945126861, 1407.2196843384597, 1397.1361942417473, 1375.1422410670086, 1378.006891771144, 1348.9617415681162, 1354.6083867861125, 1346.9470918088202, 1343.8434071407926, 1330.5094368894281, 1330.296514957444, 1292.186496340681, 1324.0396585565832, 1284.1424828332963, 1271.7471349581176, 1280.6227400469736, 1258.6368017783739, 1290.3553962470392, 1294.8082088380963, 1255.8785992016744, 1259.87404348474, 1274.0084295094982, 1244.2322568218383, 1245.9130808706152, 1286.5658069378821, 1230.3753077826568, 1256.194570629731, 1224.4600197574239, 1232.5170107807644, 1250.2505579937524, 1227.9808167495253, 1197.1354105146036, 1205.8982221696333, 1226.2920729394473, 1197.4964511489638, 1222.4822921186883, 1179.2658464008375, 1207.2794204629301, 1201.5225395843647, 1189.6054238089346, 1171.4485259575285, 1164.0543377136023, 1154.8398737596658, 1177.371156878, 1188.5822302537379, 1188.0752977761713, 1166.5341606683235, 1201.1976207230596, 1171.3939728401265, 1151.484522306826, 1160.460603061123, 1169.6768192515383, 1159.0884711627184, 1139.2908664333909, 1142.6321204663375, 1159.3002167941336, 1170.500644449005, 1178.0556081799004, 1149.0698322229593, 1140.4613771195734, 1140.8688288035219, 1141.9382207949654, 1121.7572745534876, 1151.1096859158029, 1125.708683250279, 1097.3956083669102, 1112.2265764522915, 1144.7727066936534, 1116.8892037755195, 1100.4598886058832, 1116.6752304526926, 1094.5436112652662, 1082.6478987212456, 1124.610937265014, 1129.622367471833, 1107.2843963886182, 1097.499746753669, 1089.098176499132, 1121.8736206079166, 1091.165979593819, 1093.6232496972598, 1090.7862154737081, 1076.3553300103706, 1065.5933500955023, 1054.2182023847943, 1066.251739887477, 1086.3617207473808, 1078.3011815828577, 1069.365391032309, 1080.2316240731761, 1068.2019951336365, 1066.731394692607, 1052.1455886134027, 1069.5296788985347, 1072.6755971042287, 1085.9710962614124, 1047.1688355299952, 1081.489454490856, 1044.2641706082297, 1033.8699731606866, 1033.8338969406393, 1060.9241578641577, 1068.3251965363852, 1027.2120396331243, 1035.8361482285584, 1031.3536164976204, 1055.4303534037128, 1051.6044081310627, 1029.4581324774492, 1021.2300926500699, 1013.1725239529026, 1024.4482168041204, 1018.780630077655, 1042.2238786665644, 1044.7968147358915, 1027.074766928193, 1005.4186840130042, 1015.4160626711481, 1029.2523440951356, 1003.4029896309506, 1019.547126679758, 1005.0308468522063, 1077.5318756256502, 1028.9359627512, 1020.3848990774451, 1000.544149603422, 1011.006444991842, 1036.1131031502218, 1014.7569052338815, 1025.3761944630257, 992.7932355080239, 1012.9903155430683, 1018.4057791958311, 1004.0057432772475, 1018.878449202301, 991.7419879332749, 975.8298992541742, 990.3401302492927, 995.6189157483285, 1007.1031548481129, 976.4318693691489, 999.7778872407125, 1016.3166764593803, 1002.6335671231531, 982.356793165159, 995.3235194464593, 975.5056998905161, 982.3565419382386, 989.3611964594273, 975.9217608849415, 987.2999372587477, 993.1720212910011, 966.2327276137685, 981.4231503863169, 984.0311471446855, 977.4360213508246, 978.7300439394379, 956.8461710407998, 962.9183416923063, 957.6326629534642, 971.4231215760586, 969.5737256466422, 972.5166873323767, 965.6096438019123, 950.2777316481837, 955.9126555704989, 954.5038678689593, 953.6976450294202, 965.3138569337896, 965.2548702799153, 947.3067949079716, 951.5228642933541, 959.8999076798896, 947.1426853128681]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9071.250697320847, 8560.936606270052, 7953.117237924072, 7253.1634956942735, 6433.170413583129, 5572.849548474471, 4715.24814860434, 4698.77059642179, 3855.7663364689597, 3344.4070248172593, 3028.127285091629, 2372.9591470538244, 1938.4513635843819, 1622.4961995181613, 1575.8433928213356, 1831.8636541923063, 2105.992251190987, 1874.4142030509795, 1902.7148290633772, 1657.187866651536, 1563.9026911620604, 1492.5223033010827, 1428.3650664824247, 1381.0073705060893, 1305.1219437627676, 1302.0024462773133, 1315.6097616742984, 1221.6447867727577, 1204.3531203345494, 1213.9876091656668, 1180.1906152040226, 1192.8596782298036, 1166.4785444352583, 1172.8903886677626, 1177.177715992244, 1164.3527249217104, 1160.9767629234448, 1162.0272527418945, 1159.8698311410449, 1161.1819447604787, 1163.8020100307674, 1146.5269457425243, 1154.4609748529772, 1135.6087922592883, 1135.5245410773089, 1144.1640990941683, 1141.62493537886, 1131.3039428392726, 1137.3157879187058, 1150.1144123402487, 1136.809239520801, 1140.1696712477833, 1134.4564941357294, 1133.382648230125, 1128.6066605939113, 1122.7445183404586, 1123.6841263114175, 1130.6397375118668, 1124.356788024841, 1127.0466384229858, 1129.8644247749226, 1122.681172146213, 1128.6818465314504, 1125.9392020082291, 1140.2793692310945, 1127.8105615616228, 1132.2761340128866, 1125.98640650261, 1126.1226207671195, 1127.3988304161132, 1116.6416193822688, 1113.7631496431738, 1120.3890148321182, 1107.140548776428, 1123.181571793695, 1132.476913094162, 1117.9461695868242, 1118.1651206956446, 1108.9468072799827, 1117.032333774846, 1108.1048393303056, 1117.7989761172016, 1117.4688606392244, 1133.438309208434, 1105.4732589889963, 1100.3309624555093, 1134.1765827479571, 1106.72693710679, 1112.096033500578, 1104.9969039375424, 1103.8074186733163, 1099.5315418476712, 1128.3405687061752, 1109.05387131542, 1122.1175572602046, 1107.530034094505, 1110.1117413820857, 1099.8834155327288, 1101.8464647242988, 1112.1889996755235, 1118.0413161867339, 1096.1615182384355, 1128.3516068291228, 1096.2587443662114, 1100.8274982238977, 1109.2172790613015, 1100.4153908805472, 1093.724361831974, 1103.3358543737154, 1096.5013109524216, 1108.603894466433, 1089.1017312248173, 1115.1975622703012, 1089.555450396617, 1090.3099209311397, 1095.0009209491552, 1101.2091259295653, 1088.4620583585681, 1114.8222290797871, 1098.0760445175033, 1095.1967433686962, 1094.731078049021, 1087.9799395175503, 1091.6262953846015, 1089.3389836524182, 1095.4837266835177, 1090.7476266662463, 1095.1907515693147, 1104.1509720928711, 1084.4705242419352, 1088.747440048227, 1092.456859171737, 1105.6217390557247, 1090.542382141811, 1086.4204628812256, 1091.299696496046, 1090.0724682759158, 1095.7604418215876, 1088.8389110025912, 1095.2152512058697, 1092.9371890841971, 1088.7954748753007, 1091.5896407074408, 1087.5512265728785, 1094.510510065428, 1093.548204800254, 1095.5456458520669, 1085.809141267295, 1089.441306674698, 1099.4198696395786, 1084.708327417888, 1089.3172458107877, 1095.68912159897, 1097.1418041644604, 1087.7054536374653, 1096.4040751559523, 1086.3784054361652, 1084.9816081626682, 1078.774745686441, 1081.2124805010606, 1086.6730176192475, 1092.7092551228898, 1083.7080301361666, 1082.4931802667404, 1087.5214538757803, 1083.2458567813424, 1085.4743750499345, 1081.8044934897139, 1085.4722849232223, 1084.9512083467798, 1084.74943483428, 1089.5342166733114, 1087.337143198292, 1097.7458740626018, 1084.9430346048164, 1095.7772988823676, 1080.1120469127936, 1089.2939288879602, 1087.1732906925192, 1093.9244425643392, 1089.8687769107883, 1090.0825227867847, 1078.6874454391966, 1098.562616734173, 1080.8228022741941, 1080.5744255131701, 1084.7953775764254, 1083.5271275139582, 1073.396307425674, 1081.8032113475506]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905, 1156.7920102030905]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('21628112-8235-428d-b905-fd546bfd233a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.81% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1082   : Mean absolute error \n",
      "\n",
      "9.29% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"LeakyRELU\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeakyRELU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 2s 115us/sample - loss: 11193.0127 - val_loss: 11115.5698\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11179.0485 - val_loss: 11076.8315\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 11154.1232 - val_loss: 11051.6703\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 11116.2467 - val_loss: 11000.4914\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11061.6512 - val_loss: 10915.3436\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 10985.3104 - val_loss: 10811.8844\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10882.4020 - val_loss: 10668.3015\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10748.2833 - val_loss: 10517.6703\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10576.6915 - val_loss: 10263.0022\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10362.2564 - val_loss: 10040.1934\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 10101.3670 - val_loss: 9659.5144\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 9783.2147 - val_loss: 9238.7824\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 9404.6903 - val_loss: 8713.7304\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 8958.4934 - val_loss: 8292.3039\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 8440.7684 - val_loss: 7549.3592\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 7839.3250 - val_loss: 6754.9442\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 7171.1444 - val_loss: 6144.7865\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 6450.5607 - val_loss: 5420.9140\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 5755.9816 - val_loss: 4676.8013\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 5049.0525 - val_loss: 4101.4807\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 4370.0710 - val_loss: 3718.8857\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 3775.4592 - val_loss: 3550.7752\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 3197.0355 - val_loss: 3460.2327\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 2734.8202 - val_loss: 2607.6891\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 2309.6266 - val_loss: 2305.0633\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1909.9141 - val_loss: 1764.1403\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1635.2599 - val_loss: 1850.9180\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1500.0440 - val_loss: 1804.7693\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1454.3272 - val_loss: 1772.9450\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1437.9872 - val_loss: 1648.3996\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1436.9654 - val_loss: 1557.8170\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1409.3603 - val_loss: 1573.8336\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1409.6801 - val_loss: 1384.2231\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1416.8282 - val_loss: 1347.0178\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1410.6087 - val_loss: 1431.0257\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1397.1649 - val_loss: 1342.5653\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1392.0380 - val_loss: 1320.0142\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1367.5322 - val_loss: 1327.8969\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1369.1486 - val_loss: 1274.3461\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1360.8128 - val_loss: 1281.9929\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1362.6091 - val_loss: 1279.3131\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1366.9945 - val_loss: 1245.3723\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1359.8138 - val_loss: 1238.7899\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1359.1375 - val_loss: 1233.7826\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1354.7068 - val_loss: 1231.1768\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1350.5894 - val_loss: 1235.8565\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1348.2091 - val_loss: 1221.4816\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1344.1889 - val_loss: 1217.0775\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1342.1420 - val_loss: 1217.6794\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1340.1339 - val_loss: 1210.2335\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1343.8909 - val_loss: 1218.6670\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1327.2224 - val_loss: 1221.1763\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1331.4648 - val_loss: 1209.0869\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1331.2045 - val_loss: 1209.5308\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1325.1880 - val_loss: 1205.0465\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1329.6789 - val_loss: 1218.9563\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1328.3915 - val_loss: 1203.7280\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1315.4335 - val_loss: 1199.6113\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1325.2360 - val_loss: 1208.4093\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1323.6454 - val_loss: 1203.1355\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1302.8175 - val_loss: 1199.8330\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1320.3253 - val_loss: 1229.0978\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1306.3250 - val_loss: 1192.3649\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1315.7718 - val_loss: 1191.7233\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1314.8794 - val_loss: 1187.7540\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1320.9579 - val_loss: 1190.1513\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1303.3602 - val_loss: 1187.3198\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1301.2455 - val_loss: 1211.1003\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1294.4702 - val_loss: 1194.0773\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1301.1165 - val_loss: 1183.5640\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1304.7439 - val_loss: 1185.1122\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1297.0761 - val_loss: 1199.0675\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1293.2489 - val_loss: 1173.3242\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1296.2010 - val_loss: 1185.4635\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1295.6318 - val_loss: 1177.3871\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1289.5140 - val_loss: 1177.8125\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1282.3929 - val_loss: 1171.6203\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1300.3427 - val_loss: 1192.2241\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1283.6681 - val_loss: 1182.5030\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1278.2873 - val_loss: 1177.9386\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1283.4608 - val_loss: 1173.7852\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1289.2974 - val_loss: 1176.7013\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1283.1687 - val_loss: 1187.8548\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1276.7715 - val_loss: 1175.1753\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1284.5787 - val_loss: 1176.4109\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1279.9832 - val_loss: 1168.5524\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1280.5649 - val_loss: 1179.9012\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1272.9057 - val_loss: 1183.2035\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1272.4097 - val_loss: 1166.6700\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1276.8249 - val_loss: 1176.0050\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1278.0847 - val_loss: 1198.1179\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1271.7219 - val_loss: 1172.6444\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1263.6549 - val_loss: 1192.9102\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1278.7764 - val_loss: 1176.8380\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1266.1845 - val_loss: 1168.2600\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1269.8520 - val_loss: 1164.2310\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1264.4226 - val_loss: 1211.8807\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1265.5435 - val_loss: 1172.0455\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1255.3103 - val_loss: 1157.8885\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1266.1242 - val_loss: 1159.4459\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1263.3860 - val_loss: 1164.3989\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1275.2386 - val_loss: 1166.5401\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1259.0209 - val_loss: 1166.0319\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1270.1573 - val_loss: 1175.8513\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1273.6892 - val_loss: 1166.9814\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1262.1110 - val_loss: 1162.1941\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1254.3201 - val_loss: 1199.6968\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1263.9916 - val_loss: 1154.3349\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1250.0373 - val_loss: 1172.6834\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1258.8128 - val_loss: 1161.1984\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1243.1024 - val_loss: 1154.2478\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1255.9188 - val_loss: 1156.2233\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1251.2484 - val_loss: 1161.4931\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1243.0857 - val_loss: 1164.8634\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1238.7080 - val_loss: 1169.8460\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1251.8478 - val_loss: 1161.4011\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1239.9549 - val_loss: 1154.3911\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1252.8166 - val_loss: 1155.4619\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1238.4841 - val_loss: 1165.6273\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1245.5574 - val_loss: 1155.2306\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1243.1049 - val_loss: 1152.0152\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1242.7184 - val_loss: 1166.3564\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1233.7060 - val_loss: 1151.3007\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1248.0161 - val_loss: 1157.3792\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1251.6805 - val_loss: 1165.3604\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1233.2673 - val_loss: 1162.5574\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1236.5442 - val_loss: 1154.5966\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1236.4576 - val_loss: 1147.6487\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1241.8628 - val_loss: 1169.3206\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1225.9873 - val_loss: 1159.3353\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1227.9730 - val_loss: 1153.4026\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1230.9704 - val_loss: 1152.6655\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1235.5486 - val_loss: 1153.9661\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1223.6590 - val_loss: 1146.3540\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1228.2354 - val_loss: 1156.1768\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1233.9503 - val_loss: 1144.6375\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1218.1072 - val_loss: 1164.9917\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1225.9059 - val_loss: 1157.6394\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1232.0697 - val_loss: 1150.3817\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1230.9438 - val_loss: 1153.7172\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1222.3222 - val_loss: 1156.2205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1222.5796 - val_loss: 1154.9052\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1224.0068 - val_loss: 1167.8904\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1224.4309 - val_loss: 1170.5528\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1214.5126 - val_loss: 1157.6741\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1229.9901 - val_loss: 1149.1673\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1236.8966 - val_loss: 1199.9494\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1227.0992 - val_loss: 1147.4083\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1225.3673 - val_loss: 1143.2794\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1226.9440 - val_loss: 1141.6903\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1214.8919 - val_loss: 1147.6369\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1208.5335 - val_loss: 1149.2060\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1223.0367 - val_loss: 1143.5264\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1224.8147 - val_loss: 1162.4126\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1214.6616 - val_loss: 1137.4394\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1227.8943 - val_loss: 1156.6370\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1216.6235 - val_loss: 1143.2412\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1216.0045 - val_loss: 1148.2772\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1217.6311 - val_loss: 1147.3774\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1210.1225 - val_loss: 1139.8230\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1203.5657 - val_loss: 1143.8795\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1205.1980 - val_loss: 1136.5428\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1216.9086 - val_loss: 1151.6649\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1216.8535 - val_loss: 1149.9159\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1204.2357 - val_loss: 1152.0959\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1212.3526 - val_loss: 1151.4010\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1216.3199 - val_loss: 1140.8738\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1213.0164 - val_loss: 1143.4637\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1216.4806 - val_loss: 1147.5853\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1204.5474 - val_loss: 1137.4354\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1201.9337 - val_loss: 1135.1880\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1205.5551 - val_loss: 1131.9398\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1201.6692 - val_loss: 1154.8894\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1200.8697 - val_loss: 1140.3558\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1198.1965 - val_loss: 1139.9798\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1208.2365 - val_loss: 1135.1486\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1209.4234 - val_loss: 1140.5781\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1206.9068 - val_loss: 1139.9017\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1198.2397 - val_loss: 1145.9474\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1209.2088 - val_loss: 1175.6581\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1202.3337 - val_loss: 1164.7424\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1198.7056 - val_loss: 1137.9538\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1198.8729 - val_loss: 1140.7382\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1200.4435 - val_loss: 1127.1544\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1192.8447 - val_loss: 1141.7914\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1195.5026 - val_loss: 1152.2248\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1204.3532 - val_loss: 1139.0947\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1191.5281 - val_loss: 1142.0871\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1194.6608 - val_loss: 1161.5012\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1195.6986 - val_loss: 1133.5375\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1194.2666 - val_loss: 1132.1396\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1192.5801 - val_loss: 1157.0348\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1183.2578 - val_loss: 1145.5234\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1193.2960 - val_loss: 1148.7901\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1195.8385 - val_loss: 1135.1071\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1190.5941 - val_loss: 1136.7655\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1185.6171 - val_loss: 1129.4152\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1180.2692 - val_loss: 1129.6284\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1197.6854 - val_loss: 1144.5230\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1194.3498 - val_loss: 1137.3919\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1185.6177 - val_loss: 1176.2296\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1192.9358 - val_loss: 1196.3319\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1191.7279 - val_loss: 1130.5280\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1184.1726 - val_loss: 1138.0904\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1191.4725 - val_loss: 1126.4697\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1188.1359 - val_loss: 1133.6999\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1185.5792 - val_loss: 1130.1193\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1177.4546 - val_loss: 1133.6488\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1182.8630 - val_loss: 1124.4046\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1178.6815 - val_loss: 1145.2329\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1190.2047 - val_loss: 1145.3258\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1184.2458 - val_loss: 1134.3451\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1180.3436 - val_loss: 1132.3320\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1192.5994 - val_loss: 1144.7656\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1180.0101 - val_loss: 1128.9191\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1176.9713 - val_loss: 1134.1195\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1186.1881 - val_loss: 1153.6358\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1187.4133 - val_loss: 1133.1147\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1182.1510 - val_loss: 1127.6236\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1180.2804 - val_loss: 1129.4300\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1180.3175 - val_loss: 1131.8621\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1166.4448 - val_loss: 1117.7781\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1170.7579 - val_loss: 1132.6167\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1165.6899 - val_loss: 1131.8309\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1179.7146 - val_loss: 1129.7553\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1178.2797 - val_loss: 1131.4833\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1178.2700 - val_loss: 1142.3210\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1169.4284 - val_loss: 1128.7921\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1181.9555 - val_loss: 1138.5066\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1177.7571 - val_loss: 1130.7048\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1180.7910 - val_loss: 1124.9439\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1170.1979 - val_loss: 1133.0925\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1177.7216 - val_loss: 1185.0374\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1177.6327 - val_loss: 1133.2578\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1170.1478 - val_loss: 1138.6551\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1172.0172 - val_loss: 1124.4740\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1168.8853 - val_loss: 1118.3849\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1162.9933 - val_loss: 1127.9364\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1167.0983 - val_loss: 1124.2480\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1170.6986 - val_loss: 1123.7470\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1170.7652 - val_loss: 1121.5952\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1168.0823 - val_loss: 1130.3854\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1175.8315 - val_loss: 1134.0010\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1168.3723 - val_loss: 1127.6723\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1171.1898 - val_loss: 1152.9504\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1176.3182 - val_loss: 1128.1133\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1166.9361 - val_loss: 1122.7545\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1161.6018 - val_loss: 1120.8459\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1162.3694 - val_loss: 1125.7527\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1177.2154 - val_loss: 1128.9280\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1166.9511 - val_loss: 1129.4363\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1172.2872 - val_loss: 1123.8283\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1164.6106 - val_loss: 1130.3114\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1163.5773 - val_loss: 1124.1080\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1161.0531 - val_loss: 1121.6514\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1173.9458 - val_loss: 1126.2403\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1167.1384 - val_loss: 1144.7683\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1166.9999 - val_loss: 1138.0515\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1164.4141 - val_loss: 1127.8831\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.6875 - val_loss: 1126.6797\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1162.7620 - val_loss: 1124.1550\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.2661 - val_loss: 1121.0082\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1156.7078 - val_loss: 1126.1808\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1160.9772 - val_loss: 1114.6116\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1157.4377 - val_loss: 1144.3396\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1163.8476 - val_loss: 1123.9103\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1156.5976 - val_loss: 1116.8947\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1161.5580 - val_loss: 1133.9964\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.8556 - val_loss: 1122.7469\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1155.0194 - val_loss: 1121.2310\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1161.4502 - val_loss: 1117.2006\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1156.0978 - val_loss: 1118.4368\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1151.8330 - val_loss: 1114.1406\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1161.7006 - val_loss: 1135.2256\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1168.4665 - val_loss: 1154.1728\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1163.0180 - val_loss: 1120.8373\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1164.7105 - val_loss: 1134.5728\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1155.7269 - val_loss: 1134.5619\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.4978 - val_loss: 1122.0373\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1155.7635 - val_loss: 1147.3790\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1147.5927 - val_loss: 1130.6278\n",
      "Epoch 282/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1152.9554 - val_loss: 1135.1727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1150.7890 - val_loss: 1119.1888\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1148.6266 - val_loss: 1125.2547\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1149.5249 - val_loss: 1119.9348\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1148.8255 - val_loss: 1128.7576\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1142.0614 - val_loss: 1120.1838\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1153.1345 - val_loss: 1127.9053\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1155.0581 - val_loss: 1124.1673\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1144.8657 - val_loss: 1133.0040\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1165.9516 - val_loss: 1125.1196\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1151.8700 - val_loss: 1136.9532\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1150.8436 - val_loss: 1119.4838\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1151.0231 - val_loss: 1121.1097\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1142.4156 - val_loss: 1124.9857\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1149.2515 - val_loss: 1123.7549\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1145.4854 - val_loss: 1139.9184\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1144.2628 - val_loss: 1135.0897\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1143.4540 - val_loss: 1113.1265\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.2616 - val_loss: 1120.0473\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10101.367019485037,
          9783.214746622469,
          9404.69025246892,
          8958.493435471288,
          8440.768356516004,
          7839.324987741566,
          7171.1443976017335,
          6450.560655947651,
          5755.981618223005,
          5049.052516229697,
          4370.070997875727,
          3775.4591662443445,
          3197.035530563058,
          2734.820241416746,
          2309.626606912347,
          1909.9140845544118,
          1635.2598708546518,
          1500.0439919180494,
          1454.3271717647524,
          1437.9872046715993,
          1436.9654333689468,
          1409.3603449290422,
          1409.6801075863652,
          1416.8282167669142,
          1410.608707199265,
          1397.16491429964,
          1392.0380209736722,
          1367.5322046794322,
          1369.1485593010577,
          1360.8127700625141,
          1362.609063741509,
          1366.994522599579,
          1359.8137685570373,
          1359.1374873254447,
          1354.7068336414914,
          1350.5893785267476,
          1348.209106567701,
          1344.1889377714087,
          1342.1419934015942,
          1340.1339471150382,
          1343.8908692874911,
          1327.222385526588,
          1331.4647655926894,
          1331.2045456370156,
          1325.1879608172655,
          1329.6788656365927,
          1328.3915363294175,
          1315.433479341209,
          1325.23599361269,
          1323.6453911517601,
          1302.817458693384,
          1320.3252766274345,
          1306.3249563856261,
          1315.7717927695994,
          1314.8793588427568,
          1320.9579025094151,
          1303.360172116448,
          1301.2454979137162,
          1294.4702443638616,
          1301.1165009493434,
          1304.7438755559376,
          1297.0760946233645,
          1293.2488524117784,
          1296.20103036939,
          1295.6317986824338,
          1289.513991235611,
          1282.3928964575475,
          1300.342738556771,
          1283.6681119445716,
          1278.2872890413512,
          1283.460816873872,
          1289.2973890807505,
          1283.1687462108514,
          1276.771458110423,
          1284.5786890642232,
          1279.9831657034854,
          1280.564869172544,
          1272.9057235824864,
          1272.4096953593014,
          1276.8248788402582,
          1278.0846616486067,
          1271.7218836112565,
          1263.654945362826,
          1278.7763867207082,
          1266.1845394461145,
          1269.8520030252485,
          1264.422603285143,
          1265.5434853274764,
          1255.3103326706077,
          1266.1242112549269,
          1263.3860371438395,
          1275.2385739543909,
          1259.020867194158,
          1270.157280658227,
          1273.6891783627666,
          1262.1110165727766,
          1254.320121206739,
          1263.9915900237102,
          1250.0373095047123,
          1258.812841072335,
          1243.1023593716711,
          1255.9188230953214,
          1251.2484420675396,
          1243.0856551496781,
          1238.7080120226651,
          1251.8478298703583,
          1239.9548952031141,
          1252.8166397428236,
          1238.4840845230804,
          1245.5574233240802,
          1243.1048831698745,
          1242.7183989407615,
          1233.706038116094,
          1248.0161307093756,
          1251.6805218470367,
          1233.2673311204835,
          1236.5442017409327,
          1236.4576493609948,
          1241.862800850571,
          1225.987338564643,
          1227.9730239785551,
          1230.9703665076072,
          1235.5486057940097,
          1223.6590401401065,
          1228.235409231972,
          1233.9502780960968,
          1218.1072498701703,
          1225.9058830302536,
          1232.0696928469097,
          1230.9438137546292,
          1222.3221979686248,
          1222.579556382728,
          1224.0067791247477,
          1224.4309340760587,
          1214.5125829108808,
          1229.990093383422,
          1236.8966486938305,
          1227.0991567332878,
          1225.3673400940102,
          1226.944019220481,
          1214.8919155135384,
          1208.5334660889798,
          1223.0366890683356,
          1224.8147016716118,
          1214.6615957838035,
          1227.894339711962,
          1216.6234845608346,
          1216.0045367467133,
          1217.631114114471,
          1210.12252660139,
          1203.5657054777969,
          1205.1979846575707,
          1216.9086227952441,
          1216.8534591794134,
          1204.235729498258,
          1212.3525734781526,
          1216.319873761624,
          1213.0164284561736,
          1216.4806239631243,
          1204.547449100082,
          1201.9336742326925,
          1205.5551464814378,
          1201.6692017409327,
          1200.8697101232735,
          1198.196529139826,
          1208.2364752360043,
          1209.42335690471,
          1206.906839031482,
          1198.2396980929714,
          1209.2088480341858,
          1202.3337342373375,
          1198.7055663817723,
          1198.8728601835476,
          1200.4434887151908,
          1192.8446831880076,
          1195.5025719458483,
          1204.3532070590568,
          1191.528095974338,
          1194.6607726230777,
          1195.6986356176449,
          1194.2665971565132,
          1192.5801325389373,
          1183.2577607051771,
          1193.2960385723584,
          1195.8384664629991,
          1190.594120714835,
          1185.6170932853151,
          1180.2692275306422,
          1197.685388724551,
          1194.3497673296274,
          1185.6177346011739,
          1192.9358234485835,
          1191.727900842973,
          1184.1726271264272,
          1191.472524951593,
          1188.1358650655768,
          1185.579150028355,
          1177.4546476111327,
          1182.8630322627896,
          1178.6814519998088,
          1190.2046514296546,
          1184.245804056367,
          1180.3435592452486,
          1192.5994110076824,
          1180.0100653055365,
          1176.9712844215271,
          1186.1881009276462,
          1187.4132575262088,
          1182.1510087359948,
          1180.280372107127,
          1180.3175161180789,
          1166.444803265835,
          1170.757855507327,
          1165.6899139422655,
          1179.7146270488818,
          1178.2797160801679,
          1178.2700199963265,
          1169.4283618609556,
          1181.9555168878537,
          1177.7571403667112,
          1180.7910307277436,
          1170.1979262048121,
          1177.7216166084554,
          1177.6326668331803,
          1170.1477527117381,
          1172.0172345069839,
          1168.885267564809,
          1162.9932725476865,
          1167.098262905429,
          1170.698579857434,
          1170.7652483860381,
          1168.0822944342792,
          1175.8315370206678,
          1168.3722803312048,
          1171.1897926307618,
          1176.3181637491855,
          1166.9361456486083,
          1161.6017523245987,
          1162.3694117645332,
          1177.2153901657982,
          1166.9511421835443,
          1172.2871997956406,
          1164.6106130333399,
          1163.5773386047865,
          1161.053108247459,
          1173.9458010749825,
          1167.1384410257685,
          1166.999944068445,
          1164.4140946147484,
          1159.687471826162,
          1162.7619659992936,
          1153.266146913616,
          1156.7078394058929,
          1160.9772477533384,
          1157.4377067142148,
          1163.8476106480364,
          1156.5976206838952,
          1161.5580028582126,
          1159.8556040137953,
          1155.0194187015145,
          1161.4502324941332,
          1156.097845291312,
          1151.8330259749569,
          1161.700619560075,
          1168.46649499147,
          1163.017965753929,
          1164.710495359223,
          1155.7268810528467,
          1158.4977567652459,
          1155.763547944382,
          1147.5927290104662,
          1152.9554292821492,
          1150.789003117089,
          1148.626591173183,
          1149.5248514790799,
          1148.825494381094,
          1142.0613969122453,
          1153.134496468161,
          1155.058061408882,
          1144.865711396506,
          1165.9516119596499,
          1151.8699845672968,
          1150.843592339105,
          1151.0230597600637,
          1142.4155976681952,
          1149.2515301747435,
          1145.4854130749716,
          1144.262817749978,
          1143.4540029663062,
          1143.2616186361884
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9659.514356271618,
          9238.782413572162,
          8713.730392967027,
          8292.303872490349,
          7549.35921099938,
          6754.944215213462,
          6144.786521420537,
          5420.914002970223,
          4676.80129831703,
          4101.4807321301005,
          3718.885686182512,
          3550.775171745366,
          3460.2327014103857,
          2607.689081063891,
          2305.0632984137665,
          1764.1402996178736,
          1850.918032294121,
          1804.7693093596083,
          1772.9449823447212,
          1648.399629539733,
          1557.8170473945147,
          1573.8335848743843,
          1384.2231148887497,
          1347.0177963437359,
          1431.0257391140794,
          1342.5652520038427,
          1320.0142100662895,
          1327.896872591394,
          1274.34614800728,
          1281.9928582380596,
          1279.3131341438914,
          1245.372298542617,
          1238.7899356931537,
          1233.7826413127257,
          1231.1768086037368,
          1235.8564719931978,
          1221.4816492607342,
          1217.0774690709516,
          1217.6794061287865,
          1210.2334767906323,
          1218.6670346563174,
          1221.176343674217,
          1209.0868521583852,
          1209.5308120449106,
          1205.0465006624645,
          1218.9562762596813,
          1203.7280197311838,
          1199.6113209775103,
          1208.409283668789,
          1203.1354760169409,
          1199.833042105764,
          1229.0978000565142,
          1192.36490945795,
          1191.7232780571283,
          1187.754015273696,
          1190.1513266034658,
          1187.3198309501188,
          1211.100332103704,
          1194.0772568492534,
          1183.563976274838,
          1185.11224124129,
          1199.0674993508512,
          1173.324162916356,
          1185.463521464401,
          1177.3870629799587,
          1177.812487320549,
          1171.6203151386965,
          1192.2241032495033,
          1182.502950566982,
          1177.9385923909979,
          1173.785226745789,
          1176.7013249340864,
          1187.854829955345,
          1175.1752830552796,
          1176.4109271263487,
          1168.552381210679,
          1179.90121043228,
          1183.2035301549654,
          1166.670031975715,
          1176.0050391760765,
          1198.117923887263,
          1172.6443770649391,
          1192.9101588446367,
          1176.8379989633202,
          1168.2599849452326,
          1164.2309579614027,
          1211.8806647527713,
          1172.0454784490457,
          1157.8885189872574,
          1159.4458829284265,
          1164.3989074229717,
          1166.5401471070877,
          1166.0318831158277,
          1175.851252416439,
          1166.9813780232903,
          1162.1940781627936,
          1199.6967673813242,
          1154.3348724016428,
          1172.6833618482274,
          1161.198386165358,
          1154.2477960030062,
          1156.2232852290956,
          1161.4930731767831,
          1164.8633843745693,
          1169.8460313710177,
          1161.4011324463136,
          1154.391132422815,
          1155.461949432783,
          1165.627287735221,
          1155.2306327506203,
          1152.0152061130423,
          1166.3563858561556,
          1151.3006883326336,
          1157.3792079377083,
          1165.3604039447878,
          1162.5573793621227,
          1154.5966079188704,
          1147.6486934388786,
          1169.3205756598015,
          1159.33530291551,
          1153.4026305063087,
          1152.6655483456204,
          1153.9660539766674,
          1146.3539916184423,
          1156.1768335954732,
          1144.6374762125668,
          1164.9917120205894,
          1157.6394415274635,
          1150.3817164618242,
          1153.7172046490798,
          1156.220468677546,
          1154.9051793452038,
          1167.8904474387314,
          1170.5527825716001,
          1157.6740864137382,
          1149.1673249863709,
          1199.949432734093,
          1147.4083402001454,
          1143.2794205363632,
          1141.6902521017537,
          1147.6369339579783,
          1149.2059872024686,
          1143.5263644949525,
          1162.4126324390681,
          1137.4393778560586,
          1156.6369708214013,
          1143.2412098360032,
          1148.2771760532462,
          1147.3774150192846,
          1139.8229666175023,
          1143.8795289383256,
          1136.5427797331656,
          1151.6648995151065,
          1149.91587561253,
          1152.0959316733272,
          1151.4009528778731,
          1140.873828766316,
          1143.463682356353,
          1147.5852743500288,
          1137.435380353176,
          1135.1879845851165,
          1131.9397680551465,
          1154.8893854735202,
          1140.3557845858215,
          1139.9798154645437,
          1135.1486115364792,
          1140.5781092118805,
          1139.9016868907988,
          1145.947423946793,
          1175.658131870402,
          1164.742433133765,
          1137.9537888108691,
          1140.738185077099,
          1127.1544022759956,
          1141.7914400683965,
          1152.2247532059919,
          1139.0946828942751,
          1142.087122416721,
          1161.5011951239237,
          1133.5374509858445,
          1132.1395888587672,
          1157.0348252380015,
          1145.5234415143436,
          1148.7901307315035,
          1135.1070951955553,
          1136.7655092938908,
          1129.4152293276973,
          1129.6283609112206,
          1144.5230018947702,
          1137.391861442289,
          1176.229555731621,
          1196.3318684732649,
          1130.528029101249,
          1138.0904370161247,
          1126.4696867372747,
          1133.6998874564101,
          1130.1193292991466,
          1133.648827429833,
          1124.4045795924874,
          1145.2329454286223,
          1145.3257955547706,
          1134.3451387670443,
          1132.331991816418,
          1144.7656380466165,
          1128.9190796216649,
          1134.119457170675,
          1153.6358036020606,
          1133.1146649501595,
          1127.6236383297746,
          1129.4299600102845,
          1131.8620762370836,
          1117.7781476957277,
          1132.6167199758434,
          1131.8309319562895,
          1129.7553104624758,
          1131.4833314420227,
          1142.3210191958076,
          1128.7920600886523,
          1138.506644326029,
          1130.7048150386318,
          1124.9438520132815,
          1133.092505284247,
          1185.0373783115397,
          1133.2578169059868,
          1138.655098960422,
          1124.4739804938151,
          1118.3848821388735,
          1127.9364102281165,
          1124.2479804914653,
          1123.7469959492337,
          1121.595186669912,
          1130.3853545830486,
          1134.0010335221184,
          1127.672301058925,
          1152.9504172028917,
          1128.1133295689888,
          1122.754451123703,
          1120.8459196302965,
          1125.7526680942717,
          1128.9280233587797,
          1129.4362887207913,
          1123.8282620751454,
          1130.31144356675,
          1124.107988460427,
          1121.6514011380762,
          1126.2403396536072,
          1144.7682526081485,
          1138.051491372686,
          1127.883123831924,
          1126.679676142345,
          1124.1549517025517,
          1121.008155897717,
          1126.1807620222735,
          1114.6116411703633,
          1144.3396327277092,
          1123.9102622384605,
          1116.8947492729142,
          1133.996373921806,
          1122.7469127739937,
          1121.2310112738433,
          1117.2005801020152,
          1118.4368415497481,
          1114.1406445332082,
          1135.2255884587037,
          1154.1728018727597,
          1120.837281228068,
          1134.572750014295,
          1134.56186690865,
          1122.0373316570349,
          1147.3790130951802,
          1130.6277801776807,
          1135.1727150503418,
          1119.188816215128,
          1125.254664348987,
          1119.9347609938184,
          1128.7575982221745,
          1120.1838230571361,
          1127.9053450837412,
          1124.1672921128136,
          1133.003962083644,
          1125.1196005610682,
          1136.953234684594,
          1119.4838485580087,
          1121.1097287028306,
          1124.9857320194615,
          1123.7549126018664,
          1139.9184232520176,
          1135.089722952528,
          1113.126503371657,
          1120.0472710492593
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"39b3b4fb-bd56-4ec9-a3ff-05496e9138cc\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"39b3b4fb-bd56-4ec9-a3ff-05496e9138cc\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '39b3b4fb-bd56-4ec9-a3ff-05496e9138cc',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10101.367019485037, 9783.214746622469, 9404.69025246892, 8958.493435471288, 8440.768356516004, 7839.324987741566, 7171.1443976017335, 6450.560655947651, 5755.981618223005, 5049.052516229697, 4370.070997875727, 3775.4591662443445, 3197.035530563058, 2734.820241416746, 2309.626606912347, 1909.9140845544118, 1635.2598708546518, 1500.0439919180494, 1454.3271717647524, 1437.9872046715993, 1436.9654333689468, 1409.3603449290422, 1409.6801075863652, 1416.8282167669142, 1410.608707199265, 1397.16491429964, 1392.0380209736722, 1367.5322046794322, 1369.1485593010577, 1360.8127700625141, 1362.609063741509, 1366.994522599579, 1359.8137685570373, 1359.1374873254447, 1354.7068336414914, 1350.5893785267476, 1348.209106567701, 1344.1889377714087, 1342.1419934015942, 1340.1339471150382, 1343.8908692874911, 1327.222385526588, 1331.4647655926894, 1331.2045456370156, 1325.1879608172655, 1329.6788656365927, 1328.3915363294175, 1315.433479341209, 1325.23599361269, 1323.6453911517601, 1302.817458693384, 1320.3252766274345, 1306.3249563856261, 1315.7717927695994, 1314.8793588427568, 1320.9579025094151, 1303.360172116448, 1301.2454979137162, 1294.4702443638616, 1301.1165009493434, 1304.7438755559376, 1297.0760946233645, 1293.2488524117784, 1296.20103036939, 1295.6317986824338, 1289.513991235611, 1282.3928964575475, 1300.342738556771, 1283.6681119445716, 1278.2872890413512, 1283.460816873872, 1289.2973890807505, 1283.1687462108514, 1276.771458110423, 1284.5786890642232, 1279.9831657034854, 1280.564869172544, 1272.9057235824864, 1272.4096953593014, 1276.8248788402582, 1278.0846616486067, 1271.7218836112565, 1263.654945362826, 1278.7763867207082, 1266.1845394461145, 1269.8520030252485, 1264.422603285143, 1265.5434853274764, 1255.3103326706077, 1266.1242112549269, 1263.3860371438395, 1275.2385739543909, 1259.020867194158, 1270.157280658227, 1273.6891783627666, 1262.1110165727766, 1254.320121206739, 1263.9915900237102, 1250.0373095047123, 1258.812841072335, 1243.1023593716711, 1255.9188230953214, 1251.2484420675396, 1243.0856551496781, 1238.7080120226651, 1251.8478298703583, 1239.9548952031141, 1252.8166397428236, 1238.4840845230804, 1245.5574233240802, 1243.1048831698745, 1242.7183989407615, 1233.706038116094, 1248.0161307093756, 1251.6805218470367, 1233.2673311204835, 1236.5442017409327, 1236.4576493609948, 1241.862800850571, 1225.987338564643, 1227.9730239785551, 1230.9703665076072, 1235.5486057940097, 1223.6590401401065, 1228.235409231972, 1233.9502780960968, 1218.1072498701703, 1225.9058830302536, 1232.0696928469097, 1230.9438137546292, 1222.3221979686248, 1222.579556382728, 1224.0067791247477, 1224.4309340760587, 1214.5125829108808, 1229.990093383422, 1236.8966486938305, 1227.0991567332878, 1225.3673400940102, 1226.944019220481, 1214.8919155135384, 1208.5334660889798, 1223.0366890683356, 1224.8147016716118, 1214.6615957838035, 1227.894339711962, 1216.6234845608346, 1216.0045367467133, 1217.631114114471, 1210.12252660139, 1203.5657054777969, 1205.1979846575707, 1216.9086227952441, 1216.8534591794134, 1204.235729498258, 1212.3525734781526, 1216.319873761624, 1213.0164284561736, 1216.4806239631243, 1204.547449100082, 1201.9336742326925, 1205.5551464814378, 1201.6692017409327, 1200.8697101232735, 1198.196529139826, 1208.2364752360043, 1209.42335690471, 1206.906839031482, 1198.2396980929714, 1209.2088480341858, 1202.3337342373375, 1198.7055663817723, 1198.8728601835476, 1200.4434887151908, 1192.8446831880076, 1195.5025719458483, 1204.3532070590568, 1191.528095974338, 1194.6607726230777, 1195.6986356176449, 1194.2665971565132, 1192.5801325389373, 1183.2577607051771, 1193.2960385723584, 1195.8384664629991, 1190.594120714835, 1185.6170932853151, 1180.2692275306422, 1197.685388724551, 1194.3497673296274, 1185.6177346011739, 1192.9358234485835, 1191.727900842973, 1184.1726271264272, 1191.472524951593, 1188.1358650655768, 1185.579150028355, 1177.4546476111327, 1182.8630322627896, 1178.6814519998088, 1190.2046514296546, 1184.245804056367, 1180.3435592452486, 1192.5994110076824, 1180.0100653055365, 1176.9712844215271, 1186.1881009276462, 1187.4132575262088, 1182.1510087359948, 1180.280372107127, 1180.3175161180789, 1166.444803265835, 1170.757855507327, 1165.6899139422655, 1179.7146270488818, 1178.2797160801679, 1178.2700199963265, 1169.4283618609556, 1181.9555168878537, 1177.7571403667112, 1180.7910307277436, 1170.1979262048121, 1177.7216166084554, 1177.6326668331803, 1170.1477527117381, 1172.0172345069839, 1168.885267564809, 1162.9932725476865, 1167.098262905429, 1170.698579857434, 1170.7652483860381, 1168.0822944342792, 1175.8315370206678, 1168.3722803312048, 1171.1897926307618, 1176.3181637491855, 1166.9361456486083, 1161.6017523245987, 1162.3694117645332, 1177.2153901657982, 1166.9511421835443, 1172.2871997956406, 1164.6106130333399, 1163.5773386047865, 1161.053108247459, 1173.9458010749825, 1167.1384410257685, 1166.999944068445, 1164.4140946147484, 1159.687471826162, 1162.7619659992936, 1153.266146913616, 1156.7078394058929, 1160.9772477533384, 1157.4377067142148, 1163.8476106480364, 1156.5976206838952, 1161.5580028582126, 1159.8556040137953, 1155.0194187015145, 1161.4502324941332, 1156.097845291312, 1151.8330259749569, 1161.700619560075, 1168.46649499147, 1163.017965753929, 1164.710495359223, 1155.7268810528467, 1158.4977567652459, 1155.763547944382, 1147.5927290104662, 1152.9554292821492, 1150.789003117089, 1148.626591173183, 1149.5248514790799, 1148.825494381094, 1142.0613969122453, 1153.134496468161, 1155.058061408882, 1144.865711396506, 1165.9516119596499, 1151.8699845672968, 1150.843592339105, 1151.0230597600637, 1142.4155976681952, 1149.2515301747435, 1145.4854130749716, 1144.262817749978, 1143.4540029663062, 1143.2616186361884]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9659.514356271618, 9238.782413572162, 8713.730392967027, 8292.303872490349, 7549.35921099938, 6754.944215213462, 6144.786521420537, 5420.914002970223, 4676.80129831703, 4101.4807321301005, 3718.885686182512, 3550.775171745366, 3460.2327014103857, 2607.689081063891, 2305.0632984137665, 1764.1402996178736, 1850.918032294121, 1804.7693093596083, 1772.9449823447212, 1648.399629539733, 1557.8170473945147, 1573.8335848743843, 1384.2231148887497, 1347.0177963437359, 1431.0257391140794, 1342.5652520038427, 1320.0142100662895, 1327.896872591394, 1274.34614800728, 1281.9928582380596, 1279.3131341438914, 1245.372298542617, 1238.7899356931537, 1233.7826413127257, 1231.1768086037368, 1235.8564719931978, 1221.4816492607342, 1217.0774690709516, 1217.6794061287865, 1210.2334767906323, 1218.6670346563174, 1221.176343674217, 1209.0868521583852, 1209.5308120449106, 1205.0465006624645, 1218.9562762596813, 1203.7280197311838, 1199.6113209775103, 1208.409283668789, 1203.1354760169409, 1199.833042105764, 1229.0978000565142, 1192.36490945795, 1191.7232780571283, 1187.754015273696, 1190.1513266034658, 1187.3198309501188, 1211.100332103704, 1194.0772568492534, 1183.563976274838, 1185.11224124129, 1199.0674993508512, 1173.324162916356, 1185.463521464401, 1177.3870629799587, 1177.812487320549, 1171.6203151386965, 1192.2241032495033, 1182.502950566982, 1177.9385923909979, 1173.785226745789, 1176.7013249340864, 1187.854829955345, 1175.1752830552796, 1176.4109271263487, 1168.552381210679, 1179.90121043228, 1183.2035301549654, 1166.670031975715, 1176.0050391760765, 1198.117923887263, 1172.6443770649391, 1192.9101588446367, 1176.8379989633202, 1168.2599849452326, 1164.2309579614027, 1211.8806647527713, 1172.0454784490457, 1157.8885189872574, 1159.4458829284265, 1164.3989074229717, 1166.5401471070877, 1166.0318831158277, 1175.851252416439, 1166.9813780232903, 1162.1940781627936, 1199.6967673813242, 1154.3348724016428, 1172.6833618482274, 1161.198386165358, 1154.2477960030062, 1156.2232852290956, 1161.4930731767831, 1164.8633843745693, 1169.8460313710177, 1161.4011324463136, 1154.391132422815, 1155.461949432783, 1165.627287735221, 1155.2306327506203, 1152.0152061130423, 1166.3563858561556, 1151.3006883326336, 1157.3792079377083, 1165.3604039447878, 1162.5573793621227, 1154.5966079188704, 1147.6486934388786, 1169.3205756598015, 1159.33530291551, 1153.4026305063087, 1152.6655483456204, 1153.9660539766674, 1146.3539916184423, 1156.1768335954732, 1144.6374762125668, 1164.9917120205894, 1157.6394415274635, 1150.3817164618242, 1153.7172046490798, 1156.220468677546, 1154.9051793452038, 1167.8904474387314, 1170.5527825716001, 1157.6740864137382, 1149.1673249863709, 1199.949432734093, 1147.4083402001454, 1143.2794205363632, 1141.6902521017537, 1147.6369339579783, 1149.2059872024686, 1143.5263644949525, 1162.4126324390681, 1137.4393778560586, 1156.6369708214013, 1143.2412098360032, 1148.2771760532462, 1147.3774150192846, 1139.8229666175023, 1143.8795289383256, 1136.5427797331656, 1151.6648995151065, 1149.91587561253, 1152.0959316733272, 1151.4009528778731, 1140.873828766316, 1143.463682356353, 1147.5852743500288, 1137.435380353176, 1135.1879845851165, 1131.9397680551465, 1154.8893854735202, 1140.3557845858215, 1139.9798154645437, 1135.1486115364792, 1140.5781092118805, 1139.9016868907988, 1145.947423946793, 1175.658131870402, 1164.742433133765, 1137.9537888108691, 1140.738185077099, 1127.1544022759956, 1141.7914400683965, 1152.2247532059919, 1139.0946828942751, 1142.087122416721, 1161.5011951239237, 1133.5374509858445, 1132.1395888587672, 1157.0348252380015, 1145.5234415143436, 1148.7901307315035, 1135.1070951955553, 1136.7655092938908, 1129.4152293276973, 1129.6283609112206, 1144.5230018947702, 1137.391861442289, 1176.229555731621, 1196.3318684732649, 1130.528029101249, 1138.0904370161247, 1126.4696867372747, 1133.6998874564101, 1130.1193292991466, 1133.648827429833, 1124.4045795924874, 1145.2329454286223, 1145.3257955547706, 1134.3451387670443, 1132.331991816418, 1144.7656380466165, 1128.9190796216649, 1134.119457170675, 1153.6358036020606, 1133.1146649501595, 1127.6236383297746, 1129.4299600102845, 1131.8620762370836, 1117.7781476957277, 1132.6167199758434, 1131.8309319562895, 1129.7553104624758, 1131.4833314420227, 1142.3210191958076, 1128.7920600886523, 1138.506644326029, 1130.7048150386318, 1124.9438520132815, 1133.092505284247, 1185.0373783115397, 1133.2578169059868, 1138.655098960422, 1124.4739804938151, 1118.3848821388735, 1127.9364102281165, 1124.2479804914653, 1123.7469959492337, 1121.595186669912, 1130.3853545830486, 1134.0010335221184, 1127.672301058925, 1152.9504172028917, 1128.1133295689888, 1122.754451123703, 1120.8459196302965, 1125.7526680942717, 1128.9280233587797, 1129.4362887207913, 1123.8282620751454, 1130.31144356675, 1124.107988460427, 1121.6514011380762, 1126.2403396536072, 1144.7682526081485, 1138.051491372686, 1127.883123831924, 1126.679676142345, 1124.1549517025517, 1121.008155897717, 1126.1807620222735, 1114.6116411703633, 1144.3396327277092, 1123.9102622384605, 1116.8947492729142, 1133.996373921806, 1122.7469127739937, 1121.2310112738433, 1117.2005801020152, 1118.4368415497481, 1114.1406445332082, 1135.2255884587037, 1154.1728018727597, 1120.837281228068, 1134.572750014295, 1134.56186690865, 1122.0373316570349, 1147.3790130951802, 1130.6277801776807, 1135.1727150503418, 1119.188816215128, 1125.254664348987, 1119.9347609938184, 1128.7575982221745, 1120.1838230571361, 1127.9053450837412, 1124.1672921128136, 1133.003962083644, 1125.1196005610682, 1136.953234684594, 1119.4838485580087, 1121.1097287028306, 1124.9857320194615, 1123.7549126018664, 1139.9184232520176, 1135.089722952528, 1113.126503371657, 1120.0472710492593]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('39b3b4fb-bd56-4ec9-a3ff-05496e9138cc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.32% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1120   : Mean absolute error \n",
      "\n",
      "9.53% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a larger network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Larger_network\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Larger_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              45056     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,072,897\n",
      "Trainable params: 1,067,777\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 3s 136us/sample - loss: 11192.2855 - val_loss: 11104.1744\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 11179.7319 - val_loss: 11044.3999\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 11154.4925 - val_loss: 11009.9138\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 11111.3551 - val_loss: 10935.6949\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 11050.1698 - val_loss: 10821.2224\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10966.7383 - val_loss: 10698.1283\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10856.3857 - val_loss: 10556.7407\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10712.9032 - val_loss: 10378.4995\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10531.5224 - val_loss: 10155.6538\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10306.7894 - val_loss: 9924.9278\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 10033.8911 - val_loss: 9584.2042\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 9707.2443 - val_loss: 9203.7441\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 9323.0033 - val_loss: 8668.5781\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 8875.5340 - val_loss: 8193.4802\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 8359.0416 - val_loss: 7591.3727\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 7774.7243 - val_loss: 6768.4821\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 7111.4043 - val_loss: 6280.0840\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 6368.3347 - val_loss: 5665.1585\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 5558.0608 - val_loss: 4735.6696\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 4688.8214 - val_loss: 3684.7988\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 3805.1568 - val_loss: 2845.4639\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 2956.9869 - val_loss: 2512.7667\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 2204.3707 - val_loss: 1473.9373\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1721.8770 - val_loss: 1572.7209\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1522.7319 - val_loss: 2701.7483\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1467.9970 - val_loss: 2535.5735\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1447.3879 - val_loss: 3127.5677\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1436.9643 - val_loss: 2598.3378\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1413.4537 - val_loss: 2058.3800\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1418.9658 - val_loss: 1671.0481\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1408.5084 - val_loss: 1685.3706\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1395.5548 - val_loss: 1652.6853\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1381.1949 - val_loss: 1458.5179\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1378.7597 - val_loss: 1439.5219\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1370.8338 - val_loss: 1455.1600\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1371.8628 - val_loss: 1358.0488\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1357.3602 - val_loss: 1321.2397\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1353.9686 - val_loss: 1311.1273\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1351.2390 - val_loss: 1281.8039\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1347.6519 - val_loss: 1294.3556\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1350.1313 - val_loss: 1286.4568\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1335.9268 - val_loss: 1267.9520\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1329.9227 - val_loss: 1244.7416\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1330.5322 - val_loss: 1270.4763\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1324.1862 - val_loss: 1249.0443\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1328.5986 - val_loss: 1282.8697\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1314.9089 - val_loss: 1248.6822\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1322.2437 - val_loss: 1228.5304\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1320.9515 - val_loss: 1235.3189\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1310.2369 - val_loss: 1217.6341\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1313.6017 - val_loss: 1218.2467\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1311.0221 - val_loss: 1214.4082\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1307.0234 - val_loss: 1219.8941\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1295.0135 - val_loss: 1215.0802\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1297.0504 - val_loss: 1208.5883\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1293.5410 - val_loss: 1206.8047\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1307.2770 - val_loss: 1216.9284\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1296.7745 - val_loss: 1202.6833\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1298.6976 - val_loss: 1203.5602\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1293.0594 - val_loss: 1200.8378\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1289.9130 - val_loss: 1203.1376\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1288.2662 - val_loss: 1204.7569\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1284.4969 - val_loss: 1203.9657\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1282.6730 - val_loss: 1219.5537\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1286.7677 - val_loss: 1192.4883\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1285.3951 - val_loss: 1195.0884\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1271.5121 - val_loss: 1196.7079\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1287.2410 - val_loss: 1207.6706\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1275.3474 - val_loss: 1192.4465\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1278.4826 - val_loss: 1191.9615\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1282.6997 - val_loss: 1188.6751\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1279.2504 - val_loss: 1199.4892\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1271.5428 - val_loss: 1183.5082\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1261.7858 - val_loss: 1194.8201\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1267.9999 - val_loss: 1184.3130\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1259.7683 - val_loss: 1190.8611\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1265.1761 - val_loss: 1190.2496\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1270.1030 - val_loss: 1182.9975\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1257.8939 - val_loss: 1193.7361\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1256.9575 - val_loss: 1181.3362\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1259.5356 - val_loss: 1183.3339\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1262.0804 - val_loss: 1186.9715\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1261.9683 - val_loss: 1179.9371\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1255.4363 - val_loss: 1185.7694\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1250.7649 - val_loss: 1172.0257\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1246.9635 - val_loss: 1170.3511\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1249.2052 - val_loss: 1188.1431\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1250.7605 - val_loss: 1179.0298\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1249.5040 - val_loss: 1170.5697\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1244.1964 - val_loss: 1179.1346\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1246.7849 - val_loss: 1166.7930\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1251.9059 - val_loss: 1182.8101\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1233.4359 - val_loss: 1187.1239\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1238.1955 - val_loss: 1168.7153\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1232.7099 - val_loss: 1163.4529\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1241.5621 - val_loss: 1172.7756\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1235.0322 - val_loss: 1163.9377\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1237.3758 - val_loss: 1168.4776\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1235.1079 - val_loss: 1161.7152\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1237.0415 - val_loss: 1160.8715\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1230.7905 - val_loss: 1163.6157\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1238.8735 - val_loss: 1168.1094\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1234.8617 - val_loss: 1186.2393\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1246.3298 - val_loss: 1169.7013\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1235.2357 - val_loss: 1168.8772\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1225.0968 - val_loss: 1170.9168\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1228.9552 - val_loss: 1160.4617\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1229.4522 - val_loss: 1168.2215\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1227.3840 - val_loss: 1165.7729\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1232.6462 - val_loss: 1158.7883\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1223.8078 - val_loss: 1160.6790\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1221.9520 - val_loss: 1165.7467\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1221.7804 - val_loss: 1162.8644\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1223.7565 - val_loss: 1156.5234\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1215.7840 - val_loss: 1163.1890\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1221.3178 - val_loss: 1153.0092\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1216.4092 - val_loss: 1155.2390\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1213.8908 - val_loss: 1166.7972\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1219.1032 - val_loss: 1165.3635\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1214.9996 - val_loss: 1153.1217\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1214.6596 - val_loss: 1152.3388\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1210.3925 - val_loss: 1151.5595\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1207.7492 - val_loss: 1153.9014\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1213.4886 - val_loss: 1153.0593\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1214.2749 - val_loss: 1146.7779\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1210.7488 - val_loss: 1157.8913\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1213.3048 - val_loss: 1151.1398\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1212.4652 - val_loss: 1146.7214\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1211.0466 - val_loss: 1149.1924\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1200.0024 - val_loss: 1145.3973\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1214.9754 - val_loss: 1152.3718\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1200.1852 - val_loss: 1149.6643\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1211.7450 - val_loss: 1141.0833\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1202.5160 - val_loss: 1141.4134\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1201.2164 - val_loss: 1147.7948\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1202.7911 - val_loss: 1146.8191\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1203.1250 - val_loss: 1152.5223\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1207.2730 - val_loss: 1143.3078\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1211.5331 - val_loss: 1144.1198\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1199.7763 - val_loss: 1149.5452\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1204.6242 - val_loss: 1140.5300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1201.2993 - val_loss: 1151.4612\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1192.4619 - val_loss: 1141.1378\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1201.2617 - val_loss: 1144.8733\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1191.4896 - val_loss: 1148.6613\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1197.8543 - val_loss: 1143.1152\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1200.8862 - val_loss: 1139.4277\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1195.6609 - val_loss: 1138.2745\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1195.0361 - val_loss: 1138.6611\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1195.1459 - val_loss: 1142.0300\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1193.2717 - val_loss: 1143.0690\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1186.1529 - val_loss: 1147.6101\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1194.1172 - val_loss: 1134.7920\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1196.6885 - val_loss: 1142.7879\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1188.6390 - val_loss: 1141.1169\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1192.8902 - val_loss: 1137.1696\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1193.7923 - val_loss: 1142.0492\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1185.9703 - val_loss: 1133.5290\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1182.7661 - val_loss: 1136.4375\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1188.6778 - val_loss: 1136.3646\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1194.2192 - val_loss: 1138.0496\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1185.8668 - val_loss: 1131.1479\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1183.9639 - val_loss: 1133.2864\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1180.9491 - val_loss: 1137.0088\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1187.7545 - val_loss: 1141.2900\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1184.3881 - val_loss: 1133.0146\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1183.5606 - val_loss: 1130.4894\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1181.9404 - val_loss: 1143.5029\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1184.5785 - val_loss: 1152.1543\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1184.8308 - val_loss: 1128.4789\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1175.2402 - val_loss: 1128.8404\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1180.3383 - val_loss: 1139.5570\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1173.1776 - val_loss: 1131.2229\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1173.9163 - val_loss: 1132.7361\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1177.5179 - val_loss: 1131.8939\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1178.8159 - val_loss: 1123.9179\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1175.3032 - val_loss: 1123.6588\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1180.9120 - val_loss: 1123.7053\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1173.2508 - val_loss: 1126.8167\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1177.2341 - val_loss: 1134.9376\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1189.2074 - val_loss: 1124.8021\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1170.6084 - val_loss: 1130.6581\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1175.8257 - val_loss: 1134.3747\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1171.4845 - val_loss: 1118.3072\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1165.4474 - val_loss: 1127.4425\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1178.1145 - val_loss: 1128.5003\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1176.8537 - val_loss: 1129.2317\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1169.8291 - val_loss: 1138.3907\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1174.7425 - val_loss: 1140.6449\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1171.2038 - val_loss: 1121.7703\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1172.6257 - val_loss: 1131.3126\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1175.0998 - val_loss: 1136.3507\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1167.7710 - val_loss: 1127.5508\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1172.2919 - val_loss: 1126.5968\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1163.0944 - val_loss: 1122.3462\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1160.7263 - val_loss: 1124.8876\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1161.3255 - val_loss: 1127.7077\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1160.9391 - val_loss: 1123.4005\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1157.7538 - val_loss: 1120.7820\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1161.5314 - val_loss: 1119.3021\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1164.9243 - val_loss: 1123.7026\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1164.1502 - val_loss: 1122.6385\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1169.5864 - val_loss: 1124.3014\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1164.4012 - val_loss: 1116.9602\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1170.4506 - val_loss: 1117.2895\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1163.2291 - val_loss: 1119.3386\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1163.4056 - val_loss: 1118.6519\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1163.8960 - val_loss: 1121.9935\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1154.6953 - val_loss: 1121.5020\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1155.8843 - val_loss: 1122.2681\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1158.2557 - val_loss: 1111.3457\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1162.0059 - val_loss: 1126.3242\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1151.5496 - val_loss: 1123.0265\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1161.5447 - val_loss: 1120.1518\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1148.8807 - val_loss: 1118.6170\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1155.8507 - val_loss: 1117.2882\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.5685 - val_loss: 1117.5153\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1160.9653 - val_loss: 1112.6557\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.9704 - val_loss: 1115.1119\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1146.8393 - val_loss: 1115.8488\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1151.3216 - val_loss: 1113.9195\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1151.1325 - val_loss: 1116.3069\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.0777 - val_loss: 1117.8269\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1147.3040 - val_loss: 1111.5592\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1145.9628 - val_loss: 1114.8006\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1151.9583 - val_loss: 1113.1252\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1149.5303 - val_loss: 1117.1571\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1157.7434 - val_loss: 1116.1434\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1152.5392 - val_loss: 1114.2400\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.4681 - val_loss: 1112.0242\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1148.8102 - val_loss: 1116.6471\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.8393 - val_loss: 1115.0992\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1149.3911 - val_loss: 1113.3792\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1143.7188 - val_loss: 1114.5682\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1143.9090 - val_loss: 1111.1639\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1144.9478 - val_loss: 1109.0668\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1142.4741 - val_loss: 1114.2940\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1147.0754 - val_loss: 1113.0639\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.8798 - val_loss: 1107.9412\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1137.2942 - val_loss: 1115.9012\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1138.8756 - val_loss: 1111.6623\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1141.1133 - val_loss: 1121.1981\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1142.4779 - val_loss: 1123.8738\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1143.7746 - val_loss: 1112.0703\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1142.0901 - val_loss: 1111.8992\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1143.6271 - val_loss: 1113.5426\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1136.1207 - val_loss: 1114.8846\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1139.2588 - val_loss: 1119.3270\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1142.1844 - val_loss: 1113.9402\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1144.2783 - val_loss: 1104.5411\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.5742 - val_loss: 1112.4250\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1136.4643 - val_loss: 1107.9488\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1132.0104 - val_loss: 1110.3063\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1132.9237 - val_loss: 1107.7609\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1129.2106 - val_loss: 1110.2808\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1139.5778 - val_loss: 1111.7380\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1136.2965 - val_loss: 1107.0927\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1141.2523 - val_loss: 1107.3283\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1132.5269 - val_loss: 1107.3335\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1138.2910 - val_loss: 1111.6608\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1132.2540 - val_loss: 1113.4292\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1128.8056 - val_loss: 1104.2828\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1126.6746 - val_loss: 1107.5988\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1127.0425 - val_loss: 1104.6329\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1133.9745 - val_loss: 1106.6976\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1133.4767 - val_loss: 1105.2391\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1139.9965 - val_loss: 1105.8119\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1128.5899 - val_loss: 1107.2341\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1129.1997 - val_loss: 1103.4232\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1129.7527 - val_loss: 1103.3163\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1131.9880 - val_loss: 1114.3475\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1132.6329 - val_loss: 1106.2175\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1130.3354 - val_loss: 1113.4822\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1127.3121 - val_loss: 1106.6553\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1118.2172 - val_loss: 1110.2954\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1127.3436 - val_loss: 1111.2880\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1129.6466 - val_loss: 1105.7614\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1127.7371 - val_loss: 1102.8077\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1129.4767 - val_loss: 1103.3605\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1123.4519 - val_loss: 1101.4320\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1127.9434 - val_loss: 1107.1158\n",
      "Epoch 282/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1122.9493 - val_loss: 1106.0089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1127.1820 - val_loss: 1105.6695\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1129.3647 - val_loss: 1109.6902\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1115.6847 - val_loss: 1101.0518\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1116.4707 - val_loss: 1107.8980\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1122.0188 - val_loss: 1101.3935\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.7177 - val_loss: 1100.3120\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1124.8317 - val_loss: 1103.8309\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1128.2318 - val_loss: 1098.2733\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1117.4978 - val_loss: 1096.5429\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1116.8962 - val_loss: 1099.8657\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1112.0092 - val_loss: 1099.3719\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1119.4787 - val_loss: 1098.1788\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1117.8050 - val_loss: 1105.6232\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1116.4243 - val_loss: 1094.5244\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1116.8054 - val_loss: 1103.4719\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1124.8114 - val_loss: 1092.7810\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1123.3649 - val_loss: 1099.9774\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1113.7759 - val_loss: 1098.2656\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10033.891055611779,
          9707.244335271707,
          9323.003286670342,
          8875.533958800694,
          8359.041556484359,
          7774.724291850217,
          7111.4042756283525,
          6368.334713418952,
          5558.060778727818,
          4688.821403128603,
          3805.1568089239054,
          2956.9868945880385,
          2204.370688497124,
          1721.876974910157,
          1522.7318789350359,
          1467.997024588148,
          1447.3879116464527,
          1436.964324896097,
          1413.4537125628196,
          1418.965775249046,
          1408.5084149942193,
          1395.554775815558,
          1381.1948601275426,
          1378.759721565132,
          1370.8337894590388,
          1371.8627520175503,
          1357.3602144139215,
          1353.9686407070492,
          1351.2390466922984,
          1347.65185845503,
          1350.1312690828186,
          1335.9268208915446,
          1329.922676595868,
          1330.5322069803365,
          1324.1861625871798,
          1328.5986452716515,
          1314.9088831645872,
          1322.2437072961966,
          1320.951476818243,
          1310.2368926542997,
          1313.6016660896455,
          1311.0220678745206,
          1307.023449665419,
          1295.0134762247076,
          1297.0503946491347,
          1293.5409678445208,
          1307.2770072794735,
          1296.7745456056841,
          1298.6975898566743,
          1293.0593839637354,
          1289.9130022971835,
          1288.266164488608,
          1284.496851188089,
          1282.6729509175027,
          1286.7677250157049,
          1285.3950586221442,
          1271.5120581088956,
          1287.2410051779157,
          1275.347427505851,
          1278.4825552050565,
          1282.6997367227057,
          1279.2504131102194,
          1271.5427736871725,
          1261.7858179313089,
          1267.9998724956372,
          1259.7682802434767,
          1265.1760665131685,
          1270.1030272947946,
          1257.8939185730555,
          1256.9574802200566,
          1259.5356093812663,
          1262.0804412517468,
          1261.968274324768,
          1255.4362860771992,
          1250.7648682472866,
          1246.9634672952411,
          1249.205221613304,
          1250.760523650554,
          1249.503976011458,
          1244.196424840327,
          1246.7849476265242,
          1251.905890544909,
          1233.4358918637677,
          1238.1954856797595,
          1232.7098602362234,
          1241.5621047095494,
          1235.0321542308832,
          1237.3757990501867,
          1235.1079242064525,
          1237.0414537759111,
          1230.7904790981943,
          1238.8734720037723,
          1234.8617337695196,
          1246.3297564634847,
          1235.2356659051816,
          1225.0968057966338,
          1228.9551783367224,
          1229.4521961935015,
          1227.3840479142254,
          1232.6462242993894,
          1223.8078030711881,
          1221.9520417636636,
          1221.7803610676822,
          1223.7565424743004,
          1215.783952950523,
          1221.3178191275833,
          1216.4091715364243,
          1213.8908278956928,
          1219.1032231653862,
          1214.9995792037814,
          1214.6595849893317,
          1210.39253323289,
          1207.7492136047865,
          1213.4885603937425,
          1214.2749451797329,
          1210.7488287907936,
          1213.3048324080107,
          1212.4651521201217,
          1211.0465509641474,
          1200.0023849361855,
          1214.9754055612952,
          1200.1851953262076,
          1211.7450305119482,
          1202.5159767690918,
          1201.2163924054103,
          1202.7910917017057,
          1203.1249708470539,
          1207.272993866083,
          1211.5331248942564,
          1199.7762524604989,
          1204.6242478735728,
          1201.299300569175,
          1192.4619130344365,
          1201.2617019582947,
          1191.4895885062883,
          1197.8542709335288,
          1200.886220946923,
          1195.6609470267226,
          1195.0360689012134,
          1195.1458606830886,
          1193.2716999746215,
          1186.152922500846,
          1194.117214596819,
          1196.6884729397998,
          1188.6389841370767,
          1192.890216418156,
          1193.7923296126569,
          1185.970298851232,
          1182.7660849605459,
          1188.677816473221,
          1194.2191983336363,
          1185.8668064555736,
          1183.9639463728743,
          1180.94914037239,
          1187.7545324141597,
          1184.3881218120237,
          1183.5606479189644,
          1181.9403609541057,
          1184.5784603690297,
          1184.8307992626533,
          1175.2402185624028,
          1180.338290174571,
          1173.177629732813,
          1173.916271686267,
          1177.5179009369674,
          1178.8158882528166,
          1175.3032080430603,
          1180.91199538233,
          1173.250772761132,
          1177.23405157609,
          1189.2073698745644,
          1170.6084099420211,
          1175.8257024905379,
          1171.4845061270632,
          1165.4473663997096,
          1178.114525157989,
          1176.8537146042602,
          1169.8290799731647,
          1174.742503556121,
          1171.2037557170127,
          1172.6256755356897,
          1175.099799982885,
          1167.7710351601663,
          1172.2919162331827,
          1163.0943636315749,
          1160.7262748380163,
          1161.3255415006752,
          1160.9390758990955,
          1157.7537846937194,
          1161.5314014190803,
          1164.9243446045657,
          1164.150230320513,
          1169.586398439654,
          1164.4012185099227,
          1170.4506249206922,
          1163.2291037997622,
          1163.4055786010424,
          1163.8960487208146,
          1154.6953150946367,
          1155.8843426747435,
          1158.2556790967062,
          1162.005908012199,
          1151.5495533259523,
          1161.5447250987334,
          1148.8807112848876,
          1155.8507365243383,
          1153.5684602476204,
          1160.9653116657998,
          1143.9704481652295,
          1146.8393202942887,
          1151.3216179938936,
          1151.1325260619017,
          1150.0777010216798,
          1147.3039948299172,
          1145.9628262975925,
          1151.9583288131173,
          1149.5302841832122,
          1157.7434492522355,
          1152.539161659181,
          1153.468140186947,
          1148.810246558043,
          1146.839344282439,
          1149.3910717915408,
          1143.7187551158402,
          1143.9089536603763,
          1144.9477687397389,
          1142.4741256710809,
          1147.0754199688722,
          1150.8798380917337,
          1137.2942158156127,
          1138.8755993121179,
          1141.1133398741024,
          1142.4778548004108,
          1143.774626137332,
          1142.0900821628422,
          1143.6271357531534,
          1136.1207106984018,
          1139.258832314604,
          1142.184368591737,
          1144.278266706327,
          1143.5741627205343,
          1136.4643410758597,
          1132.0104011641204,
          1132.9237409354164,
          1129.210630647496,
          1139.5778170949548,
          1136.296507760999,
          1141.2522868050683,
          1132.5268759810665,
          1138.2910299934126,
          1132.2540426642474,
          1128.8055874962402,
          1126.6746315566959,
          1127.0424596137457,
          1133.9744835595986,
          1133.4766566657293,
          1139.9965454370817,
          1128.5898589261767,
          1129.1996544041854,
          1129.7527288234567,
          1131.9879758172028,
          1132.6329242896766,
          1130.3353667533634,
          1127.312071787037,
          1118.217223668256,
          1127.3435866602777,
          1129.6465542466076,
          1127.737143795548,
          1129.4767205035828,
          1123.4518559411697,
          1127.9434474702587,
          1122.9492895639992,
          1127.1820453883222,
          1129.3646685973374,
          1115.684741803298,
          1116.4707027578345,
          1122.0188215679125,
          1123.7177477359103,
          1124.8316848415254,
          1128.2317540008319,
          1117.49778359281,
          1116.8962043990155,
          1112.009188538638,
          1119.4787107612606,
          1117.804960144912,
          1116.4243097238366,
          1116.8054228591996,
          1124.8114246210068,
          1123.364862974789,
          1113.7759292324106
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9584.204166966927,
          9203.744058771556,
          8668.578145757094,
          8193.480184808628,
          7591.372741295337,
          6768.482136562876,
          6280.083987312324,
          5665.158536119692,
          4735.66958075371,
          3684.7988393357887,
          2845.463868117653,
          2512.7666564415135,
          1473.9373290966669,
          1572.720895250503,
          2701.748262034023,
          2535.57353841668,
          3127.5676564732366,
          2598.33776255765,
          2058.380043435197,
          1671.0481354059618,
          1685.3705905373502,
          1652.6853359750978,
          1458.5179420839886,
          1439.521888501902,
          1455.1599651525607,
          1358.0488054586456,
          1321.239668548382,
          1311.1272854587944,
          1281.8038642658412,
          1294.355589376128,
          1286.456813447385,
          1267.952049939217,
          1244.7416032171927,
          1270.4762978225808,
          1249.0442554450165,
          1282.869666601915,
          1248.6821560275466,
          1228.5303884827113,
          1235.318900503614,
          1217.634064265332,
          1218.2466834179022,
          1214.4081809481997,
          1219.8941217624808,
          1215.0802152980561,
          1208.5883068683656,
          1206.8046950146552,
          1216.9283994342322,
          1202.6832835930059,
          1203.5601732962734,
          1200.8377510531288,
          1203.1376489517277,
          1204.7568655800003,
          1203.9657107258167,
          1219.5536848932225,
          1192.4882600767855,
          1195.0884469787463,
          1196.7078805773917,
          1207.670601278167,
          1192.4464928883456,
          1191.96151524726,
          1188.675124121544,
          1199.4891947824108,
          1183.508249427026,
          1194.8200762901513,
          1184.3130245816858,
          1190.861067241625,
          1190.2496043668616,
          1182.9975436133948,
          1193.7361127946333,
          1181.3361693038619,
          1183.3339148093637,
          1186.9715260409487,
          1179.937062999541,
          1185.769396720536,
          1172.02567064994,
          1170.3511327204637,
          1188.1430622450403,
          1179.0298294364175,
          1170.5696591411106,
          1179.1345783705603,
          1166.7930316087452,
          1182.8101366531498,
          1187.1238682978087,
          1168.7153020950173,
          1163.4529263144723,
          1172.775630800237,
          1163.937746294663,
          1168.4776332771849,
          1161.715188126825,
          1160.8714862499921,
          1163.615725299842,
          1168.109402341596,
          1186.2393450265613,
          1169.7012627607169,
          1168.8772278138003,
          1170.9168410131967,
          1160.4617205907234,
          1168.221478945845,
          1165.772935633624,
          1158.7883089293882,
          1160.6790426348741,
          1165.7467497769592,
          1162.864404189173,
          1156.5234089589965,
          1163.1889869226395,
          1153.0092046449674,
          1155.2389537504544,
          1166.7972266467034,
          1165.3634516882566,
          1153.1216559539178,
          1152.338758482014,
          1151.559480919923,
          1153.901426668322,
          1153.0593291336772,
          1146.7778995897145,
          1157.8912895207227,
          1151.1398038954014,
          1146.721419318157,
          1149.1923977438998,
          1145.3972826908555,
          1152.3717818672299,
          1149.6642728858706,
          1141.083264877353,
          1141.4134429692983,
          1147.7947875658351,
          1146.8191292232852,
          1152.5223493926005,
          1143.3077908812913,
          1144.1197516374605,
          1149.5452401341142,
          1140.5299573324237,
          1151.4612317220085,
          1141.1377549157105,
          1144.8733260432202,
          1148.6613111325776,
          1143.1151759956747,
          1139.4277065683277,
          1138.2744894048747,
          1138.6611472788234,
          1142.0300372031736,
          1143.068975234046,
          1147.610120027893,
          1134.7920074860654,
          1142.7878937630417,
          1141.1169039747483,
          1137.1696217783424,
          1142.0492254226222,
          1133.5290030446347,
          1136.4375407798557,
          1136.3645657665006,
          1138.0495687713837,
          1131.1479418020056,
          1133.2863964129,
          1137.008792464901,
          1141.2899989924977,
          1133.0146390870168,
          1130.489381326997,
          1143.5028855541987,
          1152.1543153801447,
          1128.4789374248046,
          1128.8404494752763,
          1139.5569943278306,
          1131.2228601492789,
          1132.7360908136545,
          1131.893894021918,
          1123.9178799938434,
          1123.6588113470023,
          1123.705300553901,
          1126.8166668641202,
          1134.9375828570298,
          1124.802063514552,
          1130.6580638958167,
          1134.374707980985,
          1118.307157153521,
          1127.4425448059485,
          1128.5003020303961,
          1129.2316988280859,
          1138.3907462625482,
          1140.6449096508343,
          1121.7703458680069,
          1131.312636854846,
          1136.350688082961,
          1127.5507526845188,
          1126.5967689332108,
          1122.3461945883516,
          1124.887570965761,
          1127.7076679885279,
          1123.4004612431777,
          1120.7819512128017,
          1119.3020901952498,
          1123.702581472571,
          1122.6384633983905,
          1124.301422957502,
          1116.9602241952123,
          1117.2895384689646,
          1119.33864399979,
          1118.6518824921359,
          1121.9934590188395,
          1121.5020060213194,
          1122.2681482352164,
          1111.3456892706192,
          1126.324169941457,
          1123.0265237958536,
          1120.1517762344204,
          1118.6169741768051,
          1117.2881773372487,
          1117.5153191266043,
          1112.6556899256427,
          1115.11188283874,
          1115.8488229063535,
          1113.919522814983,
          1116.3068702258686,
          1117.8268907607044,
          1111.5592437799216,
          1114.8006092451703,
          1113.1251532549081,
          1117.1571091184737,
          1116.1434399115435,
          1114.2399567978307,
          1112.0242424248358,
          1116.6471246383173,
          1115.0992238511537,
          1113.3792364052788,
          1114.5681990215576,
          1111.1639206272246,
          1109.0668305220527,
          1114.2940234698106,
          1113.0638741783325,
          1107.9411576544328,
          1115.9012226711325,
          1111.662265371411,
          1121.1981345345555,
          1123.8738238462972,
          1112.0702850360155,
          1111.8992418128933,
          1113.5426046196674,
          1114.884571786058,
          1119.3269951825919,
          1113.9402144305664,
          1104.5410602478553,
          1112.4249647716877,
          1107.9487948695905,
          1110.3062971048946,
          1107.760873999939,
          1110.2807856089896,
          1111.7379878602335,
          1107.09265976304,
          1107.3283469883024,
          1107.33352062041,
          1111.6608130847235,
          1113.4292154229904,
          1104.2828437188643,
          1107.5987942184836,
          1104.6329469805087,
          1106.6976113725766,
          1105.239116160024,
          1105.8119302569494,
          1107.2340551008795,
          1103.4231724407286,
          1103.3163298061288,
          1114.3475263957775,
          1106.2175076096287,
          1113.4821563163835,
          1106.6552735843661,
          1110.2953960326145,
          1111.2879830821855,
          1105.7613947875805,
          1102.8076578449673,
          1103.3604975230521,
          1101.4319952579833,
          1107.115787350823,
          1106.0089397227714,
          1105.669504921781,
          1109.690186966582,
          1101.0517764155554,
          1107.897964370449,
          1101.39352088379,
          1100.3120076064956,
          1103.830880969231,
          1098.2732695095137,
          1096.542899747351,
          1099.8656938704694,
          1099.3719067524783,
          1098.1788169014828,
          1105.623158811543,
          1094.524385888662,
          1103.4719082113495,
          1092.7810004253247,
          1099.9773546719518,
          1098.265636259744
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"449ad5ea-6fd3-4ff2-924f-8358cc148909\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"449ad5ea-6fd3-4ff2-924f-8358cc148909\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '449ad5ea-6fd3-4ff2-924f-8358cc148909',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10033.891055611779, 9707.244335271707, 9323.003286670342, 8875.533958800694, 8359.041556484359, 7774.724291850217, 7111.4042756283525, 6368.334713418952, 5558.060778727818, 4688.821403128603, 3805.1568089239054, 2956.9868945880385, 2204.370688497124, 1721.876974910157, 1522.7318789350359, 1467.997024588148, 1447.3879116464527, 1436.964324896097, 1413.4537125628196, 1418.965775249046, 1408.5084149942193, 1395.554775815558, 1381.1948601275426, 1378.759721565132, 1370.8337894590388, 1371.8627520175503, 1357.3602144139215, 1353.9686407070492, 1351.2390466922984, 1347.65185845503, 1350.1312690828186, 1335.9268208915446, 1329.922676595868, 1330.5322069803365, 1324.1861625871798, 1328.5986452716515, 1314.9088831645872, 1322.2437072961966, 1320.951476818243, 1310.2368926542997, 1313.6016660896455, 1311.0220678745206, 1307.023449665419, 1295.0134762247076, 1297.0503946491347, 1293.5409678445208, 1307.2770072794735, 1296.7745456056841, 1298.6975898566743, 1293.0593839637354, 1289.9130022971835, 1288.266164488608, 1284.496851188089, 1282.6729509175027, 1286.7677250157049, 1285.3950586221442, 1271.5120581088956, 1287.2410051779157, 1275.347427505851, 1278.4825552050565, 1282.6997367227057, 1279.2504131102194, 1271.5427736871725, 1261.7858179313089, 1267.9998724956372, 1259.7682802434767, 1265.1760665131685, 1270.1030272947946, 1257.8939185730555, 1256.9574802200566, 1259.5356093812663, 1262.0804412517468, 1261.968274324768, 1255.4362860771992, 1250.7648682472866, 1246.9634672952411, 1249.205221613304, 1250.760523650554, 1249.503976011458, 1244.196424840327, 1246.7849476265242, 1251.905890544909, 1233.4358918637677, 1238.1954856797595, 1232.7098602362234, 1241.5621047095494, 1235.0321542308832, 1237.3757990501867, 1235.1079242064525, 1237.0414537759111, 1230.7904790981943, 1238.8734720037723, 1234.8617337695196, 1246.3297564634847, 1235.2356659051816, 1225.0968057966338, 1228.9551783367224, 1229.4521961935015, 1227.3840479142254, 1232.6462242993894, 1223.8078030711881, 1221.9520417636636, 1221.7803610676822, 1223.7565424743004, 1215.783952950523, 1221.3178191275833, 1216.4091715364243, 1213.8908278956928, 1219.1032231653862, 1214.9995792037814, 1214.6595849893317, 1210.39253323289, 1207.7492136047865, 1213.4885603937425, 1214.2749451797329, 1210.7488287907936, 1213.3048324080107, 1212.4651521201217, 1211.0465509641474, 1200.0023849361855, 1214.9754055612952, 1200.1851953262076, 1211.7450305119482, 1202.5159767690918, 1201.2163924054103, 1202.7910917017057, 1203.1249708470539, 1207.272993866083, 1211.5331248942564, 1199.7762524604989, 1204.6242478735728, 1201.299300569175, 1192.4619130344365, 1201.2617019582947, 1191.4895885062883, 1197.8542709335288, 1200.886220946923, 1195.6609470267226, 1195.0360689012134, 1195.1458606830886, 1193.2716999746215, 1186.152922500846, 1194.117214596819, 1196.6884729397998, 1188.6389841370767, 1192.890216418156, 1193.7923296126569, 1185.970298851232, 1182.7660849605459, 1188.677816473221, 1194.2191983336363, 1185.8668064555736, 1183.9639463728743, 1180.94914037239, 1187.7545324141597, 1184.3881218120237, 1183.5606479189644, 1181.9403609541057, 1184.5784603690297, 1184.8307992626533, 1175.2402185624028, 1180.338290174571, 1173.177629732813, 1173.916271686267, 1177.5179009369674, 1178.8158882528166, 1175.3032080430603, 1180.91199538233, 1173.250772761132, 1177.23405157609, 1189.2073698745644, 1170.6084099420211, 1175.8257024905379, 1171.4845061270632, 1165.4473663997096, 1178.114525157989, 1176.8537146042602, 1169.8290799731647, 1174.742503556121, 1171.2037557170127, 1172.6256755356897, 1175.099799982885, 1167.7710351601663, 1172.2919162331827, 1163.0943636315749, 1160.7262748380163, 1161.3255415006752, 1160.9390758990955, 1157.7537846937194, 1161.5314014190803, 1164.9243446045657, 1164.150230320513, 1169.586398439654, 1164.4012185099227, 1170.4506249206922, 1163.2291037997622, 1163.4055786010424, 1163.8960487208146, 1154.6953150946367, 1155.8843426747435, 1158.2556790967062, 1162.005908012199, 1151.5495533259523, 1161.5447250987334, 1148.8807112848876, 1155.8507365243383, 1153.5684602476204, 1160.9653116657998, 1143.9704481652295, 1146.8393202942887, 1151.3216179938936, 1151.1325260619017, 1150.0777010216798, 1147.3039948299172, 1145.9628262975925, 1151.9583288131173, 1149.5302841832122, 1157.7434492522355, 1152.539161659181, 1153.468140186947, 1148.810246558043, 1146.839344282439, 1149.3910717915408, 1143.7187551158402, 1143.9089536603763, 1144.9477687397389, 1142.4741256710809, 1147.0754199688722, 1150.8798380917337, 1137.2942158156127, 1138.8755993121179, 1141.1133398741024, 1142.4778548004108, 1143.774626137332, 1142.0900821628422, 1143.6271357531534, 1136.1207106984018, 1139.258832314604, 1142.184368591737, 1144.278266706327, 1143.5741627205343, 1136.4643410758597, 1132.0104011641204, 1132.9237409354164, 1129.210630647496, 1139.5778170949548, 1136.296507760999, 1141.2522868050683, 1132.5268759810665, 1138.2910299934126, 1132.2540426642474, 1128.8055874962402, 1126.6746315566959, 1127.0424596137457, 1133.9744835595986, 1133.4766566657293, 1139.9965454370817, 1128.5898589261767, 1129.1996544041854, 1129.7527288234567, 1131.9879758172028, 1132.6329242896766, 1130.3353667533634, 1127.312071787037, 1118.217223668256, 1127.3435866602777, 1129.6465542466076, 1127.737143795548, 1129.4767205035828, 1123.4518559411697, 1127.9434474702587, 1122.9492895639992, 1127.1820453883222, 1129.3646685973374, 1115.684741803298, 1116.4707027578345, 1122.0188215679125, 1123.7177477359103, 1124.8316848415254, 1128.2317540008319, 1117.49778359281, 1116.8962043990155, 1112.009188538638, 1119.4787107612606, 1117.804960144912, 1116.4243097238366, 1116.8054228591996, 1124.8114246210068, 1123.364862974789, 1113.7759292324106]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9584.204166966927, 9203.744058771556, 8668.578145757094, 8193.480184808628, 7591.372741295337, 6768.482136562876, 6280.083987312324, 5665.158536119692, 4735.66958075371, 3684.7988393357887, 2845.463868117653, 2512.7666564415135, 1473.9373290966669, 1572.720895250503, 2701.748262034023, 2535.57353841668, 3127.5676564732366, 2598.33776255765, 2058.380043435197, 1671.0481354059618, 1685.3705905373502, 1652.6853359750978, 1458.5179420839886, 1439.521888501902, 1455.1599651525607, 1358.0488054586456, 1321.239668548382, 1311.1272854587944, 1281.8038642658412, 1294.355589376128, 1286.456813447385, 1267.952049939217, 1244.7416032171927, 1270.4762978225808, 1249.0442554450165, 1282.869666601915, 1248.6821560275466, 1228.5303884827113, 1235.318900503614, 1217.634064265332, 1218.2466834179022, 1214.4081809481997, 1219.8941217624808, 1215.0802152980561, 1208.5883068683656, 1206.8046950146552, 1216.9283994342322, 1202.6832835930059, 1203.5601732962734, 1200.8377510531288, 1203.1376489517277, 1204.7568655800003, 1203.9657107258167, 1219.5536848932225, 1192.4882600767855, 1195.0884469787463, 1196.7078805773917, 1207.670601278167, 1192.4464928883456, 1191.96151524726, 1188.675124121544, 1199.4891947824108, 1183.508249427026, 1194.8200762901513, 1184.3130245816858, 1190.861067241625, 1190.2496043668616, 1182.9975436133948, 1193.7361127946333, 1181.3361693038619, 1183.3339148093637, 1186.9715260409487, 1179.937062999541, 1185.769396720536, 1172.02567064994, 1170.3511327204637, 1188.1430622450403, 1179.0298294364175, 1170.5696591411106, 1179.1345783705603, 1166.7930316087452, 1182.8101366531498, 1187.1238682978087, 1168.7153020950173, 1163.4529263144723, 1172.775630800237, 1163.937746294663, 1168.4776332771849, 1161.715188126825, 1160.8714862499921, 1163.615725299842, 1168.109402341596, 1186.2393450265613, 1169.7012627607169, 1168.8772278138003, 1170.9168410131967, 1160.4617205907234, 1168.221478945845, 1165.772935633624, 1158.7883089293882, 1160.6790426348741, 1165.7467497769592, 1162.864404189173, 1156.5234089589965, 1163.1889869226395, 1153.0092046449674, 1155.2389537504544, 1166.7972266467034, 1165.3634516882566, 1153.1216559539178, 1152.338758482014, 1151.559480919923, 1153.901426668322, 1153.0593291336772, 1146.7778995897145, 1157.8912895207227, 1151.1398038954014, 1146.721419318157, 1149.1923977438998, 1145.3972826908555, 1152.3717818672299, 1149.6642728858706, 1141.083264877353, 1141.4134429692983, 1147.7947875658351, 1146.8191292232852, 1152.5223493926005, 1143.3077908812913, 1144.1197516374605, 1149.5452401341142, 1140.5299573324237, 1151.4612317220085, 1141.1377549157105, 1144.8733260432202, 1148.6613111325776, 1143.1151759956747, 1139.4277065683277, 1138.2744894048747, 1138.6611472788234, 1142.0300372031736, 1143.068975234046, 1147.610120027893, 1134.7920074860654, 1142.7878937630417, 1141.1169039747483, 1137.1696217783424, 1142.0492254226222, 1133.5290030446347, 1136.4375407798557, 1136.3645657665006, 1138.0495687713837, 1131.1479418020056, 1133.2863964129, 1137.008792464901, 1141.2899989924977, 1133.0146390870168, 1130.489381326997, 1143.5028855541987, 1152.1543153801447, 1128.4789374248046, 1128.8404494752763, 1139.5569943278306, 1131.2228601492789, 1132.7360908136545, 1131.893894021918, 1123.9178799938434, 1123.6588113470023, 1123.705300553901, 1126.8166668641202, 1134.9375828570298, 1124.802063514552, 1130.6580638958167, 1134.374707980985, 1118.307157153521, 1127.4425448059485, 1128.5003020303961, 1129.2316988280859, 1138.3907462625482, 1140.6449096508343, 1121.7703458680069, 1131.312636854846, 1136.350688082961, 1127.5507526845188, 1126.5967689332108, 1122.3461945883516, 1124.887570965761, 1127.7076679885279, 1123.4004612431777, 1120.7819512128017, 1119.3020901952498, 1123.702581472571, 1122.6384633983905, 1124.301422957502, 1116.9602241952123, 1117.2895384689646, 1119.33864399979, 1118.6518824921359, 1121.9934590188395, 1121.5020060213194, 1122.2681482352164, 1111.3456892706192, 1126.324169941457, 1123.0265237958536, 1120.1517762344204, 1118.6169741768051, 1117.2881773372487, 1117.5153191266043, 1112.6556899256427, 1115.11188283874, 1115.8488229063535, 1113.919522814983, 1116.3068702258686, 1117.8268907607044, 1111.5592437799216, 1114.8006092451703, 1113.1251532549081, 1117.1571091184737, 1116.1434399115435, 1114.2399567978307, 1112.0242424248358, 1116.6471246383173, 1115.0992238511537, 1113.3792364052788, 1114.5681990215576, 1111.1639206272246, 1109.0668305220527, 1114.2940234698106, 1113.0638741783325, 1107.9411576544328, 1115.9012226711325, 1111.662265371411, 1121.1981345345555, 1123.8738238462972, 1112.0702850360155, 1111.8992418128933, 1113.5426046196674, 1114.884571786058, 1119.3269951825919, 1113.9402144305664, 1104.5410602478553, 1112.4249647716877, 1107.9487948695905, 1110.3062971048946, 1107.760873999939, 1110.2807856089896, 1111.7379878602335, 1107.09265976304, 1107.3283469883024, 1107.33352062041, 1111.6608130847235, 1113.4292154229904, 1104.2828437188643, 1107.5987942184836, 1104.6329469805087, 1106.6976113725766, 1105.239116160024, 1105.8119302569494, 1107.2340551008795, 1103.4231724407286, 1103.3163298061288, 1114.3475263957775, 1106.2175076096287, 1113.4821563163835, 1106.6552735843661, 1110.2953960326145, 1111.2879830821855, 1105.7613947875805, 1102.8076578449673, 1103.3604975230521, 1101.4319952579833, 1107.115787350823, 1106.0089397227714, 1105.669504921781, 1109.690186966582, 1101.0517764155554, 1107.897964370449, 1101.39352088379, 1100.3120076064956, 1103.830880969231, 1098.2732695095137, 1096.542899747351, 1099.8656938704694, 1099.3719067524783, 1098.1788169014828, 1105.623158811543, 1094.524385888662, 1103.4719082113495, 1092.7810004253247, 1099.9773546719518, 1098.265636259744]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('449ad5ea-6fd3-4ff2-924f-8358cc148909');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.73% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1098   : Mean absolute error \n",
      "\n",
      "9.41% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Learning_rate_decay\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "decay = 1e-3\n",
    "n_epochs=300\n",
    "n_steps_per_epoch = len(X_train) // 1024\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          0.005,
          0.004906771344455349,
          0.004816955684007707,
          0.004730368968779565,
          0.004646840148698885,
          0.004566210045662101,
          0.004488330341113105,
          0.0044130626654898504,
          0.004340277777777778,
          0.004269854824935952,
          0.004201680672268908,
          0.0041356492969396195,
          0.004071661237785016,
          0.00400962309542903,
          0.0039494470774091624,
          0.003891050583657588,
          0.003834355828220859,
          0.003779289493575208,
          0.0037257824143070045,
          0.0036737692872887582,
          0.003623188405797102,
          0.0035739814152966403,
          0.003526093088857546,
          0.003479471120389701,
          0.0034340659340659344,
          0.003389830508474576,
          0.0033467202141900937,
          0.0033046926635822873,
          0.0032637075718015664,
          0.003223726627981947,
          0.003184713375796179,
          0.0031466331025802393,
          0.0031094527363184077,
          0.003073140749846343,
          0.003037667071688943,
          0.003003003003003003,
          0.0029691211401425177,
          0.0029359953024075164,
          0.0029036004645760743,
          0.002871912693854107,
          0.002840909090909091,
          0.002810567734682406,
          0.0027808676307007787,
          0.0027517886626307097,
          0.0027233115468409588,
          0.0026954177897574125,
          0.0026680896478121665,
          0.002641310089804543,
          0.0026150627615062765,
          0.002589331952356292,
          0.002564102564102564,
          0.0025393600812595226,
          0.0025150905432595573,
          0.0024912805181863482,
          0.00246791707798618,
          0.0024449877750611247,
          0.0024224806201550387,
          0.002400384061449832,
          0.0023786869647954324,
          0.0023573785950023575,
          0.0023364485981308414,
          0.0023158869847151463,
          0.002295684113865932,
          0.002275830678197542,
          0.002256317689530686,
          0.0022371364653243843,
          0.0022182786157941437,
          0.002199736031676199,
          0.0021815008726003495,
          0.002163565556036348,
          0.002145922746781116,
          0.0021285653469561515,
          0.002111486486486486,
          0.0020946795140343527,
          0.0020781379883624274,
          0.002061855670103093,
          0.0020458265139116204,
          0.0020300446609825416,
          0.00201450443190975,
          0.0019992003198720507,
          0.001984126984126984,
          0.0019692792437967705,
          0.0019546520719311965,
          0.0019402405898331393,
          0.001926040061633282,
          0.0019120458891013384,
          0.0018982536066818525,
          0.0018846588767433092,
          0.0018712574850299403,
          0.001858045336306206,
          0.001845018450184502,
          0.0018321729571271527,
          0.001819505094614265,
          0.0018070112034694616,
          0.0017946877243359657,
          0.0017825311942959003,
          0.0017705382436260624,
          0.0017587055926837848,
          0.0017470300489168414,
          0.0017355085039916696,
          0.0017241379310344825,
          0.0017129153819801302,
          0.0017018379850238256,
          0.0016909029421711195,
          0.0016801075268817205,
          0.001669449081803005,
          0.0016589250165892503,
          0.0016485328058028356,
          0.00163826998689384,
          0.0016281341582546401,
          0.0016181229773462784,
          0.0016082341588935349,
          0.00159846547314578,
          0.001588814744200826,
          0.0015792798483891346,
          0.0015698587127158557,
          0.001560549313358302,
          0.0015513496742165683,
          0.0015422578655151142,
          0.0015332720024532351,
          0.001524390243902439,
          0.001515610791148833,
          0.0015069318866787222,
          0.0014983518130056938,
          0.0014898688915375448,
          0.0014814814814814814,
          0.0014731879787860931,
          0.001464986815118664,
          0.001456876456876457,
          0.0014488554042306578,
          0.001440922190201729,
          0.0014330753797649758,
          0.0014253135689851768,
          0.001417635384179189,
          0.0014100394811054709,
          0.001402524544179523,
          0.0013950892857142857,
          0.0013877324451845683,
          0.0013804527885146326,
          0.0013732491073880802,
          0.001366120218579235,
          0.001359064963305246,
          0.001352082206598161,
          0.0013451708366962604,
          0.0013383297644539614,
          0.0013315579227696406,
          0.0013248542660307366,
          0.001318217769575534,
          0.001311647429171039,
          0.0013051422605063953,
          0.0012987012987012987,
          0.0012923235978288964,
          0.001286008230452675,
          0.001279754287176862,
          0.0012735608762098828,
          0.001267427122940431,
          0.0012613521695257316,
          0.0012553351744915891,
          0.001249375312343828,
          0.0012434717731907485,
          0.0012376237623762376,
          0.001231830500123183,
          0.0012260912211868563,
          0.0012204051745179402,
          0.0012147716229348885,
          0.0012091898428053206,
          0.001203659123736158,
          0.0011981787682722263,
          0.0011927480916030535,
          0.0011873664212776061,
          0.001182033096926714,
          0.0011767474699929394,
          0.0011715089034676663,
          0.0011663167716351758,
          0.0011611704598235018,
          0.0011560693641618498,
          0.0011510128913443832,
          0.0011460004584001836,
          0.0011410314924691922,
          0.0011361054305839583,
          0.0011312217194570137,
          0.0011263798152737104,
          0.0011215791834903544,
          0.0011168192986374804,
          0.0011120996441281138,
          0.0011074197120708748,
          0.0011027790030877812,
          0.0010981770261366134,
          0.0010936132983377078,
          0.0010890873448050533,
          0.001084598698481562,
          0.0010801468999783973,
          0.0010757314974182445,
          0.0010713520462824085,
          0.0010670081092616305,
          0.0010626992561105207,
          0.0010584250635055038,
          0.0010541851149061775,
          0.0010499790004199914,
          0.0010458063166701526,
          0.0010416666666666664,
          0.0010375596596804316,
          0.0010334849111202976,
          0.0010294420424130121,
          0.001025430680885972,
          0.0010214504596527069,
          0.0010175010175010176,
          0.0010135819987837017,
          0.0010096930533117932,
          0.0010058338362502514,
          0.001002004008016032,
          0.000998203234178479,
          0.000994431185361973,
          0.0009906875371507827,
          0.0009869719699960521,
          0.0009832841691248771,
          0.0009796238244514106,
          0.0009759906304899473,
          0.0009723842862699338,
          0.0009688044952528581,
          0.0009652509652509653,
          0.0009617234083477592,
          0.0009582215408202376,
          0.0009547450830628223,
          0.0009512937595129375,
          0.000947867298578199,
          0.0009444654325651681,
          0.0009410878976096369,
          0.0009377344336084022,
          0.0009344047841524949,
          0.000931098696461825,
          0.0009278159213212099,
          0.0009245562130177514,
          0.0009213193292795282,
          0.0009181050312155709,
          0.0009149130832570906,
          0.0009117432530999271,
          0.0009085953116481918,
          0.0009054690329590728,
          0.0009023641941887746,
          0.0008992805755395684,
          0.0008962179602079227,
          0.0008931761343336906,
          0.0008901548869503294,
          0.0008871540099361249,
          0.0008841732979664014,
          0.0008812125484666903,
          0.0008782715615668366,
          0.0008753501400560224,
          0.0008724480893386844,
          0.0008695652173913044,
          0.0008667013347200555,
          0.0008638562543192812,
          0.0008610297916307904,
          0.0008582217645039477,
          0.0008554319931565441,
          0.0008526603001364257,
          0.0008499065102838688,
          0.0008471704506946798,
          0.0008444519506840061,
          0.0008417508417508417,
          0.0008390669575432119,
          0.0008364001338240215,
          0.0008337502084375521,
          0.0008311170212765958,
          0.0008285004142502071,
          0.0008259002312520648,
          0.0008233163181294253,
          0.0008207485226526592,
          0.0008181966944853542,
          0.0008156606851549755,
          0.0008131403480240689,
          0.0008106355382619975,
          0.0008081461128171974,
          0.0008056719303899452,
          0.0008032128514056224,
          0.0008007687379884688,
          0.0007983394539358134,
          0.000795924864692773,
          0.0007935248373274084,
          0.0007911392405063291,
          0.0007887679444707367,
          0.0007864108210128971,
          0.0007840677434530343,
          0.0007817385866166353,
          0.0007794232268121589,
          0.0007771215418091389,
          0.0007748334108166744,
          0.0007725587144622991,
          0.0007702973347712218,
          0.0007680491551459293,
          0.000765814060346148,
          0.0007635919364691509,
          0.0007613826709304098,
          0.0007591861524445795,
          0.0007570022710068131,
          0.0007548309178743962,
          0.0007526719855486979,
          0.0007505253677574302,
          0.0007483909594372101
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Learning rate"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"a624df4d-4c52-4098-8f82-c76e4947b197\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"a624df4d-4c52-4098-8f82-c76e4947b197\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'a624df4d-4c52-4098-8f82-c76e4947b197',\n",
       "                        [{\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [0.005, 0.004906771344455349, 0.004816955684007707, 0.004730368968779565, 0.004646840148698885, 0.004566210045662101, 0.004488330341113105, 0.0044130626654898504, 0.004340277777777778, 0.004269854824935952, 0.004201680672268908, 0.0041356492969396195, 0.004071661237785016, 0.00400962309542903, 0.0039494470774091624, 0.003891050583657588, 0.003834355828220859, 0.003779289493575208, 0.0037257824143070045, 0.0036737692872887582, 0.003623188405797102, 0.0035739814152966403, 0.003526093088857546, 0.003479471120389701, 0.0034340659340659344, 0.003389830508474576, 0.0033467202141900937, 0.0033046926635822873, 0.0032637075718015664, 0.003223726627981947, 0.003184713375796179, 0.0031466331025802393, 0.0031094527363184077, 0.003073140749846343, 0.003037667071688943, 0.003003003003003003, 0.0029691211401425177, 0.0029359953024075164, 0.0029036004645760743, 0.002871912693854107, 0.002840909090909091, 0.002810567734682406, 0.0027808676307007787, 0.0027517886626307097, 0.0027233115468409588, 0.0026954177897574125, 0.0026680896478121665, 0.002641310089804543, 0.0026150627615062765, 0.002589331952356292, 0.002564102564102564, 0.0025393600812595226, 0.0025150905432595573, 0.0024912805181863482, 0.00246791707798618, 0.0024449877750611247, 0.0024224806201550387, 0.002400384061449832, 0.0023786869647954324, 0.0023573785950023575, 0.0023364485981308414, 0.0023158869847151463, 0.002295684113865932, 0.002275830678197542, 0.002256317689530686, 0.0022371364653243843, 0.0022182786157941437, 0.002199736031676199, 0.0021815008726003495, 0.002163565556036348, 0.002145922746781116, 0.0021285653469561515, 0.002111486486486486, 0.0020946795140343527, 0.0020781379883624274, 0.002061855670103093, 0.0020458265139116204, 0.0020300446609825416, 0.00201450443190975, 0.0019992003198720507, 0.001984126984126984, 0.0019692792437967705, 0.0019546520719311965, 0.0019402405898331393, 0.001926040061633282, 0.0019120458891013384, 0.0018982536066818525, 0.0018846588767433092, 0.0018712574850299403, 0.001858045336306206, 0.001845018450184502, 0.0018321729571271527, 0.001819505094614265, 0.0018070112034694616, 0.0017946877243359657, 0.0017825311942959003, 0.0017705382436260624, 0.0017587055926837848, 0.0017470300489168414, 0.0017355085039916696, 0.0017241379310344825, 0.0017129153819801302, 0.0017018379850238256, 0.0016909029421711195, 0.0016801075268817205, 0.001669449081803005, 0.0016589250165892503, 0.0016485328058028356, 0.00163826998689384, 0.0016281341582546401, 0.0016181229773462784, 0.0016082341588935349, 0.00159846547314578, 0.001588814744200826, 0.0015792798483891346, 0.0015698587127158557, 0.001560549313358302, 0.0015513496742165683, 0.0015422578655151142, 0.0015332720024532351, 0.001524390243902439, 0.001515610791148833, 0.0015069318866787222, 0.0014983518130056938, 0.0014898688915375448, 0.0014814814814814814, 0.0014731879787860931, 0.001464986815118664, 0.001456876456876457, 0.0014488554042306578, 0.001440922190201729, 0.0014330753797649758, 0.0014253135689851768, 0.001417635384179189, 0.0014100394811054709, 0.001402524544179523, 0.0013950892857142857, 0.0013877324451845683, 0.0013804527885146326, 0.0013732491073880802, 0.001366120218579235, 0.001359064963305246, 0.001352082206598161, 0.0013451708366962604, 0.0013383297644539614, 0.0013315579227696406, 0.0013248542660307366, 0.001318217769575534, 0.001311647429171039, 0.0013051422605063953, 0.0012987012987012987, 0.0012923235978288964, 0.001286008230452675, 0.001279754287176862, 0.0012735608762098828, 0.001267427122940431, 0.0012613521695257316, 0.0012553351744915891, 0.001249375312343828, 0.0012434717731907485, 0.0012376237623762376, 0.001231830500123183, 0.0012260912211868563, 0.0012204051745179402, 0.0012147716229348885, 0.0012091898428053206, 0.001203659123736158, 0.0011981787682722263, 0.0011927480916030535, 0.0011873664212776061, 0.001182033096926714, 0.0011767474699929394, 0.0011715089034676663, 0.0011663167716351758, 0.0011611704598235018, 0.0011560693641618498, 0.0011510128913443832, 0.0011460004584001836, 0.0011410314924691922, 0.0011361054305839583, 0.0011312217194570137, 0.0011263798152737104, 0.0011215791834903544, 0.0011168192986374804, 0.0011120996441281138, 0.0011074197120708748, 0.0011027790030877812, 0.0010981770261366134, 0.0010936132983377078, 0.0010890873448050533, 0.001084598698481562, 0.0010801468999783973, 0.0010757314974182445, 0.0010713520462824085, 0.0010670081092616305, 0.0010626992561105207, 0.0010584250635055038, 0.0010541851149061775, 0.0010499790004199914, 0.0010458063166701526, 0.0010416666666666664, 0.0010375596596804316, 0.0010334849111202976, 0.0010294420424130121, 0.001025430680885972, 0.0010214504596527069, 0.0010175010175010176, 0.0010135819987837017, 0.0010096930533117932, 0.0010058338362502514, 0.001002004008016032, 0.000998203234178479, 0.000994431185361973, 0.0009906875371507827, 0.0009869719699960521, 0.0009832841691248771, 0.0009796238244514106, 0.0009759906304899473, 0.0009723842862699338, 0.0009688044952528581, 0.0009652509652509653, 0.0009617234083477592, 0.0009582215408202376, 0.0009547450830628223, 0.0009512937595129375, 0.000947867298578199, 0.0009444654325651681, 0.0009410878976096369, 0.0009377344336084022, 0.0009344047841524949, 0.000931098696461825, 0.0009278159213212099, 0.0009245562130177514, 0.0009213193292795282, 0.0009181050312155709, 0.0009149130832570906, 0.0009117432530999271, 0.0009085953116481918, 0.0009054690329590728, 0.0009023641941887746, 0.0008992805755395684, 0.0008962179602079227, 0.0008931761343336906, 0.0008901548869503294, 0.0008871540099361249, 0.0008841732979664014, 0.0008812125484666903, 0.0008782715615668366, 0.0008753501400560224, 0.0008724480893386844, 0.0008695652173913044, 0.0008667013347200555, 0.0008638562543192812, 0.0008610297916307904, 0.0008582217645039477, 0.0008554319931565441, 0.0008526603001364257, 0.0008499065102838688, 0.0008471704506946798, 0.0008444519506840061, 0.0008417508417508417, 0.0008390669575432119, 0.0008364001338240215, 0.0008337502084375521, 0.0008311170212765958, 0.0008285004142502071, 0.0008259002312520648, 0.0008233163181294253, 0.0008207485226526592, 0.0008181966944853542, 0.0008156606851549755, 0.0008131403480240689, 0.0008106355382619975, 0.0008081461128171974, 0.0008056719303899452, 0.0008032128514056224, 0.0008007687379884688, 0.0007983394539358134, 0.000795924864692773, 0.0007935248373274084, 0.0007911392405063291, 0.0007887679444707367, 0.0007864108210128971, 0.0007840677434530343, 0.0007817385866166353, 0.0007794232268121589, 0.0007771215418091389, 0.0007748334108166744, 0.0007725587144622991, 0.0007702973347712218, 0.0007680491551459293, 0.000765814060346148, 0.0007635919364691509, 0.0007613826709304098, 0.0007591861524445795, 0.0007570022710068131, 0.0007548309178743962, 0.0007526719855486979, 0.0007505253677574302, 0.0007483909594372101]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Learning rate\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a624df4d-4c52-4098-8f82-c76e4947b197');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trace0=go.Scatter(\n",
    "            y=lrs,\n",
    "            x=epochs,\n",
    "            mode='lines',\n",
    "            marker=dict(\n",
    "            color=\"red\",\n",
    "            size=5,\n",
    "            opacity=0.5\n",
    "            )\n",
    "    )\n",
    "        \n",
    "\n",
    "data=[trace0]\n",
    "figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning curve\",\n",
    "                yaxis=dict(title=\"Learning rate\"),\n",
    "                xaxis=dict(title=\"Epoch\"),\n",
    "                legend=dict(\n",
    "                    x=1,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None\n",
    "\n",
    "\n",
    "            )))\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/400\n",
      "19948/19948 [==============================] - 3s 140us/sample - loss: 11171.9912 - val_loss: 10171.6175\n",
      "Epoch 2/400\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 10981.1403 - val_loss: 9519.8079\n",
      "Epoch 3/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 10367.3199 - val_loss: 7049.3895\n",
      "Epoch 4/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 8996.3459 - val_loss: 5648.5767\n",
      "Epoch 5/400\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 6523.2488 - val_loss: 6956.5481\n",
      "Epoch 6/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 3523.3090 - val_loss: 7441.0197\n",
      "Epoch 7/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1772.8196 - val_loss: 12057.2688\n",
      "Epoch 8/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1617.2185 - val_loss: 10632.2914\n",
      "Epoch 9/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1571.6941 - val_loss: 7352.6666\n",
      "Epoch 10/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1548.8343 - val_loss: 5759.5957\n",
      "Epoch 11/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1502.6432 - val_loss: 4211.7585\n",
      "Epoch 12/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1495.8578 - val_loss: 3044.0786\n",
      "Epoch 13/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1474.2995 - val_loss: 2087.8500\n",
      "Epoch 14/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1446.5363 - val_loss: 1733.0885\n",
      "Epoch 15/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1426.1631 - val_loss: 1513.6709\n",
      "Epoch 16/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1416.1177 - val_loss: 1397.7590\n",
      "Epoch 17/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1428.5048 - val_loss: 1401.9621\n",
      "Epoch 18/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1400.3267 - val_loss: 1416.3255\n",
      "Epoch 19/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1410.2138 - val_loss: 1380.7564\n",
      "Epoch 20/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1407.0759 - val_loss: 1338.0345\n",
      "Epoch 21/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1403.4562 - val_loss: 1565.4169\n",
      "Epoch 22/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1420.0277 - val_loss: 1348.9569\n",
      "Epoch 23/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1370.4788 - val_loss: 1267.4912\n",
      "Epoch 24/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1377.2036 - val_loss: 1287.6840\n",
      "Epoch 25/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1365.4119 - val_loss: 1293.2388\n",
      "Epoch 26/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1355.4594 - val_loss: 1247.6801\n",
      "Epoch 27/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1340.7047 - val_loss: 1284.0664\n",
      "Epoch 28/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1353.7416 - val_loss: 1278.6029\n",
      "Epoch 29/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1356.1713 - val_loss: 1216.5585\n",
      "Epoch 30/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1353.0292 - val_loss: 1228.8707\n",
      "Epoch 31/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1340.2140 - val_loss: 1246.1551\n",
      "Epoch 32/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1337.7177 - val_loss: 1283.4915\n",
      "Epoch 33/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1339.1805 - val_loss: 1213.7960\n",
      "Epoch 34/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1332.0837 - val_loss: 1223.9534\n",
      "Epoch 35/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1321.4758 - val_loss: 1213.2346\n",
      "Epoch 36/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1320.8164 - val_loss: 1221.1437\n",
      "Epoch 37/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1322.6376 - val_loss: 1196.7012\n",
      "Epoch 38/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1330.9539 - val_loss: 1223.3368\n",
      "Epoch 39/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1321.8941 - val_loss: 1189.4327\n",
      "Epoch 40/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1306.1989 - val_loss: 1193.9933\n",
      "Epoch 41/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1306.3046 - val_loss: 1196.9047\n",
      "Epoch 42/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1297.2694 - val_loss: 1191.3469\n",
      "Epoch 43/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1299.9743 - val_loss: 1195.3612\n",
      "Epoch 44/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1301.2208 - val_loss: 1194.8524\n",
      "Epoch 45/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1300.2285 - val_loss: 1202.0406\n",
      "Epoch 46/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1291.7244 - val_loss: 1182.6801\n",
      "Epoch 47/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1293.9398 - val_loss: 1193.8397\n",
      "Epoch 48/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1294.2147 - val_loss: 1182.1221\n",
      "Epoch 49/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1295.8089 - val_loss: 1190.9351\n",
      "Epoch 50/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1290.9435 - val_loss: 1181.4044\n",
      "Epoch 51/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1280.8361 - val_loss: 1189.0792\n",
      "Epoch 52/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1282.4972 - val_loss: 1183.0865\n",
      "Epoch 53/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1288.6097 - val_loss: 1191.3201\n",
      "Epoch 54/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1267.8269 - val_loss: 1175.4969\n",
      "Epoch 55/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1273.3128 - val_loss: 1193.0941\n",
      "Epoch 56/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1268.8362 - val_loss: 1179.9456\n",
      "Epoch 57/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1264.7603 - val_loss: 1176.0512\n",
      "Epoch 58/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1267.3290 - val_loss: 1173.2502\n",
      "Epoch 59/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1263.8297 - val_loss: 1175.1496\n",
      "Epoch 60/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1260.1320 - val_loss: 1172.2704\n",
      "Epoch 61/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1260.0957 - val_loss: 1167.8910\n",
      "Epoch 62/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1259.1481 - val_loss: 1172.0713\n",
      "Epoch 63/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1255.4812 - val_loss: 1171.4739\n",
      "Epoch 64/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1256.7783 - val_loss: 1176.4928\n",
      "Epoch 65/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1259.2988 - val_loss: 1176.0918\n",
      "Epoch 66/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1257.7938 - val_loss: 1176.2732\n",
      "Epoch 67/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1248.0222 - val_loss: 1168.5739\n",
      "Epoch 68/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1244.0206 - val_loss: 1175.0635\n",
      "Epoch 69/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1247.8183 - val_loss: 1161.4109\n",
      "Epoch 70/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1254.8525 - val_loss: 1170.0737\n",
      "Epoch 71/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1250.8150 - val_loss: 1178.7694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1244.8074 - val_loss: 1161.9507\n",
      "Epoch 73/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1230.9441 - val_loss: 1162.1774\n",
      "Epoch 74/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1233.3487 - val_loss: 1158.4019\n",
      "Epoch 75/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1226.2658 - val_loss: 1174.6435\n",
      "Epoch 76/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1234.9970 - val_loss: 1171.7485\n",
      "Epoch 77/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1227.0185 - val_loss: 1179.9511\n",
      "Epoch 78/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1227.5357 - val_loss: 1160.8970\n",
      "Epoch 79/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1221.1107 - val_loss: 1172.6220\n",
      "Epoch 80/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1238.3771 - val_loss: 1171.9728\n",
      "Epoch 81/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1223.0761 - val_loss: 1162.7403\n",
      "Epoch 82/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1229.4338 - val_loss: 1157.3878\n",
      "Epoch 83/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1225.7884 - val_loss: 1153.9946\n",
      "Epoch 84/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1216.9127 - val_loss: 1156.0120\n",
      "Epoch 85/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1223.9434 - val_loss: 1150.1605\n",
      "Epoch 86/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1225.5129 - val_loss: 1169.8901\n",
      "Epoch 87/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1223.3707 - val_loss: 1169.3951\n",
      "Epoch 88/400\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1216.2320 - val_loss: 1150.3465\n",
      "Epoch 89/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1212.1351 - val_loss: 1147.3291\n",
      "Epoch 90/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1213.5849 - val_loss: 1151.7558\n",
      "Epoch 91/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1221.3862 - val_loss: 1143.9279\n",
      "Epoch 92/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1212.0287 - val_loss: 1159.0395\n",
      "Epoch 93/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1212.9508 - val_loss: 1154.8556\n",
      "Epoch 94/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1210.6053 - val_loss: 1147.2334\n",
      "Epoch 95/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1203.5866 - val_loss: 1150.0566\n",
      "Epoch 96/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1203.8089 - val_loss: 1146.0123\n",
      "Epoch 97/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1214.3629 - val_loss: 1143.3663\n",
      "Epoch 98/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1211.9614 - val_loss: 1142.2685\n",
      "Epoch 99/400\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 1209.8719 - val_loss: 1150.2392\n",
      "Epoch 100/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1202.5022 - val_loss: 1140.7399\n",
      "Epoch 101/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1207.7484 - val_loss: 1141.9211\n",
      "Epoch 102/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1206.5555 - val_loss: 1151.7999\n",
      "Epoch 103/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1203.2381 - val_loss: 1146.2544\n",
      "Epoch 104/400\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1207.0475 - val_loss: 1140.9454\n",
      "Epoch 105/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1206.5951 - val_loss: 1158.7143\n",
      "Epoch 106/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1199.3254 - val_loss: 1133.3037\n",
      "Epoch 107/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1194.5618 - val_loss: 1144.7620\n",
      "Epoch 108/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1198.9104 - val_loss: 1138.1912\n",
      "Epoch 109/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1200.0323 - val_loss: 1144.1593\n",
      "Epoch 110/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1186.0515 - val_loss: 1133.9313\n",
      "Epoch 111/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1193.0460 - val_loss: 1138.9914\n",
      "Epoch 112/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1186.8499 - val_loss: 1138.4004\n",
      "Epoch 113/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1190.4080 - val_loss: 1141.9660\n",
      "Epoch 114/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1193.6809 - val_loss: 1137.6565\n",
      "Epoch 115/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1193.8322 - val_loss: 1136.4670\n",
      "Epoch 116/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1180.8681 - val_loss: 1136.7217\n",
      "Epoch 117/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1190.8109 - val_loss: 1138.2030\n",
      "Epoch 118/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1181.3267 - val_loss: 1139.4562\n",
      "Epoch 119/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1180.3320 - val_loss: 1144.2673\n",
      "Epoch 120/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1193.3423 - val_loss: 1140.9416\n",
      "Epoch 121/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1184.6914 - val_loss: 1134.6622\n",
      "Epoch 122/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1186.9117 - val_loss: 1129.5120\n",
      "Epoch 123/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1178.6386 - val_loss: 1140.5651\n",
      "Epoch 124/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1177.6978 - val_loss: 1133.5647\n",
      "Epoch 125/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1180.2393 - val_loss: 1131.2125\n",
      "Epoch 126/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1178.5850 - val_loss: 1137.1876\n",
      "Epoch 127/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1177.9082 - val_loss: 1127.8514\n",
      "Epoch 128/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1182.5988 - val_loss: 1131.1280\n",
      "Epoch 129/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1171.0224 - val_loss: 1131.0259\n",
      "Epoch 130/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1171.1767 - val_loss: 1134.3962\n",
      "Epoch 131/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1177.6678 - val_loss: 1131.2855\n",
      "Epoch 132/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1171.1467 - val_loss: 1134.1684\n",
      "Epoch 133/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1173.0797 - val_loss: 1133.0483\n",
      "Epoch 134/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1174.8500 - val_loss: 1134.0210\n",
      "Epoch 135/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1176.2258 - val_loss: 1128.8198\n",
      "Epoch 136/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1161.4606 - val_loss: 1127.3915\n",
      "Epoch 137/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1172.8600 - val_loss: 1139.1067\n",
      "Epoch 138/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1172.6552 - val_loss: 1132.2058\n",
      "Epoch 139/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1172.5019 - val_loss: 1133.2281\n",
      "Epoch 140/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1163.0776 - val_loss: 1130.3801\n",
      "Epoch 141/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1170.2573 - val_loss: 1127.9452\n",
      "Epoch 142/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1168.3212 - val_loss: 1134.0370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1167.5881 - val_loss: 1126.2949\n",
      "Epoch 144/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1163.2005 - val_loss: 1133.9120\n",
      "Epoch 145/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1164.4124 - val_loss: 1127.9940\n",
      "Epoch 146/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1172.9892 - val_loss: 1126.6358\n",
      "Epoch 147/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1168.2314 - val_loss: 1130.8351\n",
      "Epoch 148/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1155.9621 - val_loss: 1130.8051\n",
      "Epoch 149/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1165.6345 - val_loss: 1128.1870\n",
      "Epoch 150/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1166.2103 - val_loss: 1124.9474\n",
      "Epoch 151/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1165.3027 - val_loss: 1122.5332\n",
      "Epoch 152/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1161.4529 - val_loss: 1123.1345\n",
      "Epoch 153/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1149.3414 - val_loss: 1128.3802\n",
      "Epoch 154/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1163.0424 - val_loss: 1125.9039\n",
      "Epoch 155/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1148.2641 - val_loss: 1127.7031\n",
      "Epoch 156/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1160.4570 - val_loss: 1125.0862\n",
      "Epoch 157/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1160.8171 - val_loss: 1127.1756\n",
      "Epoch 158/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1149.0350 - val_loss: 1127.2895\n",
      "Epoch 159/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1161.7323 - val_loss: 1125.6414\n",
      "Epoch 160/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1147.1898 - val_loss: 1120.3514\n",
      "Epoch 161/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1152.3664 - val_loss: 1129.9028\n",
      "Epoch 162/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1155.8678 - val_loss: 1124.3795\n",
      "Epoch 163/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1149.6314 - val_loss: 1121.8675\n",
      "Epoch 164/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1152.8155 - val_loss: 1118.1292\n",
      "Epoch 165/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1149.0217 - val_loss: 1116.1973\n",
      "Epoch 166/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1147.8751 - val_loss: 1116.9430\n",
      "Epoch 167/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1154.3631 - val_loss: 1124.3925\n",
      "Epoch 168/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1160.9116 - val_loss: 1122.5274\n",
      "Epoch 169/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.9193 - val_loss: 1125.7735\n",
      "Epoch 170/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1153.1338 - val_loss: 1115.2404\n",
      "Epoch 171/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1143.9914 - val_loss: 1120.4895\n",
      "Epoch 172/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.9252 - val_loss: 1119.8055\n",
      "Epoch 173/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.1679 - val_loss: 1116.5101\n",
      "Epoch 174/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1137.6100 - val_loss: 1121.0385\n",
      "Epoch 175/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1145.8683 - val_loss: 1120.5955\n",
      "Epoch 176/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1149.4994 - val_loss: 1120.1373\n",
      "Epoch 177/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1137.9756 - val_loss: 1121.7009\n",
      "Epoch 178/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.2643 - val_loss: 1119.4085\n",
      "Epoch 179/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1143.7792 - val_loss: 1118.1359\n",
      "Epoch 180/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1142.6033 - val_loss: 1118.0836\n",
      "Epoch 181/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.2068 - val_loss: 1117.6340\n",
      "Epoch 182/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1139.2858 - val_loss: 1119.9206\n",
      "Epoch 183/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1144.9645 - val_loss: 1118.3896\n",
      "Epoch 184/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1135.8391 - val_loss: 1118.9019\n",
      "Epoch 185/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1139.0649 - val_loss: 1118.7690\n",
      "Epoch 186/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1145.9895 - val_loss: 1119.0240\n",
      "Epoch 187/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1145.4502 - val_loss: 1117.4803\n",
      "Epoch 188/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1144.2591 - val_loss: 1120.5501\n",
      "Epoch 189/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1142.0125 - val_loss: 1117.7403\n",
      "Epoch 190/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1138.8743 - val_loss: 1119.2912\n",
      "Epoch 191/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1135.4377 - val_loss: 1119.2623\n",
      "Epoch 192/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1139.2248 - val_loss: 1114.6380\n",
      "Epoch 193/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1135.2103 - val_loss: 1115.6122\n",
      "Epoch 194/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1138.7651 - val_loss: 1118.6987\n",
      "Epoch 195/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.3468 - val_loss: 1117.9666\n",
      "Epoch 196/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1130.8783 - val_loss: 1112.7189\n",
      "Epoch 197/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1124.9589 - val_loss: 1111.6495\n",
      "Epoch 198/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1131.4777 - val_loss: 1111.5794\n",
      "Epoch 199/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1133.7626 - val_loss: 1115.8856\n",
      "Epoch 200/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1135.3442 - val_loss: 1113.5378\n",
      "Epoch 201/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.6424 - val_loss: 1104.4855\n",
      "Epoch 202/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1130.7859 - val_loss: 1117.5032\n",
      "Epoch 203/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1136.5035 - val_loss: 1109.3663\n",
      "Epoch 204/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1127.7889 - val_loss: 1110.6397\n",
      "Epoch 205/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1131.8139 - val_loss: 1114.0674\n",
      "Epoch 206/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1130.3264 - val_loss: 1113.0255\n",
      "Epoch 207/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1131.3044 - val_loss: 1116.2942\n",
      "Epoch 208/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1129.6773 - val_loss: 1113.7314\n",
      "Epoch 209/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1120.9674 - val_loss: 1108.4159\n",
      "Epoch 210/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1126.8328 - val_loss: 1109.0434\n",
      "Epoch 211/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1132.7454 - val_loss: 1112.6018\n",
      "Epoch 212/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1128.1330 - val_loss: 1107.7240\n",
      "Epoch 213/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.9956 - val_loss: 1109.7509\n",
      "Epoch 214/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1114.7326 - val_loss: 1106.2844\n",
      "Epoch 215/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1121.2656 - val_loss: 1105.0983\n",
      "Epoch 216/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1116.7079 - val_loss: 1104.8909\n",
      "Epoch 217/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1133.8237 - val_loss: 1109.6679\n",
      "Epoch 218/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1125.0377 - val_loss: 1111.6402\n",
      "Epoch 219/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1132.4009 - val_loss: 1111.0711\n",
      "Epoch 220/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1119.8802 - val_loss: 1105.2066\n",
      "Epoch 221/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1119.0788 - val_loss: 1110.5955\n",
      "Epoch 222/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1121.3614 - val_loss: 1115.1379\n",
      "Epoch 223/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.4322 - val_loss: 1105.4852\n",
      "Epoch 224/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1126.4573 - val_loss: 1110.6386\n",
      "Epoch 225/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.5579 - val_loss: 1110.3526\n",
      "Epoch 226/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1122.3819 - val_loss: 1109.8593\n",
      "Epoch 227/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1118.1008 - val_loss: 1103.0858\n",
      "Epoch 228/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1122.3056 - val_loss: 1106.7568\n",
      "Epoch 229/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1113.9625 - val_loss: 1110.2229\n",
      "Epoch 230/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1125.1502 - val_loss: 1105.3485\n",
      "Epoch 231/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1116.3051 - val_loss: 1109.9133\n",
      "Epoch 232/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1120.8477 - val_loss: 1106.4406\n",
      "Epoch 233/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1125.4804 - val_loss: 1111.0349\n",
      "Epoch 234/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1112.9402 - val_loss: 1109.5660\n",
      "Epoch 235/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1114.0511 - val_loss: 1108.6008\n",
      "Epoch 236/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1111.1717 - val_loss: 1103.7909\n",
      "Epoch 237/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1114.7948 - val_loss: 1105.3004\n",
      "Epoch 238/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1116.9862 - val_loss: 1107.5923\n",
      "Epoch 239/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1116.7076 - val_loss: 1108.5207\n",
      "Epoch 240/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1115.2198 - val_loss: 1106.8678\n",
      "Epoch 241/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1111.5899 - val_loss: 1109.9964\n",
      "Epoch 242/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1108.6424 - val_loss: 1107.0837\n",
      "Epoch 243/400\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1106.05 - 0s 19us/sample - loss: 1107.9496 - val_loss: 1107.6037\n",
      "Epoch 244/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1112.4573 - val_loss: 1104.2462\n",
      "Epoch 245/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1114.4398 - val_loss: 1109.4857\n",
      "Epoch 246/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1114.6392 - val_loss: 1105.1482\n",
      "Epoch 247/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1119.1389 - val_loss: 1108.5280\n",
      "Epoch 248/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1110.0826 - val_loss: 1104.7619\n",
      "Epoch 249/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1112.5677 - val_loss: 1109.3781\n",
      "Epoch 250/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1116.4534 - val_loss: 1103.3750\n",
      "Epoch 251/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1106.3294 - val_loss: 1105.7908\n",
      "Epoch 252/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1123.5131 - val_loss: 1105.0769\n",
      "Epoch 253/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1112.5137 - val_loss: 1107.7115\n",
      "Epoch 254/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1103.8551 - val_loss: 1102.7102\n",
      "Epoch 255/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1109.0061 - val_loss: 1105.1798\n",
      "Epoch 256/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1109.9370 - val_loss: 1110.8763\n",
      "Epoch 257/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1110.1354 - val_loss: 1104.8257\n",
      "Epoch 258/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1108.8262 - val_loss: 1103.8319\n",
      "Epoch 259/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1102.1148 - val_loss: 1110.5638\n",
      "Epoch 260/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1119.3543 - val_loss: 1101.3462\n",
      "Epoch 261/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1103.6159 - val_loss: 1105.0479\n",
      "Epoch 262/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1115.8412 - val_loss: 1103.0842\n",
      "Epoch 263/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1102.8919 - val_loss: 1103.0103\n",
      "Epoch 264/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1101.8916 - val_loss: 1099.7696\n",
      "Epoch 265/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1104.4555 - val_loss: 1104.0365\n",
      "Epoch 266/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1111.8301 - val_loss: 1102.0441\n",
      "Epoch 267/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1103.5504 - val_loss: 1098.8695\n",
      "Epoch 268/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.2453 - val_loss: 1102.9640\n",
      "Epoch 269/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1108.7192 - val_loss: 1101.5875\n",
      "Epoch 270/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1103.0833 - val_loss: 1102.1688\n",
      "Epoch 271/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1107.8526 - val_loss: 1100.8184\n",
      "Epoch 272/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1097.0999 - val_loss: 1102.9459\n",
      "Epoch 273/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1100.5285 - val_loss: 1098.9409\n",
      "Epoch 274/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1114.7025 - val_loss: 1103.3896\n",
      "Epoch 275/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1106.4706 - val_loss: 1100.4513\n",
      "Epoch 276/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1101.7387 - val_loss: 1099.5449\n",
      "Epoch 277/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1111.1511 - val_loss: 1098.9545\n",
      "Epoch 278/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1109.1940 - val_loss: 1108.4787\n",
      "Epoch 279/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1105.6374 - val_loss: 1104.2197\n",
      "Epoch 280/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1104.2088 - val_loss: 1106.8280\n",
      "Epoch 281/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1111.7959 - val_loss: 1100.6350\n",
      "Epoch 282/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1108.7548 - val_loss: 1102.7278\n",
      "Epoch 283/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1101.7443 - val_loss: 1100.1086\n",
      "Epoch 284/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1110.6791 - val_loss: 1103.5713\n",
      "Epoch 285/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1096.0764 - val_loss: 1099.6288\n",
      "Epoch 286/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1096.3791 - val_loss: 1103.8458\n",
      "Epoch 287/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1099.9575 - val_loss: 1099.4184\n",
      "Epoch 288/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1103.2459 - val_loss: 1106.6235\n",
      "Epoch 289/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1101.4990 - val_loss: 1102.6933\n",
      "Epoch 290/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1102.7079 - val_loss: 1097.8390\n",
      "Epoch 291/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1098.8337 - val_loss: 1100.2161\n",
      "Epoch 292/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1091.3124 - val_loss: 1102.4411\n",
      "Epoch 293/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1101.1109 - val_loss: 1097.4617\n",
      "Epoch 294/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1102.3702 - val_loss: 1098.2520\n",
      "Epoch 295/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1101.7075 - val_loss: 1101.7107\n",
      "Epoch 296/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1092.2492 - val_loss: 1101.5657\n",
      "Epoch 297/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1099.0788 - val_loss: 1102.4811\n",
      "Epoch 298/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1101.2154 - val_loss: 1100.2436\n",
      "Epoch 299/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1104.1840 - val_loss: 1099.5521\n",
      "Epoch 300/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1094.3327 - val_loss: 1099.4420\n",
      "Epoch 301/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1101.5056 - val_loss: 1096.0809\n",
      "Epoch 302/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1089.5865 - val_loss: 1103.4938\n",
      "Epoch 303/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1096.8044 - val_loss: 1103.8206\n",
      "Epoch 304/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1098.8459 - val_loss: 1105.3220\n",
      "Epoch 305/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1092.2125 - val_loss: 1097.5914\n",
      "Epoch 306/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1092.7556 - val_loss: 1095.5984\n",
      "Epoch 307/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1099.2370 - val_loss: 1097.6176\n",
      "Epoch 308/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1084.3485 - val_loss: 1097.5445\n",
      "Epoch 309/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1091.4433 - val_loss: 1097.4540\n",
      "Epoch 310/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.7724 - val_loss: 1098.8691\n",
      "Epoch 311/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1089.9524 - val_loss: 1100.1970\n",
      "Epoch 312/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1091.8751 - val_loss: 1103.0513\n",
      "Epoch 313/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1096.7301 - val_loss: 1099.6746\n",
      "Epoch 314/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1089.7073 - val_loss: 1101.9388\n",
      "Epoch 315/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1093.3985 - val_loss: 1098.7804\n",
      "Epoch 316/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1088.6121 - val_loss: 1104.5815\n",
      "Epoch 317/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1087.6305 - val_loss: 1097.5388\n",
      "Epoch 318/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1090.5715 - val_loss: 1099.3153\n",
      "Epoch 319/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1085.0059 - val_loss: 1097.2824\n",
      "Epoch 320/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1094.0276 - val_loss: 1095.6776\n",
      "Epoch 321/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1091.1150 - val_loss: 1096.4017\n",
      "Epoch 322/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.8574 - val_loss: 1097.1251\n",
      "Epoch 323/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1085.1081 - val_loss: 1097.1615\n",
      "Epoch 324/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1084.7308 - val_loss: 1098.4603\n",
      "Epoch 325/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1090.1817 - val_loss: 1098.7541\n",
      "Epoch 326/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1090.6295 - val_loss: 1099.7006\n",
      "Epoch 327/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.4275 - val_loss: 1100.4270\n",
      "Epoch 328/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.5424 - val_loss: 1095.4330\n",
      "Epoch 329/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1091.8301 - val_loss: 1096.3084\n",
      "Epoch 330/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1087.0219 - val_loss: 1098.8646\n",
      "Epoch 331/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1088.6243 - val_loss: 1097.7460\n",
      "Epoch 332/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1093.1221 - val_loss: 1092.3168\n",
      "Epoch 333/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1090.2225 - val_loss: 1096.2615\n",
      "Epoch 334/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1086.8482 - val_loss: 1098.6628\n",
      "Epoch 335/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1089.2393 - val_loss: 1100.6411\n",
      "Epoch 336/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1095.1857 - val_loss: 1098.5010\n",
      "Epoch 337/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1088.4246 - val_loss: 1098.6560\n",
      "Epoch 338/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1092.4292 - val_loss: 1093.4999\n",
      "Epoch 339/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1083.2893 - val_loss: 1096.4972\n",
      "Epoch 340/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1085.3490 - val_loss: 1098.1592\n",
      "Epoch 341/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.0152 - val_loss: 1098.5001\n",
      "Epoch 342/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1088.4385 - val_loss: 1099.1405\n",
      "Epoch 343/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1080.4586 - val_loss: 1096.9594\n",
      "Epoch 344/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1084.4666 - val_loss: 1096.9590\n",
      "Epoch 345/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1082.9132 - val_loss: 1097.6559\n",
      "Epoch 346/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1084.9905 - val_loss: 1097.5146\n",
      "Epoch 347/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1082.2925 - val_loss: 1094.4625\n",
      "Epoch 348/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1074.8770 - val_loss: 1095.9123\n",
      "Epoch 349/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1075.2815 - val_loss: 1097.1377\n",
      "Epoch 350/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1082.7459 - val_loss: 1101.3618\n",
      "Epoch 351/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1087.3867 - val_loss: 1099.2711\n",
      "Epoch 352/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.6746 - val_loss: 1098.0335\n",
      "Epoch 353/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.1327 - val_loss: 1098.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1083.5307 - val_loss: 1100.5034\n",
      "Epoch 355/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1084.6343 - val_loss: 1095.3567\n",
      "Epoch 356/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1086.1410 - val_loss: 1095.6316\n",
      "Epoch 357/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1086.2421 - val_loss: 1095.8183\n",
      "Epoch 358/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.6851 - val_loss: 1096.9199\n",
      "Epoch 359/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1081.3995 - val_loss: 1091.7221\n",
      "Epoch 360/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1074.2338 - val_loss: 1093.8421\n",
      "Epoch 361/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1085.1035 - val_loss: 1097.1383\n",
      "Epoch 362/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1081.9954 - val_loss: 1091.6572\n",
      "Epoch 363/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.4856 - val_loss: 1095.8091\n",
      "Epoch 364/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.8266 - val_loss: 1094.7131\n",
      "Epoch 365/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1074.3908 - val_loss: 1093.4344\n",
      "Epoch 366/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1084.9795 - val_loss: 1095.4399\n",
      "Epoch 367/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1078.8403 - val_loss: 1093.7926\n",
      "Epoch 368/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1074.6624 - val_loss: 1094.2658\n",
      "Epoch 369/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1079.1259 - val_loss: 1095.6046\n",
      "Epoch 370/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1077.9140 - val_loss: 1098.0152\n",
      "Epoch 371/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.3900 - val_loss: 1096.1609\n",
      "Epoch 372/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1077.1362 - val_loss: 1095.5813\n",
      "Epoch 373/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1075.0866 - val_loss: 1093.7910\n",
      "Epoch 374/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1082.6206 - val_loss: 1096.0575\n",
      "Epoch 375/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1072.8100 - val_loss: 1094.8768\n",
      "Epoch 376/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1078.1173 - val_loss: 1093.0492\n",
      "Epoch 377/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1076.5361 - val_loss: 1093.9353\n",
      "Epoch 378/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1076.8167 - val_loss: 1098.1902\n",
      "Epoch 379/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1074.5185 - val_loss: 1096.2248\n",
      "Epoch 380/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1069.6833 - val_loss: 1094.4743\n",
      "Epoch 381/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1067.9371 - val_loss: 1093.6273\n",
      "Epoch 382/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1074.3578 - val_loss: 1090.8508\n",
      "Epoch 383/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1079.7311 - val_loss: 1091.7618\n",
      "Epoch 384/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1081.0968 - val_loss: 1092.7756\n",
      "Epoch 385/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1073.9490 - val_loss: 1092.0122\n",
      "Epoch 386/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1079.0750 - val_loss: 1095.4450\n",
      "Epoch 387/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1074.6327 - val_loss: 1092.6114\n",
      "Epoch 388/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.9411 - val_loss: 1092.9237\n",
      "Epoch 389/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1075.1613 - val_loss: 1092.0673\n",
      "Epoch 390/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1079.6909 - val_loss: 1091.0815\n",
      "Epoch 391/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1075.5358 - val_loss: 1092.6802\n",
      "Epoch 392/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.6506 - val_loss: 1094.2115\n",
      "Epoch 393/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1072.6332 - val_loss: 1093.1183\n",
      "Epoch 394/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1070.8857 - val_loss: 1089.5999\n",
      "Epoch 395/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.7071 - val_loss: 1092.7338\n",
      "Epoch 396/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1062.7337 - val_loss: 1091.6880\n",
      "Epoch 397/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1069.2304 - val_loss: 1093.2759\n",
      "Epoch 398/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1069.4212 - val_loss: 1093.1950\n",
      "Epoch 399/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1067.0245 - val_loss: 1095.3199\n",
      "Epoch 400/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1070.4521 - val_loss: 1091.0114\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.005, decay=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=400, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1502.6431976801794,
          1495.8577816238235,
          1474.2995325933364,
          1446.5363193081307,
          1426.163068949973,
          1416.1177411856766,
          1428.5048099668513,
          1400.3266919527882,
          1410.2137508939259,
          1407.0759096453514,
          1403.4561815064872,
          1420.027697575219,
          1370.478772273732,
          1377.203589170711,
          1365.4118956601617,
          1355.4593743782664,
          1340.7046685885255,
          1353.7416225301017,
          1356.171337934685,
          1353.029233330879,
          1340.2140435983165,
          1337.717656042429,
          1339.1804594778455,
          1332.0836916216538,
          1321.4757720414875,
          1320.8164018195355,
          1322.6376319152453,
          1330.9539032441378,
          1321.8941499118412,
          1306.1989489567407,
          1306.3046355827887,
          1297.2693904542434,
          1299.974275523588,
          1301.2207547974342,
          1300.2285088201982,
          1291.7243503274528,
          1293.939782007438,
          1294.2146667027632,
          1295.8089019243,
          1290.9434617897157,
          1280.8360957973152,
          1282.4972018801618,
          1288.6097356544985,
          1267.8269162909503,
          1273.312781591513,
          1268.8361531240992,
          1264.7603367632798,
          1267.3289908253646,
          1263.829663252386,
          1260.131970466964,
          1260.0956562746735,
          1259.1481023757472,
          1255.4811962518563,
          1256.7782727278425,
          1259.2987922651628,
          1257.7938102445264,
          1248.0221550151487,
          1244.0206425867411,
          1247.8183234417297,
          1254.8525351460673,
          1250.8150098459118,
          1244.807363941169,
          1230.9441036685616,
          1233.3486553809437,
          1226.265812474739,
          1234.996984665012,
          1227.018459493707,
          1227.535733507706,
          1221.1107376738112,
          1238.3770633236259,
          1223.076076534341,
          1229.433787442076,
          1225.78835401732,
          1216.9127187474544,
          1223.9433673792093,
          1225.5128930923527,
          1223.3707226190438,
          1216.2319945921897,
          1212.1351102955575,
          1213.584926546325,
          1221.3862423404366,
          1212.028739835878,
          1212.9507991040377,
          1210.6052535664994,
          1203.5865783997378,
          1203.8088832233339,
          1214.3629074999294,
          1211.9613937644124,
          1209.8719358075136,
          1202.5022266388705,
          1207.7484144322113,
          1206.5554761961178,
          1203.2381122314503,
          1207.0474661120868,
          1206.5950504270086,
          1199.3253831788822,
          1194.561756563158,
          1198.9103977225552,
          1200.0323035448807,
          1186.051502338698,
          1193.046033480996,
          1186.8499058000018,
          1190.4080488420284,
          1193.680937184727,
          1193.832176818909,
          1180.8681008365893,
          1190.8108980811046,
          1181.326732732644,
          1180.331987410431,
          1193.3423256178955,
          1184.6913955532432,
          1186.9117489897562,
          1178.6385673307238,
          1177.6977724113947,
          1180.2392739922627,
          1178.5850333915055,
          1177.9082239800043,
          1182.5988478001789,
          1171.022423168402,
          1171.1766603667584,
          1177.6677673474471,
          1171.1466994850675,
          1173.0797350454932,
          1174.8499799674466,
          1176.2257972290456,
          1161.4605849476216,
          1172.8600124973368,
          1172.655177264599,
          1172.5018788596444,
          1163.07759992876,
          1170.2573466403273,
          1168.321161068583,
          1167.588100961915,
          1163.200491150037,
          1164.4123635270062,
          1172.9892252816308,
          1168.2314307727436,
          1155.9620655305357,
          1165.6345380313032,
          1166.2103143955535,
          1165.3026689705734,
          1161.4529357628662,
          1149.3413750996733,
          1163.0424187359793,
          1148.2640867965695,
          1160.4569989639078,
          1160.817125747647,
          1149.0349591555232,
          1161.7323333636205,
          1147.189802226022,
          1152.36639584208,
          1155.8677594949995,
          1149.6313675076683,
          1152.8155350884956,
          1149.0216557434094,
          1147.8751397432152,
          1154.3631142141442,
          1160.9115924548357,
          1146.9193450823705,
          1153.1337940069964,
          1143.9913537649059,
          1146.925222546394,
          1146.1679446149833,
          1137.6099872363457,
          1145.8682907835764,
          1149.4993709229934,
          1137.97558295122,
          1143.264276302958,
          1143.7791855993553,
          1142.6032645571847,
          1150.2068067649718,
          1139.2857665526071,
          1144.9645274246243,
          1135.8391415091346,
          1139.064871228671,
          1145.9895186713973,
          1145.450212887492,
          1144.259071804465,
          1142.0125499883682,
          1138.8742750193471,
          1135.4377483752678,
          1139.2248385842252,
          1135.210284287977,
          1138.7651106744725,
          1131.3467591176511,
          1130.878293132457,
          1124.9589110153195,
          1131.4777207849784,
          1133.7626175468326,
          1135.344189154497,
          1131.6424242336716,
          1130.7859341024946,
          1136.5035363184513,
          1127.7889161478047,
          1131.8138843365798,
          1130.3263509538863,
          1131.3043744546367,
          1129.6772947064712,
          1120.967390850978,
          1126.832791503025,
          1132.7453718290603,
          1128.1329782874927,
          1131.9956297995884,
          1114.7326307922083,
          1121.2655867903031,
          1116.7078975649188,
          1133.8237261851516,
          1125.037692017801,
          1132.4008636076846,
          1119.8801511860524,
          1119.0788097637842,
          1121.3614030757312,
          1131.4321932826128,
          1126.457277642574,
          1123.5579496681607,
          1122.3819397964003,
          1118.1007804862957,
          1122.3056029416719,
          1113.9624902187093,
          1125.1502237604882,
          1116.3050680793501,
          1120.8476670446678,
          1125.4804003837712,
          1112.9401681677048,
          1114.0511051879419,
          1111.171730434677,
          1114.7948210268573,
          1116.9861863990907,
          1116.70755930752,
          1115.2197537817074,
          1111.5898795119263,
          1108.64244408509,
          1107.9495906887203,
          1112.4572920109863,
          1114.4398456005144,
          1114.6391950614566,
          1119.13888461073,
          1110.082594677803,
          1112.5677138734538,
          1116.4533718821278,
          1106.3294388652685,
          1123.5130659294243,
          1112.5137356883756,
          1103.8551356818743,
          1109.006072135208,
          1109.9369864088035,
          1110.135375854174,
          1108.8261602480904,
          1102.1148231495638,
          1119.3542518164415,
          1103.6158561086952,
          1115.8411923246535,
          1102.8919439076756,
          1101.891604206092,
          1104.455495078219,
          1111.830066693912,
          1103.5504243405903,
          1100.245315187652,
          1108.7191777478868,
          1103.083256677322,
          1107.8526071839517,
          1097.099945478361,
          1100.5284753787582,
          1114.7025322723848,
          1106.4705858523175,
          1101.7386867231755,
          1111.1511081399528,
          1109.1939656877412,
          1105.6374219944512,
          1104.208758617131,
          1111.7959033575187,
          1108.754790776331,
          1101.7442791198523,
          1110.6791396400172,
          1096.0764007170596,
          1096.379091228013,
          1099.9574786290057,
          1103.24591033856,
          1101.4990483068477,
          1102.7079441704682,
          1098.833748360973,
          1091.31239197989,
          1101.1109418668225,
          1102.3702321103228,
          1101.7074794612477,
          1092.2492249379636,
          1099.078757014331,
          1101.2154302113229,
          1104.1839997127297,
          1094.3327208603698,
          1101.5056149651202,
          1089.5865486838045,
          1096.8044143532952,
          1098.8459482936885,
          1092.212463244279,
          1092.75562957831,
          1099.2370489444431,
          1084.348475249516,
          1091.443341971352,
          1090.7723705168594,
          1089.9523796049416,
          1091.875113649982,
          1096.7301402959217,
          1089.7073221675184,
          1093.3984913264724,
          1088.612077216192,
          1087.630484082834,
          1090.571531783418,
          1085.0058758484952,
          1094.0276110710113,
          1091.1150026200935,
          1086.8573703983873,
          1085.108108401179,
          1084.7308237746265,
          1090.1816821881423,
          1090.6295351556626,
          1086.4274791704524,
          1090.5423838062948,
          1091.8301099949713,
          1087.0218604504523,
          1088.624277516073,
          1093.1221395109708,
          1090.222458592536,
          1086.8481649701332,
          1089.2392892663504,
          1095.185687719712,
          1088.4245714639312,
          1092.4291602991998,
          1083.28928120829,
          1085.3490425203186,
          1083.0151794078668,
          1088.43854448813,
          1080.4585597113041,
          1084.4665938079631,
          1082.9132189983018,
          1084.9904671090146,
          1082.2925069536263,
          1074.876951387083,
          1075.281486760597,
          1082.745931046698,
          1087.3866766238705,
          1083.6745721737846,
          1083.1326852036975,
          1083.5306525971432,
          1084.6343496264114,
          1086.1410334594555,
          1086.2420728709096,
          1071.6851363716562,
          1081.3994540345916,
          1074.2338469914355,
          1085.1034517993855,
          1081.9954331701879,
          1086.4855920804248,
          1078.826578229373,
          1074.3908042502303,
          1084.979511402498,
          1078.8402972239542,
          1074.6623808816987,
          1079.1259288064985,
          1077.9139801325243,
          1079.390001675254,
          1077.1362126979366,
          1075.086608213582,
          1082.6206092138389,
          1072.8099895666232,
          1078.1173486856844,
          1076.5361254202332,
          1076.8167313995882,
          1074.518546071348,
          1069.6832898103428,
          1067.9371001322188,
          1074.357830212067,
          1079.7310567044628,
          1081.0968312779241,
          1073.9489756741552,
          1079.0749725164333,
          1074.632725506238,
          1078.9410947956876,
          1075.1612646738943,
          1079.690948945285,
          1075.535766968728,
          1078.6506381313093,
          1072.6332000799734,
          1070.885711786191,
          1078.7071434313198,
          1062.7337277605368,
          1069.230391057766,
          1069.421151182038,
          1067.0245214706674,
          1070.4521034841957
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          4211.758502085892,
          3044.078591398182,
          2087.8500410148417,
          1733.0884504056248,
          1513.6709177748867,
          1397.7589538219293,
          1401.9620745138532,
          1416.3254628782881,
          1380.7564400106291,
          1338.0344655136557,
          1565.4168876677018,
          1348.9568729037294,
          1267.4911594608873,
          1287.684045094394,
          1293.2388444085482,
          1247.6800997045443,
          1284.066405344325,
          1278.6029124454049,
          1216.558531356331,
          1228.8707241611394,
          1246.155129361731,
          1283.491519356577,
          1213.7959663876077,
          1223.9534186345436,
          1213.2346297639488,
          1221.143689804429,
          1196.701245675279,
          1223.336823764248,
          1189.4327336034628,
          1193.9933065227403,
          1196.9047433728083,
          1191.3469262514177,
          1195.361197316147,
          1194.8524266364032,
          1202.0405629098548,
          1182.6800507980906,
          1193.839714923841,
          1182.1221027699362,
          1190.9351251486287,
          1181.404355278803,
          1189.07922184594,
          1183.086489570148,
          1191.3201498211756,
          1175.4969099835353,
          1193.0940781236293,
          1179.945576516521,
          1176.0511788903102,
          1173.250205196597,
          1175.149573950866,
          1172.2704344528195,
          1167.8910401663466,
          1172.0712935663976,
          1171.4739337413996,
          1176.4927545015478,
          1176.091792224236,
          1176.2732083152525,
          1168.5738614244692,
          1175.0634918610654,
          1161.4109163806365,
          1170.0736935074162,
          1178.7694254084056,
          1161.9507412876997,
          1162.1773663527124,
          1158.4019452529703,
          1174.6434975075822,
          1171.7485010098521,
          1179.9511121004455,
          1160.8969828634529,
          1172.6219518648877,
          1171.9727752850772,
          1162.74030795498,
          1157.3877771904217,
          1153.9946450370574,
          1156.0119563306002,
          1150.160485132438,
          1169.890132582018,
          1169.3951052521713,
          1150.3464574788982,
          1147.3291131159765,
          1151.7557866762174,
          1143.927876957629,
          1159.0394936013322,
          1154.8556401673648,
          1147.2333867861125,
          1150.0566463772607,
          1146.0122712607467,
          1143.3662814088113,
          1142.2685155231727,
          1150.239196104207,
          1140.739928599515,
          1141.9211451238064,
          1151.7999295972263,
          1146.2543630773996,
          1140.9454165302443,
          1158.7142980930105,
          1133.3036998980554,
          1144.761972167675,
          1138.1912204887003,
          1144.1592867529796,
          1133.9313388638586,
          1138.9914379926872,
          1138.4003961324836,
          1141.9660449199168,
          1137.6565011167709,
          1136.4670335254475,
          1136.7217225969162,
          1138.2030037374518,
          1139.4561537977256,
          1144.267345905017,
          1140.9416306615951,
          1134.6621982290674,
          1129.5120163744093,
          1140.5651368607207,
          1133.5646722249332,
          1131.2124606985976,
          1137.1875771782024,
          1127.8513667028415,
          1131.128015261555,
          1131.0258906310705,
          1134.396185967304,
          1131.2854504230531,
          1134.1683883712888,
          1133.04828631101,
          1134.0210308765681,
          1128.8198167040948,
          1127.3915089633438,
          1139.1066762596422,
          1132.2057748339041,
          1133.2281281918927,
          1130.3800738120285,
          1127.9452161802326,
          1134.0369962635273,
          1126.2949012647728,
          1133.9120427956436,
          1127.9940044555296,
          1126.6357922933612,
          1130.8350693737311,
          1130.80506925428,
          1128.18699167151,
          1124.9474327832443,
          1122.5331504489798,
          1123.134545007449,
          1128.3801722858339,
          1125.9038968603525,
          1127.7030884547871,
          1125.0861719719317,
          1127.175638765282,
          1127.2894632245009,
          1125.6413840536175,
          1120.3513888796417,
          1129.9027662841356,
          1124.3794948164054,
          1121.8674722736146,
          1118.1292414966413,
          1116.197318325498,
          1116.942999503788,
          1124.3924601914196,
          1122.527435957513,
          1125.7734769580597,
          1115.2404186676374,
          1120.48954412821,
          1119.8055435342828,
          1116.5100536296716,
          1121.0385118985143,
          1120.595468090081,
          1120.137287734242,
          1121.7009016656198,
          1119.408495315359,
          1118.1358614673543,
          1118.0835873221547,
          1117.6340008680772,
          1119.9206120973124,
          1118.389582910685,
          1118.901862763106,
          1118.768966010847,
          1119.0240357840541,
          1117.4803178939069,
          1120.5501236564678,
          1117.7403159347116,
          1119.291233501048,
          1119.2622552768057,
          1114.6380393466372,
          1115.6121992375488,
          1118.698698647734,
          1117.9666477078686,
          1112.718932652631,
          1111.6495368671435,
          1111.5794150974173,
          1115.885638377555,
          1113.5378469127152,
          1104.4855251583806,
          1117.5031792621753,
          1109.3663192513425,
          1110.6397233882312,
          1114.0673880996842,
          1113.0254887316398,
          1116.294154401052,
          1113.7313785128442,
          1108.4159476504144,
          1109.0433857276967,
          1112.6017623604575,
          1107.7239518549009,
          1109.7509004613166,
          1106.2843682882135,
          1105.0982875055222,
          1104.890925806511,
          1109.6678708147042,
          1111.640186188191,
          1111.0710842330684,
          1105.2065559908888,
          1110.595544729774,
          1115.1379400650676,
          1105.4851804633454,
          1110.6385520321587,
          1110.3526000119844,
          1109.8593410004685,
          1103.085796826632,
          1106.7567526153937,
          1110.2228808574168,
          1105.3484869498586,
          1109.913288417072,
          1106.4406474411596,
          1111.0348811940341,
          1109.5659827367563,
          1108.6007949770967,
          1103.7909375166448,
          1105.3003895822499,
          1107.5922932094147,
          1108.5206840764504,
          1106.8678197835857,
          1109.996419499292,
          1107.0836586012304,
          1107.6036586727053,
          1104.246192321716,
          1109.4857087656421,
          1105.1481777915549,
          1108.5279805619612,
          1104.7619062492167,
          1109.3780824283763,
          1103.3750472419697,
          1105.7908026934483,
          1105.0768606776055,
          1107.7115299338827,
          1102.7102326400202,
          1105.1797515826304,
          1110.876288139201,
          1104.8257217544913,
          1103.8318947623195,
          1110.5638181488666,
          1101.3461681034753,
          1105.0479199532065,
          1103.084162572689,
          1103.0103290038476,
          1099.7696010359357,
          1104.0365255327915,
          1102.0441477431166,
          1098.869532733349,
          1102.9639771658265,
          1101.5874932784222,
          1102.1687525750544,
          1100.8183510281028,
          1102.9459404755098,
          1098.9409380649454,
          1103.3895606359738,
          1100.4512794789891,
          1099.5448888056212,
          1098.9545271563486,
          1108.4786797969643,
          1104.2196915593825,
          1106.8279540232338,
          1100.6350070975552,
          1102.7278442015647,
          1100.1085536261467,
          1103.571265049872,
          1099.6288211410213,
          1103.8457843420235,
          1099.4183849199324,
          1106.6234904354837,
          1102.6933292429458,
          1097.8389622344266,
          1100.2160912317338,
          1102.4410680170788,
          1097.461697336904,
          1098.2520201204773,
          1101.7106853796433,
          1101.5656925290912,
          1102.4811271268188,
          1100.243556170849,
          1099.5520693402457,
          1099.4420033346466,
          1096.0809027788657,
          1103.4938424376035,
          1103.82061918116,
          1105.3219778406278,
          1097.5913561539298,
          1095.5983727858447,
          1097.6176406067887,
          1097.5445326305426,
          1097.4540438685503,
          1098.8690908128713,
          1100.196956447114,
          1103.051337799568,
          1099.6746194157545,
          1101.9387586680448,
          1098.780417341925,
          1104.5815207429944,
          1097.5387644839473,
          1099.3153440644896,
          1097.2824436552614,
          1095.6775698113925,
          1096.4017158479232,
          1097.1251033693463,
          1097.161458806569,
          1098.4602768712323,
          1098.7540756357153,
          1099.7006120943752,
          1100.4269679291158,
          1095.433022048733,
          1096.3084029952486,
          1098.8646064729232,
          1097.7459965490352,
          1092.316751459067,
          1096.2614846697115,
          1098.6627716437738,
          1100.641113109906,
          1098.5009906861355,
          1098.6560262003475,
          1093.4999075232322,
          1096.4971779899222,
          1098.159168427756,
          1098.5000820003102,
          1099.1405405519195,
          1096.959386421297,
          1096.9590002610303,
          1097.6559196733774,
          1097.514569913024,
          1094.4624576144067,
          1095.9123186593738,
          1097.1377467891127,
          1101.3618461711387,
          1099.2711232799027,
          1098.0335325459475,
          1098.6628544518483,
          1100.503403404516,
          1095.356739754808,
          1095.6315955909192,
          1095.818284987256,
          1096.9199100767464,
          1091.722117593634,
          1093.84208319071,
          1097.138306936903,
          1091.6571887444466,
          1095.8091328469645,
          1094.7131030425198,
          1093.4344027381348,
          1095.4398706167285,
          1093.7926413117464,
          1094.2657725760807,
          1095.6046285772698,
          1098.0152180581622,
          1096.1609442117865,
          1095.581282011942,
          1093.7909655680942,
          1096.0575406212402,
          1094.8767719410312,
          1093.0491971753513,
          1093.9352590906278,
          1098.1902201583491,
          1096.2248198343036,
          1094.4743139046286,
          1093.6272615930325,
          1090.8507854063141,
          1091.7618136500605,
          1092.775607424029,
          1092.0122261238596,
          1095.4449722109516,
          1092.6114438800648,
          1092.9236927388163,
          1092.067346095943,
          1091.0814793022405,
          1092.68024189553,
          1094.2115427602,
          1093.1183494076788,
          1089.5999078806067,
          1092.733801634249,
          1091.6880182174827,
          1093.2759101887564,
          1093.1950009722543,
          1095.319939949337,
          1091.0114333644428
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039,
          1076.153028783039
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          399
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"af6c7e15-8095-4a82-8318-994e7a9ff21b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"af6c7e15-8095-4a82-8318-994e7a9ff21b\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'af6c7e15-8095-4a82-8318-994e7a9ff21b',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1502.6431976801794, 1495.8577816238235, 1474.2995325933364, 1446.5363193081307, 1426.163068949973, 1416.1177411856766, 1428.5048099668513, 1400.3266919527882, 1410.2137508939259, 1407.0759096453514, 1403.4561815064872, 1420.027697575219, 1370.478772273732, 1377.203589170711, 1365.4118956601617, 1355.4593743782664, 1340.7046685885255, 1353.7416225301017, 1356.171337934685, 1353.029233330879, 1340.2140435983165, 1337.717656042429, 1339.1804594778455, 1332.0836916216538, 1321.4757720414875, 1320.8164018195355, 1322.6376319152453, 1330.9539032441378, 1321.8941499118412, 1306.1989489567407, 1306.3046355827887, 1297.2693904542434, 1299.974275523588, 1301.2207547974342, 1300.2285088201982, 1291.7243503274528, 1293.939782007438, 1294.2146667027632, 1295.8089019243, 1290.9434617897157, 1280.8360957973152, 1282.4972018801618, 1288.6097356544985, 1267.8269162909503, 1273.312781591513, 1268.8361531240992, 1264.7603367632798, 1267.3289908253646, 1263.829663252386, 1260.131970466964, 1260.0956562746735, 1259.1481023757472, 1255.4811962518563, 1256.7782727278425, 1259.2987922651628, 1257.7938102445264, 1248.0221550151487, 1244.0206425867411, 1247.8183234417297, 1254.8525351460673, 1250.8150098459118, 1244.807363941169, 1230.9441036685616, 1233.3486553809437, 1226.265812474739, 1234.996984665012, 1227.018459493707, 1227.535733507706, 1221.1107376738112, 1238.3770633236259, 1223.076076534341, 1229.433787442076, 1225.78835401732, 1216.9127187474544, 1223.9433673792093, 1225.5128930923527, 1223.3707226190438, 1216.2319945921897, 1212.1351102955575, 1213.584926546325, 1221.3862423404366, 1212.028739835878, 1212.9507991040377, 1210.6052535664994, 1203.5865783997378, 1203.8088832233339, 1214.3629074999294, 1211.9613937644124, 1209.8719358075136, 1202.5022266388705, 1207.7484144322113, 1206.5554761961178, 1203.2381122314503, 1207.0474661120868, 1206.5950504270086, 1199.3253831788822, 1194.561756563158, 1198.9103977225552, 1200.0323035448807, 1186.051502338698, 1193.046033480996, 1186.8499058000018, 1190.4080488420284, 1193.680937184727, 1193.832176818909, 1180.8681008365893, 1190.8108980811046, 1181.326732732644, 1180.331987410431, 1193.3423256178955, 1184.6913955532432, 1186.9117489897562, 1178.6385673307238, 1177.6977724113947, 1180.2392739922627, 1178.5850333915055, 1177.9082239800043, 1182.5988478001789, 1171.022423168402, 1171.1766603667584, 1177.6677673474471, 1171.1466994850675, 1173.0797350454932, 1174.8499799674466, 1176.2257972290456, 1161.4605849476216, 1172.8600124973368, 1172.655177264599, 1172.5018788596444, 1163.07759992876, 1170.2573466403273, 1168.321161068583, 1167.588100961915, 1163.200491150037, 1164.4123635270062, 1172.9892252816308, 1168.2314307727436, 1155.9620655305357, 1165.6345380313032, 1166.2103143955535, 1165.3026689705734, 1161.4529357628662, 1149.3413750996733, 1163.0424187359793, 1148.2640867965695, 1160.4569989639078, 1160.817125747647, 1149.0349591555232, 1161.7323333636205, 1147.189802226022, 1152.36639584208, 1155.8677594949995, 1149.6313675076683, 1152.8155350884956, 1149.0216557434094, 1147.8751397432152, 1154.3631142141442, 1160.9115924548357, 1146.9193450823705, 1153.1337940069964, 1143.9913537649059, 1146.925222546394, 1146.1679446149833, 1137.6099872363457, 1145.8682907835764, 1149.4993709229934, 1137.97558295122, 1143.264276302958, 1143.7791855993553, 1142.6032645571847, 1150.2068067649718, 1139.2857665526071, 1144.9645274246243, 1135.8391415091346, 1139.064871228671, 1145.9895186713973, 1145.450212887492, 1144.259071804465, 1142.0125499883682, 1138.8742750193471, 1135.4377483752678, 1139.2248385842252, 1135.210284287977, 1138.7651106744725, 1131.3467591176511, 1130.878293132457, 1124.9589110153195, 1131.4777207849784, 1133.7626175468326, 1135.344189154497, 1131.6424242336716, 1130.7859341024946, 1136.5035363184513, 1127.7889161478047, 1131.8138843365798, 1130.3263509538863, 1131.3043744546367, 1129.6772947064712, 1120.967390850978, 1126.832791503025, 1132.7453718290603, 1128.1329782874927, 1131.9956297995884, 1114.7326307922083, 1121.2655867903031, 1116.7078975649188, 1133.8237261851516, 1125.037692017801, 1132.4008636076846, 1119.8801511860524, 1119.0788097637842, 1121.3614030757312, 1131.4321932826128, 1126.457277642574, 1123.5579496681607, 1122.3819397964003, 1118.1007804862957, 1122.3056029416719, 1113.9624902187093, 1125.1502237604882, 1116.3050680793501, 1120.8476670446678, 1125.4804003837712, 1112.9401681677048, 1114.0511051879419, 1111.171730434677, 1114.7948210268573, 1116.9861863990907, 1116.70755930752, 1115.2197537817074, 1111.5898795119263, 1108.64244408509, 1107.9495906887203, 1112.4572920109863, 1114.4398456005144, 1114.6391950614566, 1119.13888461073, 1110.082594677803, 1112.5677138734538, 1116.4533718821278, 1106.3294388652685, 1123.5130659294243, 1112.5137356883756, 1103.8551356818743, 1109.006072135208, 1109.9369864088035, 1110.135375854174, 1108.8261602480904, 1102.1148231495638, 1119.3542518164415, 1103.6158561086952, 1115.8411923246535, 1102.8919439076756, 1101.891604206092, 1104.455495078219, 1111.830066693912, 1103.5504243405903, 1100.245315187652, 1108.7191777478868, 1103.083256677322, 1107.8526071839517, 1097.099945478361, 1100.5284753787582, 1114.7025322723848, 1106.4705858523175, 1101.7386867231755, 1111.1511081399528, 1109.1939656877412, 1105.6374219944512, 1104.208758617131, 1111.7959033575187, 1108.754790776331, 1101.7442791198523, 1110.6791396400172, 1096.0764007170596, 1096.379091228013, 1099.9574786290057, 1103.24591033856, 1101.4990483068477, 1102.7079441704682, 1098.833748360973, 1091.31239197989, 1101.1109418668225, 1102.3702321103228, 1101.7074794612477, 1092.2492249379636, 1099.078757014331, 1101.2154302113229, 1104.1839997127297, 1094.3327208603698, 1101.5056149651202, 1089.5865486838045, 1096.8044143532952, 1098.8459482936885, 1092.212463244279, 1092.75562957831, 1099.2370489444431, 1084.348475249516, 1091.443341971352, 1090.7723705168594, 1089.9523796049416, 1091.875113649982, 1096.7301402959217, 1089.7073221675184, 1093.3984913264724, 1088.612077216192, 1087.630484082834, 1090.571531783418, 1085.0058758484952, 1094.0276110710113, 1091.1150026200935, 1086.8573703983873, 1085.108108401179, 1084.7308237746265, 1090.1816821881423, 1090.6295351556626, 1086.4274791704524, 1090.5423838062948, 1091.8301099949713, 1087.0218604504523, 1088.624277516073, 1093.1221395109708, 1090.222458592536, 1086.8481649701332, 1089.2392892663504, 1095.185687719712, 1088.4245714639312, 1092.4291602991998, 1083.28928120829, 1085.3490425203186, 1083.0151794078668, 1088.43854448813, 1080.4585597113041, 1084.4665938079631, 1082.9132189983018, 1084.9904671090146, 1082.2925069536263, 1074.876951387083, 1075.281486760597, 1082.745931046698, 1087.3866766238705, 1083.6745721737846, 1083.1326852036975, 1083.5306525971432, 1084.6343496264114, 1086.1410334594555, 1086.2420728709096, 1071.6851363716562, 1081.3994540345916, 1074.2338469914355, 1085.1034517993855, 1081.9954331701879, 1086.4855920804248, 1078.826578229373, 1074.3908042502303, 1084.979511402498, 1078.8402972239542, 1074.6623808816987, 1079.1259288064985, 1077.9139801325243, 1079.390001675254, 1077.1362126979366, 1075.086608213582, 1082.6206092138389, 1072.8099895666232, 1078.1173486856844, 1076.5361254202332, 1076.8167313995882, 1074.518546071348, 1069.6832898103428, 1067.9371001322188, 1074.357830212067, 1079.7310567044628, 1081.0968312779241, 1073.9489756741552, 1079.0749725164333, 1074.632725506238, 1078.9410947956876, 1075.1612646738943, 1079.690948945285, 1075.535766968728, 1078.6506381313093, 1072.6332000799734, 1070.885711786191, 1078.7071434313198, 1062.7337277605368, 1069.230391057766, 1069.421151182038, 1067.0245214706674, 1070.4521034841957]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [4211.758502085892, 3044.078591398182, 2087.8500410148417, 1733.0884504056248, 1513.6709177748867, 1397.7589538219293, 1401.9620745138532, 1416.3254628782881, 1380.7564400106291, 1338.0344655136557, 1565.4168876677018, 1348.9568729037294, 1267.4911594608873, 1287.684045094394, 1293.2388444085482, 1247.6800997045443, 1284.066405344325, 1278.6029124454049, 1216.558531356331, 1228.8707241611394, 1246.155129361731, 1283.491519356577, 1213.7959663876077, 1223.9534186345436, 1213.2346297639488, 1221.143689804429, 1196.701245675279, 1223.336823764248, 1189.4327336034628, 1193.9933065227403, 1196.9047433728083, 1191.3469262514177, 1195.361197316147, 1194.8524266364032, 1202.0405629098548, 1182.6800507980906, 1193.839714923841, 1182.1221027699362, 1190.9351251486287, 1181.404355278803, 1189.07922184594, 1183.086489570148, 1191.3201498211756, 1175.4969099835353, 1193.0940781236293, 1179.945576516521, 1176.0511788903102, 1173.250205196597, 1175.149573950866, 1172.2704344528195, 1167.8910401663466, 1172.0712935663976, 1171.4739337413996, 1176.4927545015478, 1176.091792224236, 1176.2732083152525, 1168.5738614244692, 1175.0634918610654, 1161.4109163806365, 1170.0736935074162, 1178.7694254084056, 1161.9507412876997, 1162.1773663527124, 1158.4019452529703, 1174.6434975075822, 1171.7485010098521, 1179.9511121004455, 1160.8969828634529, 1172.6219518648877, 1171.9727752850772, 1162.74030795498, 1157.3877771904217, 1153.9946450370574, 1156.0119563306002, 1150.160485132438, 1169.890132582018, 1169.3951052521713, 1150.3464574788982, 1147.3291131159765, 1151.7557866762174, 1143.927876957629, 1159.0394936013322, 1154.8556401673648, 1147.2333867861125, 1150.0566463772607, 1146.0122712607467, 1143.3662814088113, 1142.2685155231727, 1150.239196104207, 1140.739928599515, 1141.9211451238064, 1151.7999295972263, 1146.2543630773996, 1140.9454165302443, 1158.7142980930105, 1133.3036998980554, 1144.761972167675, 1138.1912204887003, 1144.1592867529796, 1133.9313388638586, 1138.9914379926872, 1138.4003961324836, 1141.9660449199168, 1137.6565011167709, 1136.4670335254475, 1136.7217225969162, 1138.2030037374518, 1139.4561537977256, 1144.267345905017, 1140.9416306615951, 1134.6621982290674, 1129.5120163744093, 1140.5651368607207, 1133.5646722249332, 1131.2124606985976, 1137.1875771782024, 1127.8513667028415, 1131.128015261555, 1131.0258906310705, 1134.396185967304, 1131.2854504230531, 1134.1683883712888, 1133.04828631101, 1134.0210308765681, 1128.8198167040948, 1127.3915089633438, 1139.1066762596422, 1132.2057748339041, 1133.2281281918927, 1130.3800738120285, 1127.9452161802326, 1134.0369962635273, 1126.2949012647728, 1133.9120427956436, 1127.9940044555296, 1126.6357922933612, 1130.8350693737311, 1130.80506925428, 1128.18699167151, 1124.9474327832443, 1122.5331504489798, 1123.134545007449, 1128.3801722858339, 1125.9038968603525, 1127.7030884547871, 1125.0861719719317, 1127.175638765282, 1127.2894632245009, 1125.6413840536175, 1120.3513888796417, 1129.9027662841356, 1124.3794948164054, 1121.8674722736146, 1118.1292414966413, 1116.197318325498, 1116.942999503788, 1124.3924601914196, 1122.527435957513, 1125.7734769580597, 1115.2404186676374, 1120.48954412821, 1119.8055435342828, 1116.5100536296716, 1121.0385118985143, 1120.595468090081, 1120.137287734242, 1121.7009016656198, 1119.408495315359, 1118.1358614673543, 1118.0835873221547, 1117.6340008680772, 1119.9206120973124, 1118.389582910685, 1118.901862763106, 1118.768966010847, 1119.0240357840541, 1117.4803178939069, 1120.5501236564678, 1117.7403159347116, 1119.291233501048, 1119.2622552768057, 1114.6380393466372, 1115.6121992375488, 1118.698698647734, 1117.9666477078686, 1112.718932652631, 1111.6495368671435, 1111.5794150974173, 1115.885638377555, 1113.5378469127152, 1104.4855251583806, 1117.5031792621753, 1109.3663192513425, 1110.6397233882312, 1114.0673880996842, 1113.0254887316398, 1116.294154401052, 1113.7313785128442, 1108.4159476504144, 1109.0433857276967, 1112.6017623604575, 1107.7239518549009, 1109.7509004613166, 1106.2843682882135, 1105.0982875055222, 1104.890925806511, 1109.6678708147042, 1111.640186188191, 1111.0710842330684, 1105.2065559908888, 1110.595544729774, 1115.1379400650676, 1105.4851804633454, 1110.6385520321587, 1110.3526000119844, 1109.8593410004685, 1103.085796826632, 1106.7567526153937, 1110.2228808574168, 1105.3484869498586, 1109.913288417072, 1106.4406474411596, 1111.0348811940341, 1109.5659827367563, 1108.6007949770967, 1103.7909375166448, 1105.3003895822499, 1107.5922932094147, 1108.5206840764504, 1106.8678197835857, 1109.996419499292, 1107.0836586012304, 1107.6036586727053, 1104.246192321716, 1109.4857087656421, 1105.1481777915549, 1108.5279805619612, 1104.7619062492167, 1109.3780824283763, 1103.3750472419697, 1105.7908026934483, 1105.0768606776055, 1107.7115299338827, 1102.7102326400202, 1105.1797515826304, 1110.876288139201, 1104.8257217544913, 1103.8318947623195, 1110.5638181488666, 1101.3461681034753, 1105.0479199532065, 1103.084162572689, 1103.0103290038476, 1099.7696010359357, 1104.0365255327915, 1102.0441477431166, 1098.869532733349, 1102.9639771658265, 1101.5874932784222, 1102.1687525750544, 1100.8183510281028, 1102.9459404755098, 1098.9409380649454, 1103.3895606359738, 1100.4512794789891, 1099.5448888056212, 1098.9545271563486, 1108.4786797969643, 1104.2196915593825, 1106.8279540232338, 1100.6350070975552, 1102.7278442015647, 1100.1085536261467, 1103.571265049872, 1099.6288211410213, 1103.8457843420235, 1099.4183849199324, 1106.6234904354837, 1102.6933292429458, 1097.8389622344266, 1100.2160912317338, 1102.4410680170788, 1097.461697336904, 1098.2520201204773, 1101.7106853796433, 1101.5656925290912, 1102.4811271268188, 1100.243556170849, 1099.5520693402457, 1099.4420033346466, 1096.0809027788657, 1103.4938424376035, 1103.82061918116, 1105.3219778406278, 1097.5913561539298, 1095.5983727858447, 1097.6176406067887, 1097.5445326305426, 1097.4540438685503, 1098.8690908128713, 1100.196956447114, 1103.051337799568, 1099.6746194157545, 1101.9387586680448, 1098.780417341925, 1104.5815207429944, 1097.5387644839473, 1099.3153440644896, 1097.2824436552614, 1095.6775698113925, 1096.4017158479232, 1097.1251033693463, 1097.161458806569, 1098.4602768712323, 1098.7540756357153, 1099.7006120943752, 1100.4269679291158, 1095.433022048733, 1096.3084029952486, 1098.8646064729232, 1097.7459965490352, 1092.316751459067, 1096.2614846697115, 1098.6627716437738, 1100.641113109906, 1098.5009906861355, 1098.6560262003475, 1093.4999075232322, 1096.4971779899222, 1098.159168427756, 1098.5000820003102, 1099.1405405519195, 1096.959386421297, 1096.9590002610303, 1097.6559196733774, 1097.514569913024, 1094.4624576144067, 1095.9123186593738, 1097.1377467891127, 1101.3618461711387, 1099.2711232799027, 1098.0335325459475, 1098.6628544518483, 1100.503403404516, 1095.356739754808, 1095.6315955909192, 1095.818284987256, 1096.9199100767464, 1091.722117593634, 1093.84208319071, 1097.138306936903, 1091.6571887444466, 1095.8091328469645, 1094.7131030425198, 1093.4344027381348, 1095.4398706167285, 1093.7926413117464, 1094.2657725760807, 1095.6046285772698, 1098.0152180581622, 1096.1609442117865, 1095.581282011942, 1093.7909655680942, 1096.0575406212402, 1094.8767719410312, 1093.0491971753513, 1093.9352590906278, 1098.1902201583491, 1096.2248198343036, 1094.4743139046286, 1093.6272615930325, 1090.8507854063141, 1091.7618136500605, 1092.775607424029, 1092.0122261238596, 1095.4449722109516, 1092.6114438800648, 1092.9236927388163, 1092.067346095943, 1091.0814793022405, 1092.68024189553, 1094.2115427602, 1093.1183494076788, 1089.5999078806067, 1092.733801634249, 1091.6880182174827, 1093.2759101887564, 1093.1950009722543, 1095.319939949337, 1091.0114333644428]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039, 1076.153028783039]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 399], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('af6c7e15-8095-4a82-8318-994e7a9ff21b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.11% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1091   : Mean absolute error \n",
      "\n",
      "9.33% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.1),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Callbacks\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 11194.7606\n",
      "Epoch 00001: val_loss improved from inf to 11105.48480, saving model to Weights\\Weights-001--11105.48480.hdf5\n",
      "19948/19948 [==============================] - 4s 183us/sample - loss: 11192.1923 - val_loss: 11105.4848\n",
      "Epoch 2/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 11186.7066- ETA: 0s - loss: 112\n",
      "Epoch 00002: val_loss improved from 11105.48480 to 11046.21975, saving model to Weights\\Weights-002--11046.21975.hdf5\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 11178.5528 - val_loss: 11046.2197\n",
      "Epoch 3/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 11144.768 - ETA: 0s - loss: 11141.2362\n",
      "Epoch 00003: val_loss improved from 11046.21975 to 10986.67571, saving model to Weights\\Weights-003--10986.67571.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 11150.9024 - val_loss: 10986.6757\n",
      "Epoch 4/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 11104.8680- ETA: 0s - loss: 11110.\n",
      "Epoch 00004: val_loss improved from 10986.67571 to 10918.13267, saving model to Weights\\Weights-004--10918.13267.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 11107.9432 - val_loss: 10918.1327\n",
      "Epoch 5/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 11050.3148\n",
      "Epoch 00005: val_loss improved from 10918.13267 to 10842.58990, saving model to Weights\\Weights-005--10842.58990.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 11047.6094 - val_loss: 10842.5899\n",
      "Epoch 6/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 10962.5222\n",
      "Epoch 00006: val_loss improved from 10842.58990 to 10713.50145, saving model to Weights\\Weights-006--10713.50145.hdf5\n",
      "19948/19948 [==============================] - 1s 48us/sample - loss: 10966.0617 - val_loss: 10713.5015\n",
      "Epoch 7/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 10870.5076\n",
      "Epoch 00007: val_loss improved from 10713.50145 to 10568.64516, saving model to Weights\\Weights-007--10568.64516.hdf5\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 10858.1868 - val_loss: 10568.6452\n",
      "Epoch 8/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 10723.0385\n",
      "Epoch 00008: val_loss improved from 10568.64516 to 10406.24041, saving model to Weights\\Weights-008--10406.24041.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 10718.8955 - val_loss: 10406.2404\n",
      "Epoch 9/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 10544.9025\n",
      "Epoch 00009: val_loss improved from 10406.24041 to 10182.92115, saving model to Weights\\Weights-009--10182.92115.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 10542.4472 - val_loss: 10182.9211\n",
      "Epoch 10/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 10326.4558\n",
      "Epoch 00010: val_loss improved from 10182.92115 to 9890.86608, saving model to Weights\\Weights-010--9890.86608.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 10325.3528 - val_loss: 9890.8661\n",
      "Epoch 11/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 10066.4338\n",
      "Epoch 00011: val_loss improved from 9890.86608 to 9519.09641, saving model to Weights\\Weights-011--9519.09641.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 10061.4298 - val_loss: 9519.0964\n",
      "Epoch 12/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 9782.9828\n",
      "Epoch 00012: val_loss improved from 9519.09641 to 9180.08806, saving model to Weights\\Weights-012--9180.08806.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 9747.5126 - val_loss: 9180.0881\n",
      "Epoch 13/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 9379.3189\n",
      "Epoch 00013: val_loss improved from 9180.08806 to 8717.61901, saving model to Weights\\Weights-013--8717.61901.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 9379.7952 - val_loss: 8717.6190\n",
      "Epoch 14/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 8970.8809\n",
      "Epoch 00014: val_loss improved from 8717.61901 to 8255.42723, saving model to Weights\\Weights-014--8255.42723.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 8953.9812 - val_loss: 8255.4272\n",
      "Epoch 15/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 8474.5672\n",
      "Epoch 00015: val_loss improved from 8255.42723 to 7774.80910, saving model to Weights\\Weights-015--7774.80910.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 8467.3987 - val_loss: 7774.8091\n",
      "Epoch 16/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 7936.8963\n",
      "Epoch 00016: val_loss improved from 7774.80910 to 7044.84457, saving model to Weights\\Weights-016--7044.84457.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 7915.4261 - val_loss: 7044.8446\n",
      "Epoch 17/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 7300.0880\n",
      "Epoch 00017: val_loss improved from 7044.84457 to 6429.23204, saving model to Weights\\Weights-017--6429.23204.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 7292.5825 - val_loss: 6429.2320\n",
      "Epoch 18/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 6680.5887\n",
      "Epoch 00018: val_loss improved from 6429.23204 to 5659.09859, saving model to Weights\\Weights-018--5659.09859.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 6605.0404 - val_loss: 5659.0986\n",
      "Epoch 19/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 5847.0405\n",
      "Epoch 00019: val_loss improved from 5659.09859 to 4875.59722, saving model to Weights\\Weights-019--4875.59722.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 5837.1935 - val_loss: 4875.5972\n",
      "Epoch 20/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 5040.9695\n",
      "Epoch 00020: val_loss improved from 4875.59722 to 3413.29319, saving model to Weights\\Weights-020--3413.29319.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 5015.7334 - val_loss: 3413.2932\n",
      "Epoch 21/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 4195.0634\n",
      "Epoch 00021: val_loss improved from 3413.29319 to 2871.93549, saving model to Weights\\Weights-021--2871.93549.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 4167.7398 - val_loss: 2871.9355\n",
      "Epoch 22/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 3356.7984\n",
      "Epoch 00022: val_loss improved from 2871.93549 to 2386.77112, saving model to Weights\\Weights-022--2386.77112.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 3340.5908 - val_loss: 2386.7711\n",
      "Epoch 23/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 2590.9663\n",
      "Epoch 00023: val_loss improved from 2386.77112 to 1778.22912, saving model to Weights\\Weights-023--1778.22912.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 2542.0127 - val_loss: 1778.2291\n",
      "Epoch 24/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1934.3438\n",
      "Epoch 00024: val_loss improved from 1778.22912 to 1536.23979, saving model to Weights\\Weights-024--1536.23979.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1926.6901 - val_loss: 1536.2398\n",
      "Epoch 25/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1591.5854\n",
      "Epoch 00025: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1583.9124 - val_loss: 2068.3111\n",
      "Epoch 26/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1473.3150\n",
      "Epoch 00026: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1473.2194 - val_loss: 2014.2340\n",
      "Epoch 27/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1428.8057\n",
      "Epoch 00027: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1431.7824 - val_loss: 2460.1052\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1412.9166\n",
      "Epoch 00028: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1412.5230 - val_loss: 2082.8408\n",
      "Epoch 29/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1397.3998\n",
      "Epoch 00029: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1401.1522 - val_loss: 1742.0741\n",
      "Epoch 30/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1396.5089\n",
      "Epoch 00030: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1393.9749 - val_loss: 1569.2119\n",
      "Epoch 31/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1394.9883\n",
      "Epoch 00031: val_loss did not improve from 1536.23979\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1388.8545 - val_loss: 1592.5279\n",
      "Epoch 32/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1373.4311\n",
      "Epoch 00032: val_loss improved from 1536.23979 to 1472.04586, saving model to Weights\\Weights-032--1472.04586.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1371.0415 - val_loss: 1472.0459\n",
      "Epoch 33/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1360.8498\n",
      "Epoch 00033: val_loss did not improve from 1472.04586\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1364.3957 - val_loss: 1521.0490\n",
      "Epoch 34/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1377.9024\n",
      "Epoch 00034: val_loss improved from 1472.04586 to 1368.93658, saving model to Weights\\Weights-034--1368.93658.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1364.0659 - val_loss: 1368.9366\n",
      "Epoch 35/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1359.6976\n",
      "Epoch 00035: val_loss improved from 1368.93658 to 1364.03520, saving model to Weights\\Weights-035--1364.03520.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1358.0316 - val_loss: 1364.0352\n",
      "Epoch 36/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1339.8585\n",
      "Epoch 00036: val_loss improved from 1364.03520 to 1302.05873, saving model to Weights\\Weights-036--1302.05873.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1340.2357 - val_loss: 1302.0587\n",
      "Epoch 37/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1350.2199\n",
      "Epoch 00037: val_loss improved from 1302.05873 to 1273.25948, saving model to Weights\\Weights-037--1273.25948.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1343.5695 - val_loss: 1273.2595\n",
      "Epoch 38/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1338.4283\n",
      "Epoch 00038: val_loss did not improve from 1273.25948\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1342.0134 - val_loss: 1316.8741\n",
      "Epoch 39/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1335.1345\n",
      "Epoch 00039: val_loss improved from 1273.25948 to 1247.86951, saving model to Weights\\Weights-039--1247.86951.hdf5\n",
      "19948/19948 [==============================] - 1s 50us/sample - loss: 1334.7023 - val_loss: 1247.8695\n",
      "Epoch 40/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1332.8352\n",
      "Epoch 00040: val_loss improved from 1247.86951 to 1242.67336, saving model to Weights\\Weights-040--1242.67336.hdf5\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1331.0930 - val_loss: 1242.6734\n",
      "Epoch 41/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1332.1277\n",
      "Epoch 00041: val_loss did not improve from 1242.67336\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1330.1067 - val_loss: 1247.0179\n",
      "Epoch 42/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1321.4730\n",
      "Epoch 00042: val_loss improved from 1242.67336 to 1233.53026, saving model to Weights\\Weights-042--1233.53026.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1318.7343 - val_loss: 1233.5303\n",
      "Epoch 43/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1310.1705\n",
      "Epoch 00043: val_loss improved from 1233.53026 to 1232.79035, saving model to Weights\\Weights-043--1232.79035.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1315.5775 - val_loss: 1232.7903\n",
      "Epoch 44/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1316.1541\n",
      "Epoch 00044: val_loss improved from 1232.79035 to 1218.36005, saving model to Weights\\Weights-044--1218.36005.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1316.7092 - val_loss: 1218.3600\n",
      "Epoch 45/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1303.8438\n",
      "Epoch 00045: val_loss did not improve from 1218.36005\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1305.1755 - val_loss: 1220.2915\n",
      "Epoch 46/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1311.3223\n",
      "Epoch 00046: val_loss did not improve from 1218.36005\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1306.1208 - val_loss: 1228.0044\n",
      "Epoch 47/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1296.7531\n",
      "Epoch 00047: val_loss improved from 1218.36005 to 1216.72923, saving model to Weights\\Weights-047--1216.72923.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1296.9791 - val_loss: 1216.7292\n",
      "Epoch 48/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1304.8751\n",
      "Epoch 00048: val_loss improved from 1216.72923 to 1210.87494, saving model to Weights\\Weights-048--1210.87494.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1303.6745 - val_loss: 1210.8749\n",
      "Epoch 49/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1294.7699\n",
      "Epoch 00049: val_loss improved from 1210.87494 to 1208.82202, saving model to Weights\\Weights-049--1208.82202.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1294.5705 - val_loss: 1208.8220\n",
      "Epoch 50/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1299.5730\n",
      "Epoch 00050: val_loss did not improve from 1208.82202\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1300.1430 - val_loss: 1209.7664\n",
      "Epoch 51/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1299.5851\n",
      "Epoch 00051: val_loss improved from 1208.82202 to 1204.50274, saving model to Weights\\Weights-051--1204.50274.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1298.2662 - val_loss: 1204.5027\n",
      "Epoch 52/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1290.2668\n",
      "Epoch 00052: val_loss improved from 1204.50274 to 1196.97430, saving model to Weights\\Weights-052--1196.97430.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1293.7285 - val_loss: 1196.9743\n",
      "Epoch 53/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1291.6278\n",
      "Epoch 00053: val_loss did not improve from 1196.97430\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1292.2140 - val_loss: 1200.7647\n",
      "Epoch 54/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1280.3099\n",
      "Epoch 00054: val_loss did not improve from 1196.97430\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1280.9863 - val_loss: 1201.5422\n",
      "Epoch 55/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1284.4834\n",
      "Epoch 00055: val_loss improved from 1196.97430 to 1195.38757, saving model to Weights\\Weights-055--1195.38757.hdf5\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1286.6320 - val_loss: 1195.3876\n",
      "Epoch 56/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1286.0517\n",
      "Epoch 00056: val_loss improved from 1195.38757 to 1193.75896, saving model to Weights\\Weights-056--1193.75896.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1285.8442 - val_loss: 1193.7590\n",
      "Epoch 57/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1279.1039\n",
      "Epoch 00057: val_loss improved from 1193.75896 to 1193.43730, saving model to Weights\\Weights-057--1193.43730.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1287.7338 - val_loss: 1193.4373\n",
      "Epoch 58/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1278.8764\n",
      "Epoch 00058: val_loss did not improve from 1193.43730\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1281.0132 - val_loss: 1202.3001\n",
      "Epoch 59/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1289.4134\n",
      "Epoch 00059: val_loss did not improve from 1193.43730\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1282.8448 - val_loss: 1198.1756\n",
      "Epoch 60/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1275.9570\n",
      "Epoch 00060: val_loss improved from 1193.43730 to 1184.75829, saving model to Weights\\Weights-060--1184.75829.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1274.2041 - val_loss: 1184.7583\n",
      "Epoch 61/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1265.5445\n",
      "Epoch 00061: val_loss did not improve from 1184.75829\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1266.0912 - val_loss: 1188.3892\n",
      "Epoch 62/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1275.3057\n",
      "Epoch 00062: val_loss improved from 1184.75829 to 1176.95199, saving model to Weights\\Weights-062--1176.95199.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1278.6921 - val_loss: 1176.9520\n",
      "Epoch 63/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1257.8309\n",
      "Epoch 00063: val_loss did not improve from 1176.95199\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1255.9582 - val_loss: 1185.2117\n",
      "Epoch 64/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1272.5829\n",
      "Epoch 00064: val_loss did not improve from 1176.95199\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1271.2633 - val_loss: 1185.2976\n",
      "Epoch 65/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1267.3742\n",
      "Epoch 00065: val_loss did not improve from 1176.95199\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1262.5759 - val_loss: 1181.7302\n",
      "Epoch 66/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1269.6854\n",
      "Epoch 00066: val_loss improved from 1176.95199 to 1169.26350, saving model to Weights\\Weights-066--1169.26350.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1266.5821 - val_loss: 1169.2635\n",
      "Epoch 67/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1246.4048\n",
      "Epoch 00067: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1251.7666 - val_loss: 1178.4428\n",
      "Epoch 68/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1267.5605\n",
      "Epoch 00068: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1264.1990 - val_loss: 1190.3658\n",
      "Epoch 69/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1255.5574\n",
      "Epoch 00069: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1256.7478 - val_loss: 1177.3648\n",
      "Epoch 70/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1258.5510\n",
      "Epoch 00070: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1259.4415 - val_loss: 1172.0375\n",
      "Epoch 71/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1261.2646\n",
      "Epoch 00071: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1260.1660 - val_loss: 1174.9418\n",
      "Epoch 72/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1254.0379\n",
      "Epoch 00072: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1255.3622 - val_loss: 1183.5860\n",
      "Epoch 73/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1243.2480\n",
      "Epoch 00073: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1243.0831 - val_loss: 1177.3919\n",
      "Epoch 74/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1252.7082\n",
      "Epoch 00074: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1255.0945 - val_loss: 1173.2199\n",
      "Epoch 75/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1245.5901\n",
      "Epoch 00075: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1246.3408 - val_loss: 1172.9969\n",
      "Epoch 76/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1238.5200\n",
      "Epoch 00076: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1241.2350 - val_loss: 1193.5521\n",
      "Epoch 77/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1236.1777\n",
      "Epoch 00077: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1236.3335 - val_loss: 1179.9658\n",
      "Epoch 78/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1238.8039\n",
      "Epoch 00078: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1241.2273 - val_loss: 1175.6437\n",
      "Epoch 79/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1237.8250\n",
      "Epoch 00079: val_loss did not improve from 1169.26350\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1237.5763 - val_loss: 1171.7808\n",
      "Epoch 80/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1246.0114\n",
      "Epoch 00080: val_loss improved from 1169.26350 to 1163.35959, saving model to Weights\\Weights-080--1163.35959.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1244.4751 - val_loss: 1163.3596\n",
      "Epoch 81/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1235.1513\n",
      "Epoch 00081: val_loss did not improve from 1163.35959\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1233.1928 - val_loss: 1167.4541\n",
      "Epoch 82/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1234.4637\n",
      "Epoch 00082: val_loss did not improve from 1163.35959\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1238.0655 - val_loss: 1182.4528\n",
      "Epoch 83/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1238.5251\n",
      "Epoch 00083: val_loss did not improve from 1163.35959\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1238.0743 - val_loss: 1167.9984\n",
      "Epoch 84/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1235.7666\n",
      "Epoch 00084: val_loss did not improve from 1163.35959\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1239.4181 - val_loss: 1170.4145\n",
      "Epoch 85/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1236.7925\n",
      "Epoch 00085: val_loss did not improve from 1163.35959\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1236.3994 - val_loss: 1165.4946\n",
      "Epoch 86/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1218.3281\n",
      "Epoch 00086: val_loss improved from 1163.35959 to 1160.97947, saving model to Weights\\Weights-086--1160.97947.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1221.2223 - val_loss: 1160.9795\n",
      "Epoch 87/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1231.1760\n",
      "Epoch 00087: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1232.6984 - val_loss: 1162.7259\n",
      "Epoch 88/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1224.4135\n",
      "Epoch 00088: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1220.1990 - val_loss: 1163.8890\n",
      "Epoch 89/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1223.2545\n",
      "Epoch 00089: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1222.5014 - val_loss: 1162.6119\n",
      "Epoch 90/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1227.5145\n",
      "Epoch 00090: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1231.5712 - val_loss: 1172.5581\n",
      "Epoch 91/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1232.0958\n",
      "Epoch 00091: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1231.3971 - val_loss: 1162.7642\n",
      "Epoch 92/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1233.7182\n",
      "Epoch 00092: val_loss did not improve from 1160.97947\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1227.4570 - val_loss: 1167.9029\n",
      "Epoch 93/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1219.6169\n",
      "Epoch 00093: val_loss improved from 1160.97947 to 1157.57813, saving model to Weights\\Weights-093--1157.57813.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1218.0871 - val_loss: 1157.5781\n",
      "Epoch 94/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1215.4414\n",
      "Epoch 00094: val_loss did not improve from 1157.57813\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1220.3066 - val_loss: 1157.6486\n",
      "Epoch 95/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1213.8794\n",
      "Epoch 00095: val_loss improved from 1157.57813 to 1157.24981, saving model to Weights\\Weights-095--1157.24981.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1214.0949 - val_loss: 1157.2498\n",
      "Epoch 96/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1212.5332\n",
      "Epoch 00096: val_loss improved from 1157.24981 to 1155.08150, saving model to Weights\\Weights-096--1155.08150.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1211.8418 - val_loss: 1155.0815\n",
      "Epoch 97/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1217.1194\n",
      "Epoch 00097: val_loss improved from 1155.08150 to 1154.53730, saving model to Weights\\Weights-097--1154.53730.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1215.9982 - val_loss: 1154.5373\n",
      "Epoch 98/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1220.0011\n",
      "Epoch 00098: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1217.0973 - val_loss: 1155.4816\n",
      "Epoch 99/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1214.1054\n",
      "Epoch 00099: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1214.3867 - val_loss: 1157.2243\n",
      "Epoch 100/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1213.4800\n",
      "Epoch 00100: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1214.7153 - val_loss: 1156.9907\n",
      "Epoch 101/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1214.3205\n",
      "Epoch 00101: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1213.6208 - val_loss: 1156.5659\n",
      "Epoch 102/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1219.9634\n",
      "Epoch 00102: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1218.1474 - val_loss: 1159.6368\n",
      "Epoch 103/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1205.5956\n",
      "Epoch 00103: val_loss did not improve from 1154.53730\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1205.7068 - val_loss: 1157.7156\n",
      "Epoch 104/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1213.8405\n",
      "Epoch 00104: val_loss improved from 1154.53730 to 1145.34592, saving model to Weights\\Weights-104--1145.34592.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1210.8211 - val_loss: 1145.3459\n",
      "Epoch 105/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1216.0360\n",
      "Epoch 00105: val_loss did not improve from 1145.34592\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1217.5681 - val_loss: 1156.6687\n",
      "Epoch 106/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1208.7288\n",
      "Epoch 00106: val_loss did not improve from 1145.34592\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1207.9724 - val_loss: 1155.2119\n",
      "Epoch 107/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1199.5693\n",
      "Epoch 00107: val_loss improved from 1145.34592 to 1144.61568, saving model to Weights\\Weights-107--1144.61568.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1202.4003 - val_loss: 1144.6157\n",
      "Epoch 108/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1207.7504\n",
      "Epoch 00108: val_loss did not improve from 1144.61568\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1207.7887 - val_loss: 1147.0690\n",
      "Epoch 109/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1199.8365\n",
      "Epoch 00109: val_loss did not improve from 1144.61568\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1199.4389 - val_loss: 1153.6843\n",
      "Epoch 110/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1196.8946\n",
      "Epoch 00110: val_loss did not improve from 1144.61568\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1203.7363 - val_loss: 1153.2887\n",
      "Epoch 111/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1210.2935\n",
      "Epoch 00111: val_loss did not improve from 1144.61568\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1213.5311 - val_loss: 1156.3719\n",
      "Epoch 112/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1202.2268\n",
      "Epoch 00112: val_loss improved from 1144.61568 to 1142.85337, saving model to Weights\\Weights-112--1142.85337.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1200.9564 - val_loss: 1142.8534\n",
      "Epoch 113/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1202.0742\n",
      "Epoch 00113: val_loss did not improve from 1142.85337\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1203.1269 - val_loss: 1147.2119\n",
      "Epoch 114/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1206.3242\n",
      "Epoch 00114: val_loss did not improve from 1142.85337\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1205.7722 - val_loss: 1150.2096\n",
      "Epoch 115/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1205.6372\n",
      "Epoch 00115: val_loss did not improve from 1142.85337\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1202.0850 - val_loss: 1148.4038\n",
      "Epoch 116/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1190.3360\n",
      "Epoch 00116: val_loss did not improve from 1142.85337\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1197.2373 - val_loss: 1143.8691\n",
      "Epoch 117/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1193.7537\n",
      "Epoch 00117: val_loss did not improve from 1142.85337\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1193.6716 - val_loss: 1151.8319\n",
      "Epoch 118/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1193.6477\n",
      "Epoch 00118: val_loss improved from 1142.85337 to 1141.19912, saving model to Weights\\Weights-118--1141.19912.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1192.5346 - val_loss: 1141.1991\n",
      "Epoch 119/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1192.7694\n",
      "Epoch 00119: val_loss did not improve from 1141.19912\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1195.8559 - val_loss: 1144.0741\n",
      "Epoch 120/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1196.0940\n",
      "Epoch 00120: val_loss did not improve from 1141.19912\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1195.5873 - val_loss: 1148.9529\n",
      "Epoch 121/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1193.7514\n",
      "Epoch 00121: val_loss did not improve from 1141.19912\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1192.7470 - val_loss: 1142.4387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1187.6805\n",
      "Epoch 00122: val_loss did not improve from 1141.19912\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1186.7115 - val_loss: 1158.8678\n",
      "Epoch 123/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1182.4951\n",
      "Epoch 00123: val_loss did not improve from 1141.19912\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1184.9158 - val_loss: 1142.4527\n",
      "Epoch 124/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1196.3520\n",
      "Epoch 00124: val_loss improved from 1141.19912 to 1134.58526, saving model to Weights\\Weights-124--1134.58526.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1194.1411 - val_loss: 1134.5853\n",
      "Epoch 125/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1191.2348\n",
      "Epoch 00125: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1189.9200 - val_loss: 1150.3735\n",
      "Epoch 126/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1185.8694\n",
      "Epoch 00126: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1186.1108 - val_loss: 1147.7756\n",
      "Epoch 127/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1184.5387\n",
      "Epoch 00127: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1184.8759 - val_loss: 1146.9294\n",
      "Epoch 128/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1180.6657\n",
      "Epoch 00128: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1182.4400 - val_loss: 1140.1312\n",
      "Epoch 129/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1187.9201\n",
      "Epoch 00129: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1187.2968 - val_loss: 1139.4515\n",
      "Epoch 130/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1168.2515\n",
      "Epoch 00130: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1176.2338 - val_loss: 1134.7015\n",
      "Epoch 131/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1188.1475\n",
      "Epoch 00131: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1191.6290 - val_loss: 1142.8128\n",
      "Epoch 132/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1178.7840\n",
      "Epoch 00132: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1181.4881 - val_loss: 1142.0008\n",
      "Epoch 133/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1184.5031\n",
      "Epoch 00133: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1185.8570 - val_loss: 1135.6192\n",
      "Epoch 134/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1176.8680\n",
      "Epoch 00134: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1178.1132 - val_loss: 1146.0346\n",
      "Epoch 135/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1174.8991\n",
      "Epoch 00135: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1179.0677 - val_loss: 1139.3451\n",
      "Epoch 136/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1171.7861\n",
      "Epoch 00136: val_loss did not improve from 1134.58526\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1173.6991 - val_loss: 1134.9981\n",
      "Epoch 137/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1180.1315\n",
      "Epoch 00137: val_loss improved from 1134.58526 to 1133.26155, saving model to Weights\\Weights-137--1133.26155.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1177.6819 - val_loss: 1133.2616\n",
      "Epoch 138/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1178.5797\n",
      "Epoch 00138: val_loss did not improve from 1133.26155\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1179.1754 - val_loss: 1139.1315\n",
      "Epoch 139/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1183.6994\n",
      "Epoch 00139: val_loss improved from 1133.26155 to 1129.63224, saving model to Weights\\Weights-139--1129.63224.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1179.6216 - val_loss: 1129.6322\n",
      "Epoch 140/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1177.7509\n",
      "Epoch 00140: val_loss did not improve from 1129.63224\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1177.2089 - val_loss: 1139.4250\n",
      "Epoch 141/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1183.6260\n",
      "Epoch 00141: val_loss did not improve from 1129.63224\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1181.5836 - val_loss: 1133.7958\n",
      "Epoch 142/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1173.9968\n",
      "Epoch 00142: val_loss did not improve from 1129.63224\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1177.1614 - val_loss: 1143.6692\n",
      "Epoch 143/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1180.3640\n",
      "Epoch 00143: val_loss did not improve from 1129.63224\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1178.6516 - val_loss: 1131.0675\n",
      "Epoch 144/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1177.3474\n",
      "Epoch 00144: val_loss improved from 1129.63224 to 1128.64493, saving model to Weights\\Weights-144--1128.64493.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1179.7991 - val_loss: 1128.6449\n",
      "Epoch 145/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1172.7592\n",
      "Epoch 00145: val_loss did not improve from 1128.64493\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1175.1167 - val_loss: 1133.7870\n",
      "Epoch 146/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1181.5991\n",
      "Epoch 00146: val_loss did not improve from 1128.64493\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1176.0952 - val_loss: 1144.8435\n",
      "Epoch 147/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1177.4510\n",
      "Epoch 00147: val_loss improved from 1128.64493 to 1126.25278, saving model to Weights\\Weights-147--1126.25278.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1176.7191 - val_loss: 1126.2528\n",
      "Epoch 148/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1160.8724\n",
      "Epoch 00148: val_loss did not improve from 1126.25278\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1161.9027 - val_loss: 1134.6772\n",
      "Epoch 149/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1171.5337\n",
      "Epoch 00149: val_loss did not improve from 1126.25278\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1170.0659 - val_loss: 1128.8072\n",
      "Epoch 150/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1173.7022\n",
      "Epoch 00150: val_loss did not improve from 1126.25278\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1172.7715 - val_loss: 1130.4231\n",
      "Epoch 151/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1163.3896\n",
      "Epoch 00151: val_loss did not improve from 1126.25278\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1165.3575 - val_loss: 1128.1595\n",
      "Epoch 152/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1170.7245\n",
      "Epoch 00152: val_loss did not improve from 1126.25278\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1174.7821 - val_loss: 1131.4828\n",
      "Epoch 153/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1169.0384\n",
      "Epoch 00153: val_loss improved from 1126.25278 to 1124.57621, saving model to Weights\\Weights-153--1124.57621.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1164.8597 - val_loss: 1124.5762\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19456/19948 [============================>.] - ETA: 0s - loss: 1165.7164\n",
      "Epoch 00154: val_loss did not improve from 1124.57621\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1166.9325 - val_loss: 1139.7501\n",
      "Epoch 155/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1163.9629\n",
      "Epoch 00155: val_loss did not improve from 1124.57621\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1163.9469 - val_loss: 1128.3049\n",
      "Epoch 156/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1160.1397\n",
      "Epoch 00156: val_loss did not improve from 1124.57621\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1165.1154 - val_loss: 1129.7012\n",
      "Epoch 157/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1161.3787\n",
      "Epoch 00157: val_loss did not improve from 1124.57621\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1164.5256 - val_loss: 1131.8054\n",
      "Epoch 158/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1159.0062\n",
      "Epoch 00158: val_loss did not improve from 1124.57621\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1154.3859 - val_loss: 1131.6750\n",
      "Epoch 159/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1170.1874\n",
      "Epoch 00159: val_loss improved from 1124.57621 to 1124.26782, saving model to Weights\\Weights-159--1124.26782.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1169.9909 - val_loss: 1124.2678\n",
      "Epoch 160/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1160.8302\n",
      "Epoch 00160: val_loss did not improve from 1124.26782\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1165.2039 - val_loss: 1151.7047\n",
      "Epoch 161/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1159.3756\n",
      "Epoch 00161: val_loss did not improve from 1124.26782\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1161.2640 - val_loss: 1131.2060\n",
      "Epoch 162/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1154.8530\n",
      "Epoch 00162: val_loss did not improve from 1124.26782\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1155.9401 - val_loss: 1128.0008\n",
      "Epoch 163/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1159.5239\n",
      "Epoch 00163: val_loss did not improve from 1124.26782\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1159.5555 - val_loss: 1126.4663\n",
      "Epoch 164/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1155.1368\n",
      "Epoch 00164: val_loss improved from 1124.26782 to 1123.75922, saving model to Weights\\Weights-164--1123.75922.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1155.4813 - val_loss: 1123.7592\n",
      "Epoch 165/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1155.0573\n",
      "Epoch 00165: val_loss did not improve from 1123.75922\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1158.5842 - val_loss: 1125.6369\n",
      "Epoch 166/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1163.2182\n",
      "Epoch 00166: val_loss did not improve from 1123.75922\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1165.9178 - val_loss: 1135.5772\n",
      "Epoch 167/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1155.4954\n",
      "Epoch 00167: val_loss improved from 1123.75922 to 1121.37430, saving model to Weights\\Weights-167--1121.37430.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1154.5927 - val_loss: 1121.3743\n",
      "Epoch 168/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1157.7636\n",
      "Epoch 00168: val_loss did not improve from 1121.37430\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1156.2720 - val_loss: 1128.3048\n",
      "Epoch 169/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1155.2360\n",
      "Epoch 00169: val_loss did not improve from 1121.37430\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1153.7497 - val_loss: 1143.4494\n",
      "Epoch 170/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1169.0472\n",
      "Epoch 00170: val_loss did not improve from 1121.37430\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1164.3310 - val_loss: 1132.6031\n",
      "Epoch 171/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1150.1479\n",
      "Epoch 00171: val_loss improved from 1121.37430 to 1120.57696, saving model to Weights\\Weights-171--1120.57696.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1151.3736 - val_loss: 1120.5770\n",
      "Epoch 172/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1155.2337\n",
      "Epoch 00172: val_loss did not improve from 1120.57696\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1156.9047 - val_loss: 1127.8180\n",
      "Epoch 173/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1156.7207\n",
      "Epoch 00173: val_loss improved from 1120.57696 to 1117.17134, saving model to Weights\\Weights-173--1117.17134.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1153.4006 - val_loss: 1117.1713\n",
      "Epoch 174/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1142.7023\n",
      "Epoch 00174: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.6584 - val_loss: 1118.1810\n",
      "Epoch 175/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1148.8406\n",
      "Epoch 00175: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1151.3165 - val_loss: 1124.8304\n",
      "Epoch 176/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1149.7599\n",
      "Epoch 00176: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1150.7508 - val_loss: 1121.9369\n",
      "Epoch 177/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1150.1200\n",
      "Epoch 00177: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1149.3011 - val_loss: 1118.2676\n",
      "Epoch 178/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1160.4525\n",
      "Epoch 00178: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1159.1317 - val_loss: 1120.9928\n",
      "Epoch 179/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1155.7888\n",
      "Epoch 00179: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1153.1885 - val_loss: 1123.1583\n",
      "Epoch 180/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1150.6701\n",
      "Epoch 00180: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1149.6281 - val_loss: 1121.2327\n",
      "Epoch 181/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1149.6592\n",
      "Epoch 00181: val_loss did not improve from 1117.17134\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1154.1619 - val_loss: 1119.5687\n",
      "Epoch 182/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1147.5624\n",
      "Epoch 00182: val_loss improved from 1117.17134 to 1113.20861, saving model to Weights\\Weights-182--1113.20861.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1148.5638 - val_loss: 1113.2086\n",
      "Epoch 183/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1152.0729\n",
      "Epoch 00183: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1152.2305 - val_loss: 1120.9228\n",
      "Epoch 184/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1144.9403\n",
      "Epoch 00184: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1147.2141 - val_loss: 1116.0756\n",
      "Epoch 185/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1140.1982\n",
      "Epoch 00185: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1143.6810 - val_loss: 1122.8078\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1152.4373\n",
      "Epoch 00186: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1148.3528 - val_loss: 1119.6035\n",
      "Epoch 187/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1147.8758\n",
      "Epoch 00187: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1149.4280 - val_loss: 1124.4588\n",
      "Epoch 188/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1150.8590\n",
      "Epoch 00188: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1150.0910 - val_loss: 1120.8980\n",
      "Epoch 189/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1147.2889\n",
      "Epoch 00189: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1147.9290 - val_loss: 1123.3347\n",
      "Epoch 190/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1138.6158\n",
      "Epoch 00190: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1137.7431 - val_loss: 1120.1117\n",
      "Epoch 191/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1137.9582\n",
      "Epoch 00191: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1140.7614 - val_loss: 1117.8017\n",
      "Epoch 192/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1149.2704\n",
      "Epoch 00192: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1148.7440 - val_loss: 1117.9428\n",
      "Epoch 193/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1138.3832\n",
      "Epoch 00193: val_loss did not improve from 1113.20861\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1138.4919 - val_loss: 1114.5858\n",
      "Epoch 194/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1143.2015\n",
      "Epoch 00194: val_loss improved from 1113.20861 to 1112.41086, saving model to Weights\\Weights-194--1112.41086.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1142.0014 - val_loss: 1112.4109\n",
      "Epoch 195/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1152.2286\n",
      "Epoch 00195: val_loss did not improve from 1112.41086\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1151.0178 - val_loss: 1112.5802\n",
      "Epoch 196/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1132.0269\n",
      "Epoch 00196: val_loss did not improve from 1112.41086\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1139.7095 - val_loss: 1121.8799\n",
      "Epoch 197/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1142.4318\n",
      "Epoch 00197: val_loss did not improve from 1112.41086\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1142.4829 - val_loss: 1121.6312\n",
      "Epoch 198/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1130.7835\n",
      "Epoch 00198: val_loss did not improve from 1112.41086\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1127.3021 - val_loss: 1117.0630\n",
      "Epoch 199/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1134.7633\n",
      "Epoch 00199: val_loss did not improve from 1112.41086\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1128.9701 - val_loss: 1119.4039\n",
      "Epoch 200/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1132.5368\n",
      "Epoch 00200: val_loss improved from 1112.41086 to 1111.80464, saving model to Weights\\Weights-200--1111.80464.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1134.1351 - val_loss: 1111.8046\n",
      "Epoch 201/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1142.9489\n",
      "Epoch 00201: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1143.0247 - val_loss: 1115.4951\n",
      "Epoch 202/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1140.0793\n",
      "Epoch 00202: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1139.3171 - val_loss: 1119.8187\n",
      "Epoch 203/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1143.7929\n",
      "Epoch 00203: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1141.3010 - val_loss: 1116.1085\n",
      "Epoch 204/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1137.9324\n",
      "Epoch 00204: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1139.4775 - val_loss: 1112.3788\n",
      "Epoch 205/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1140.6500\n",
      "Epoch 00205: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1141.2192 - val_loss: 1122.6929\n",
      "Epoch 206/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1145.1550\n",
      "Epoch 00206: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1143.2797 - val_loss: 1122.1762\n",
      "Epoch 207/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1136.9207\n",
      "Epoch 00207: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1136.3158 - val_loss: 1112.2589\n",
      "Epoch 208/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1134.8713\n",
      "Epoch 00208: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1133.6305 - val_loss: 1116.3067\n",
      "Epoch 209/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1124.2240\n",
      "Epoch 00209: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1123.5389 - val_loss: 1112.6853\n",
      "Epoch 210/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1132.1382\n",
      "Epoch 00210: val_loss did not improve from 1111.80464\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1136.7378 - val_loss: 1116.7621\n",
      "Epoch 211/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1143.2043\n",
      "Epoch 00211: val_loss improved from 1111.80464 to 1111.06704, saving model to Weights\\Weights-211--1111.06704.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1143.0053 - val_loss: 1111.0670\n",
      "Epoch 212/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1133.1290\n",
      "Epoch 00212: val_loss did not improve from 1111.06704\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1134.7895 - val_loss: 1118.8446\n",
      "Epoch 213/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1127.0384\n",
      "Epoch 00213: val_loss improved from 1111.06704 to 1109.43177, saving model to Weights\\Weights-213--1109.43177.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1125.3566 - val_loss: 1109.4318\n",
      "Epoch 214/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1130.1668\n",
      "Epoch 00214: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1130.8737 - val_loss: 1114.6892\n",
      "Epoch 215/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1131.3874\n",
      "Epoch 00215: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1128.4063 - val_loss: 1110.0757\n",
      "Epoch 216/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1121.5060\n",
      "Epoch 00216: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1124.6254 - val_loss: 1111.6134\n",
      "Epoch 217/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1129.2843\n",
      "Epoch 00217: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1131.2894 - val_loss: 1120.1992\n",
      "Epoch 218/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1130.4978\n",
      "Epoch 00218: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1128.4940 - val_loss: 1113.2113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1122.6257\n",
      "Epoch 00219: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1125.7970 - val_loss: 1123.0889\n",
      "Epoch 220/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1129.6584\n",
      "Epoch 00220: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1131.1800 - val_loss: 1115.7536\n",
      "Epoch 221/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1127.5449\n",
      "Epoch 00221: val_loss did not improve from 1109.43177\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1128.1263 - val_loss: 1118.0129\n",
      "Epoch 222/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1128.6192\n",
      "Epoch 00222: val_loss improved from 1109.43177 to 1108.88585, saving model to Weights\\Weights-222--1108.88585.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1127.7670 - val_loss: 1108.8858\n",
      "Epoch 223/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1122.4902\n",
      "Epoch 00223: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1126.7421 - val_loss: 1114.8747\n",
      "Epoch 224/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1124.1209\n",
      "Epoch 00224: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1124.6142 - val_loss: 1111.0121\n",
      "Epoch 225/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1121.6303\n",
      "Epoch 00225: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1124.5907 - val_loss: 1114.5941\n",
      "Epoch 226/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1126.6645\n",
      "Epoch 00226: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1127.6342 - val_loss: 1124.2856\n",
      "Epoch 227/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1131.5705\n",
      "Epoch 00227: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1130.1227 - val_loss: 1110.2489\n",
      "Epoch 228/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1121.1869\n",
      "Epoch 00228: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1122.4035 - val_loss: 1109.9476\n",
      "Epoch 229/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1130.6034\n",
      "Epoch 00229: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1129.3103 - val_loss: 1114.9545\n",
      "Epoch 230/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1122.1712\n",
      "Epoch 00230: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1122.4492 - val_loss: 1111.2368\n",
      "Epoch 231/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1124.9374\n",
      "Epoch 00231: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1125.8974 - val_loss: 1117.7542\n",
      "Epoch 232/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1125.5455\n",
      "Epoch 00232: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1124.9858 - val_loss: 1112.7686\n",
      "Epoch 233/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1122.3098\n",
      "Epoch 00233: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1122.3482 - val_loss: 1112.1380\n",
      "Epoch 234/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1117.7668\n",
      "Epoch 00234: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1122.3367 - val_loss: 1120.8008\n",
      "Epoch 235/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1127.2026\n",
      "Epoch 00235: val_loss did not improve from 1108.88585\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1130.5495 - val_loss: 1111.5011\n",
      "Epoch 236/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1120.7394\n",
      "Epoch 00236: val_loss improved from 1108.88585 to 1102.61137, saving model to Weights\\Weights-236--1102.61137.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1118.3791 - val_loss: 1102.6114\n",
      "Epoch 237/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1119.9799\n",
      "Epoch 00237: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1117.9839 - val_loss: 1109.6516\n",
      "Epoch 238/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1127.1776\n",
      "Epoch 00238: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1119.4793 - val_loss: 1113.6838\n",
      "Epoch 239/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1125.1905\n",
      "Epoch 00239: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1122.1442 - val_loss: 1103.6899\n",
      "Epoch 240/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1115.2063\n",
      "Epoch 00240: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1116.8554 - val_loss: 1110.6740\n",
      "Epoch 241/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.6575\n",
      "Epoch 00241: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1120.4407 - val_loss: 1110.2759\n",
      "Epoch 242/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1124.0845\n",
      "Epoch 00242: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1123.9800 - val_loss: 1105.6968\n",
      "Epoch 243/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1115.2787\n",
      "Epoch 00243: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1117.2258 - val_loss: 1107.7354\n",
      "Epoch 244/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1114.6883\n",
      "Epoch 00244: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1124.0846 - val_loss: 1108.5717\n",
      "Epoch 245/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1113.2266\n",
      "Epoch 00245: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1116.0230 - val_loss: 1106.0416\n",
      "Epoch 246/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1123.3345\n",
      "Epoch 00246: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1122.6213 - val_loss: 1107.1488\n",
      "Epoch 247/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1118.1663\n",
      "Epoch 00247: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1117.4220 - val_loss: 1123.2849\n",
      "Epoch 248/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1116.2400\n",
      "Epoch 00248: val_loss did not improve from 1102.61137\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1114.8331 - val_loss: 1106.2869\n",
      "Epoch 249/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1105.0986\n",
      "Epoch 00249: val_loss improved from 1102.61137 to 1102.39743, saving model to Weights\\Weights-249--1102.39743.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1109.1674 - val_loss: 1102.3974\n",
      "Epoch 250/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1115.5953\n",
      "Epoch 00250: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1117.4187 - val_loss: 1108.7634\n",
      "Epoch 251/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1109.7893\n",
      "Epoch 00251: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1111.0812 - val_loss: 1106.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1110.1467\n",
      "Epoch 00252: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1117.9132 - val_loss: 1110.2247\n",
      "Epoch 253/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1109.8263\n",
      "Epoch 00253: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1109.4384 - val_loss: 1108.6343\n",
      "Epoch 254/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1110.8374\n",
      "Epoch 00254: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1111.2927 - val_loss: 1105.8447\n",
      "Epoch 255/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1109.0694\n",
      "Epoch 00255: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1106.9258 - val_loss: 1102.6870\n",
      "Epoch 256/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1114.2070\n",
      "Epoch 00256: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1113.0337 - val_loss: 1105.6508\n",
      "Epoch 257/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1113.2635\n",
      "Epoch 00257: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1114.4665 - val_loss: 1113.3052\n",
      "Epoch 258/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1122.3688\n",
      "Epoch 00258: val_loss did not improve from 1102.39743\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1121.4921 - val_loss: 1107.2222\n",
      "Epoch 259/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1110.4588\n",
      "Epoch 00259: val_loss improved from 1102.39743 to 1099.63744, saving model to Weights\\Weights-259--1099.63744.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1111.6017 - val_loss: 1099.6374\n",
      "Epoch 260/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1113.7931\n",
      "Epoch 00260: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1117.3594 - val_loss: 1110.3808\n",
      "Epoch 261/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1110.3683\n",
      "Epoch 00261: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1111.7815 - val_loss: 1106.8339\n",
      "Epoch 262/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1104.3920\n",
      "Epoch 00262: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1101.2064 - val_loss: 1101.2107\n",
      "Epoch 263/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1103.6553\n",
      "Epoch 00263: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1110.4217 - val_loss: 1099.8437\n",
      "Epoch 264/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1109.5013\n",
      "Epoch 00264: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1109.5251 - val_loss: 1100.5092\n",
      "Epoch 265/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1117.2155\n",
      "Epoch 00265: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1114.6393 - val_loss: 1108.3386\n",
      "Epoch 266/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1104.7774\n",
      "Epoch 00266: val_loss did not improve from 1099.63744\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1110.1034 - val_loss: 1107.7250\n",
      "Epoch 267/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1109.5275\n",
      "Epoch 00267: val_loss improved from 1099.63744 to 1099.12110, saving model to Weights\\Weights-267--1099.12110.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1110.0468 - val_loss: 1099.1211\n",
      "Epoch 268/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1099.5825\n",
      "Epoch 00268: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1101.2831 - val_loss: 1109.7825\n",
      "Epoch 269/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1113.9140\n",
      "Epoch 00269: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1113.2849 - val_loss: 1101.3361\n",
      "Epoch 270/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1115.2594\n",
      "Epoch 00270: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1113.8897 - val_loss: 1107.6327\n",
      "Epoch 271/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1107.6730\n",
      "Epoch 00271: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1107.3271 - val_loss: 1113.8049\n",
      "Epoch 272/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1112.1150\n",
      "Epoch 00272: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1110.9547 - val_loss: 1100.7313\n",
      "Epoch 273/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1104.2238\n",
      "Epoch 00273: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1103.7384 - val_loss: 1101.4661\n",
      "Epoch 274/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1114.4912\n",
      "Epoch 00274: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1115.8400 - val_loss: 1104.2694\n",
      "Epoch 275/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1103.8990\n",
      "Epoch 00275: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1105.5558 - val_loss: 1105.5432\n",
      "Epoch 276/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1102.7513\n",
      "Epoch 00276: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1105.4195 - val_loss: 1104.7483\n",
      "Epoch 277/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1101.2330\n",
      "Epoch 00277: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1100.3506 - val_loss: 1106.9523\n",
      "Epoch 278/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1099.5054\n",
      "Epoch 00278: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1100.6948 - val_loss: 1110.8528\n",
      "Epoch 279/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1103.0227\n",
      "Epoch 00279: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1104.2134 - val_loss: 1101.4091\n",
      "Epoch 280/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1103.5101\n",
      "Epoch 00280: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1101.3189 - val_loss: 1105.7637\n",
      "Epoch 281/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1105.9369\n",
      "Epoch 00281: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1107.3098 - val_loss: 1101.3272\n",
      "Epoch 282/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1109.6450\n",
      "Epoch 00282: val_loss did not improve from 1099.12110\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1106.0631 - val_loss: 1106.9181\n",
      "Epoch 283/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1099.8934\n",
      "Epoch 00283: val_loss improved from 1099.12110 to 1099.07163, saving model to Weights\\Weights-283--1099.07163.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1098.2711 - val_loss: 1099.0716\n",
      "Epoch 284/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1101.0448\n",
      "Epoch 00284: val_loss did not improve from 1099.07163\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1099.6615 - val_loss: 1103.6607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1097.7537\n",
      "Epoch 00285: val_loss did not improve from 1099.07163\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1093.3695 - val_loss: 1103.6225\n",
      "Epoch 286/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1097.9643\n",
      "Epoch 00286: val_loss did not improve from 1099.07163\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1100.9148 - val_loss: 1100.2215\n",
      "Epoch 287/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1110.9102\n",
      "Epoch 00287: val_loss did not improve from 1099.07163\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1103.3570 - val_loss: 1100.3681\n",
      "Epoch 288/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1099.7847\n",
      "Epoch 00288: val_loss did not improve from 1099.07163\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1104.1635 - val_loss: 1107.2557\n",
      "Epoch 289/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1096.3514\n",
      "Epoch 00289: val_loss improved from 1099.07163 to 1096.88270, saving model to Weights\\Weights-289--1096.88270.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1104.6201 - val_loss: 1096.8827\n",
      "Epoch 290/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1100.7318\n",
      "Epoch 00290: val_loss did not improve from 1096.88270\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.0632 - val_loss: 1098.6073\n",
      "Epoch 291/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1089.7409\n",
      "Epoch 00291: val_loss improved from 1096.88270 to 1095.90994, saving model to Weights\\Weights-291--1095.90994.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1092.4097 - val_loss: 1095.9099\n",
      "Epoch 292/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1098.1686\n",
      "Epoch 00292: val_loss did not improve from 1095.90994\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1096.8088 - val_loss: 1104.1405\n",
      "Epoch 293/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1095.8800\n",
      "Epoch 00293: val_loss improved from 1095.90994 to 1092.68968, saving model to Weights\\Weights-293--1092.68968.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1096.8059 - val_loss: 1092.6897\n",
      "Epoch 294/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1091.7080\n",
      "Epoch 00294: val_loss did not improve from 1092.68968\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1091.5941 - val_loss: 1100.1597\n",
      "Epoch 295/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1102.1840\n",
      "Epoch 00295: val_loss did not improve from 1092.68968\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1094.5015 - val_loss: 1098.7531\n",
      "Epoch 296/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1088.84 - ETA: 0s - loss: 1093.0262\n",
      "Epoch 00296: val_loss did not improve from 1092.68968\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1094.7792 - val_loss: 1095.5293\n",
      "Epoch 297/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1092.9069 ETA: 0s - loss: 1088.\n",
      "Epoch 00297: val_loss improved from 1092.68968 to 1091.83480, saving model to Weights\\Weights-297--1091.83480.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1093.2412 - val_loss: 1091.8348\n",
      "Epoch 298/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1100.7850\n",
      "Epoch 00298: val_loss did not improve from 1091.83480\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1099.1176 - val_loss: 1099.3397\n",
      "Epoch 299/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1103.6731\n",
      "Epoch 00299: val_loss did not improve from 1091.83480\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1099.9083 - val_loss: 1106.1427\n",
      "Epoch 300/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1090.6370\n",
      "Epoch 00300: val_loss did not improve from 1091.83480\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1092.9467 - val_loss: 1097.3922\n",
      "Epoch 301/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1092.0502\n",
      "Epoch 00301: val_loss improved from 1091.83480 to 1090.87760, saving model to Weights\\Weights-301--1090.87760.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1093.3004 - val_loss: 1090.8776\n",
      "Epoch 302/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1086.5279\n",
      "Epoch 00302: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1087.2980 - val_loss: 1096.9737\n",
      "Epoch 303/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1096.6109\n",
      "Epoch 00303: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1095.9408 - val_loss: 1107.0771\n",
      "Epoch 304/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1104.2571\n",
      "Epoch 00304: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1105.4080 - val_loss: 1102.1851\n",
      "Epoch 305/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1088.8045\n",
      "Epoch 00305: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1090.7899 - val_loss: 1098.3308\n",
      "Epoch 306/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1090.1355\n",
      "Epoch 00306: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1090.9367 - val_loss: 1095.1492\n",
      "Epoch 307/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1089.4780\n",
      "Epoch 00307: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1092.8926 - val_loss: 1104.7325\n",
      "Epoch 308/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1097.5131\n",
      "Epoch 00308: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1097.0974 - val_loss: 1094.8324\n",
      "Epoch 309/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1091.0684\n",
      "Epoch 00309: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1090.6260 - val_loss: 1100.1690\n",
      "Epoch 310/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.6816\n",
      "Epoch 00310: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1089.8683 - val_loss: 1094.3182\n",
      "Epoch 311/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.1664\n",
      "Epoch 00311: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.4420 - val_loss: 1100.7250\n",
      "Epoch 312/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1091.5136\n",
      "Epoch 00312: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1093.7299 - val_loss: 1093.9101\n",
      "Epoch 313/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1094.2688\n",
      "Epoch 00313: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1092.3096 - val_loss: 1095.3186\n",
      "Epoch 314/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1089.6360\n",
      "Epoch 00314: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1088.3009 - val_loss: 1096.9464\n",
      "Epoch 315/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1082.8808 ETA: 0s - loss: 1083.91\n",
      "Epoch 00315: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1082.9727 - val_loss: 1095.8737\n",
      "Epoch 316/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.3725\n",
      "Epoch 00316: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1089.0566 - val_loss: 1101.4928\n",
      "Epoch 317/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1088.1654\n",
      "Epoch 00317: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1090.3745 - val_loss: 1099.5587\n",
      "Epoch 318/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1089.3861\n",
      "Epoch 00318: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1089.6909 - val_loss: 1098.6862\n",
      "Epoch 319/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1090.2068\n",
      "Epoch 00319: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1091.5736 - val_loss: 1102.4248\n",
      "Epoch 320/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1084.8515\n",
      "Epoch 00320: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1086.9608 - val_loss: 1095.9427\n",
      "Epoch 321/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1096.0392\n",
      "Epoch 00321: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1093.9387 - val_loss: 1098.1066\n",
      "Epoch 322/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1083.9485\n",
      "Epoch 00322: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1084.2199 - val_loss: 1100.1139\n",
      "Epoch 323/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1087.6536\n",
      "Epoch 00323: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1086.9250 - val_loss: 1093.3487\n",
      "Epoch 324/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1080.9433\n",
      "Epoch 00324: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1078.0808 - val_loss: 1100.4819\n",
      "Epoch 325/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1083.2179\n",
      "Epoch 00325: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1088.2413 - val_loss: 1093.8018\n",
      "Epoch 326/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1088.0351\n",
      "Epoch 00326: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1087.1417 - val_loss: 1092.2787\n",
      "Epoch 327/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1084.3698\n",
      "Epoch 00327: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1083.4258 - val_loss: 1092.6651\n",
      "Epoch 328/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1081.3748\n",
      "Epoch 00328: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1082.7930 - val_loss: 1096.1951\n",
      "Epoch 329/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1077.7520\n",
      "Epoch 00329: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1079.0532 - val_loss: 1100.8879\n",
      "Epoch 330/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1080.6299\n",
      "Epoch 00330: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1080.3136 - val_loss: 1094.3150\n",
      "Epoch 331/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1078.9633\n",
      "Epoch 00331: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1079.0189 - val_loss: 1093.4536\n",
      "Epoch 332/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1082.4016\n",
      "Epoch 00332: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1082.3533 - val_loss: 1095.5085\n",
      "Epoch 333/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1083.5059\n",
      "Epoch 00333: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1084.0676 - val_loss: 1094.5393\n",
      "Epoch 334/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1076.5691\n",
      "Epoch 00334: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1077.8390 - val_loss: 1096.0091\n",
      "Epoch 335/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1075.5901\n",
      "Epoch 00335: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.5950 - val_loss: 1093.2778\n",
      "Epoch 336/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1085.6408\n",
      "Epoch 00336: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1085.2721 - val_loss: 1096.3325\n",
      "Epoch 337/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1089.6163\n",
      "Epoch 00337: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1085.6867 - val_loss: 1094.2381\n",
      "Epoch 338/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1079.8011\n",
      "Epoch 00338: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1080.9472 - val_loss: 1091.2336\n",
      "Epoch 339/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1078.7849\n",
      "Epoch 00339: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1080.7463 - val_loss: 1093.6312\n",
      "Epoch 340/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1081.8506\n",
      "Epoch 00340: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1082.5169 - val_loss: 1107.7816\n",
      "Epoch 341/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1081.3574\n",
      "Epoch 00341: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1080.8752 - val_loss: 1093.3182\n",
      "Epoch 342/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1080.8831\n",
      "Epoch 00342: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1079.9377 - val_loss: 1097.8952\n",
      "Epoch 343/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1075.6731\n",
      "Epoch 00343: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1077.4726 - val_loss: 1097.2371\n",
      "Epoch 344/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1078.6169\n",
      "Epoch 00344: val_loss did not improve from 1090.87760\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1080.0074 - val_loss: 1095.0430\n",
      "Epoch 345/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1077.9959\n",
      "Epoch 00345: val_loss improved from 1090.87760 to 1088.49181, saving model to Weights\\Weights-345--1088.49181.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1076.9156 - val_loss: 1088.4918\n",
      "Epoch 346/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1075.1204\n",
      "Epoch 00346: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1076.3583 - val_loss: 1092.8757\n",
      "Epoch 347/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1079.9510\n",
      "Epoch 00347: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1075.9855 - val_loss: 1091.4726\n",
      "Epoch 348/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1071.7024\n",
      "Epoch 00348: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.5762 - val_loss: 1097.6128\n",
      "Epoch 349/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1081.3492\n",
      "Epoch 00349: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1077.6554 - val_loss: 1099.6408\n",
      "Epoch 350/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1078.4639\n",
      "Epoch 00350: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.3018 - val_loss: 1097.3254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 351/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1081.0937\n",
      "Epoch 00351: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1082.6691 - val_loss: 1096.6645\n",
      "Epoch 352/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1078.9748\n",
      "Epoch 00352: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1083.2092 - val_loss: 1092.5334\n",
      "Epoch 353/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1073.4323 ETA: 0s - loss: 1060.\n",
      "Epoch 00353: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1074.8193 - val_loss: 1099.0905\n",
      "Epoch 354/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1077.6155\n",
      "Epoch 00354: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1078.0548 - val_loss: 1096.0650\n",
      "Epoch 355/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1075.9292\n",
      "Epoch 00355: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1079.4460 - val_loss: 1095.7262\n",
      "Epoch 356/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1077.0951\n",
      "Epoch 00356: val_loss did not improve from 1088.49181\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1078.7850 - val_loss: 1093.0986\n",
      "Epoch 357/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1077.9077\n",
      "Epoch 00357: val_loss improved from 1088.49181 to 1087.19401, saving model to Weights\\Weights-357--1087.19401.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1078.3074 - val_loss: 1087.1940\n",
      "Epoch 358/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1072.6491\n",
      "Epoch 00358: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1069.2082 - val_loss: 1090.5864\n",
      "Epoch 359/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1074.4941\n",
      "Epoch 00359: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.1282 - val_loss: 1095.4057\n",
      "Epoch 360/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1086.4415\n",
      "Epoch 00360: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1081.0009 - val_loss: 1089.6033\n",
      "Epoch 361/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1074.6846\n",
      "Epoch 00361: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1072.4442 - val_loss: 1091.8345\n",
      "Epoch 362/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1075.3357\n",
      "Epoch 00362: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1078.6288 - val_loss: 1098.2873\n",
      "Epoch 363/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1064.8076\n",
      "Epoch 00363: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1073.6118 - val_loss: 1089.4159\n",
      "Epoch 364/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1063.8622\n",
      "Epoch 00364: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1069.4866 - val_loss: 1090.1116\n",
      "Epoch 365/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1068.7851\n",
      "Epoch 00365: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1067.7654 - val_loss: 1091.0222\n",
      "Epoch 366/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1069.7043\n",
      "Epoch 00366: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1071.4520 - val_loss: 1094.5445\n",
      "Epoch 367/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1077.2070\n",
      "Epoch 00367: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.1627 - val_loss: 1087.4396\n",
      "Epoch 368/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1068.1690\n",
      "Epoch 00368: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1072.2212 - val_loss: 1090.0144\n",
      "Epoch 369/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1072.0163\n",
      "Epoch 00369: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1071.0101 - val_loss: 1092.3710\n",
      "Epoch 370/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1070.7582\n",
      "Epoch 00370: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1069.6935 - val_loss: 1089.1009\n",
      "Epoch 371/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1074.6727\n",
      "Epoch 00371: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1072.1949 - val_loss: 1088.9664\n",
      "Epoch 372/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1069.4766\n",
      "Epoch 00372: val_loss did not improve from 1087.19401\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1069.8850 - val_loss: 1093.4254\n",
      "Epoch 373/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1075.5408\n",
      "Epoch 00373: val_loss improved from 1087.19401 to 1085.60532, saving model to Weights\\Weights-373--1085.60532.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1072.8713 - val_loss: 1085.6053\n",
      "Epoch 374/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1060.2640\n",
      "Epoch 00374: val_loss did not improve from 1085.60532\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1065.7368 - val_loss: 1090.9845\n",
      "Epoch 375/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1069.1545\n",
      "Epoch 00375: val_loss did not improve from 1085.60532\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1070.0911 - val_loss: 1091.7972\n",
      "Epoch 376/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1080.0623\n",
      "Epoch 00376: val_loss did not improve from 1085.60532\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1078.6786 - val_loss: 1097.5597\n",
      "Epoch 377/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1073.5429\n",
      "Epoch 00377: val_loss did not improve from 1085.60532\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1071.0847 - val_loss: 1088.1633\n",
      "Epoch 378/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1072.5103\n",
      "Epoch 00378: val_loss did not improve from 1085.60532\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1068.1233 - val_loss: 1091.1999\n",
      "Epoch 379/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1055.6840\n",
      "Epoch 00379: val_loss improved from 1085.60532 to 1085.36634, saving model to Weights\\Weights-379--1085.36634.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1061.0960 - val_loss: 1085.3663\n",
      "Epoch 380/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1066.3976\n",
      "Epoch 00380: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1066.2755 - val_loss: 1090.5828\n",
      "Epoch 381/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.3659\n",
      "Epoch 00381: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1066.6680 - val_loss: 1088.9560\n",
      "Epoch 382/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.8776\n",
      "Epoch 00382: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1066.1786 - val_loss: 1087.7380\n",
      "Epoch 383/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.5331\n",
      "Epoch 00383: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.8071 - val_loss: 1095.2747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1058.3956\n",
      "Epoch 00384: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1064.5961 - val_loss: 1092.0893\n",
      "Epoch 385/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1063.7391\n",
      "Epoch 00385: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1063.7072 - val_loss: 1092.3078\n",
      "Epoch 386/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1067.3431\n",
      "Epoch 00386: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1066.4729 - val_loss: 1091.0055\n",
      "Epoch 387/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1069.8429\n",
      "Epoch 00387: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.0150 - val_loss: 1089.3437\n",
      "Epoch 388/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1068.1464\n",
      "Epoch 00388: val_loss did not improve from 1085.36634\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1068.8412 - val_loss: 1093.7559\n",
      "Epoch 389/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1066.0327\n",
      "Epoch 00389: val_loss improved from 1085.36634 to 1081.83831, saving model to Weights\\Weights-389--1081.83831.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1067.9084 - val_loss: 1081.8383\n",
      "Epoch 390/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1062.4107\n",
      "Epoch 00390: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1061.3399 - val_loss: 1092.7488\n",
      "Epoch 391/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1073.6430\n",
      "Epoch 00391: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1073.2922 - val_loss: 1093.6733\n",
      "Epoch 392/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1065.4478\n",
      "Epoch 00392: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1063.7788 - val_loss: 1083.9213\n",
      "Epoch 393/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1071.4937\n",
      "Epoch 00393: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1070.0403 - val_loss: 1089.6564\n",
      "Epoch 394/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1070.2141\n",
      "Epoch 00394: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1068.7708 - val_loss: 1084.3417\n",
      "Epoch 395/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1062.8969\n",
      "Epoch 00395: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1060.5061 - val_loss: 1089.5057\n",
      "Epoch 396/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1068.6172\n",
      "Epoch 00396: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1071.8104 - val_loss: 1089.3172\n",
      "Epoch 397/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1065.3632\n",
      "Epoch 00397: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1063.8147 - val_loss: 1095.4424\n",
      "Epoch 398/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1064.4892\n",
      "Epoch 00398: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1066.6758 - val_loss: 1085.0536\n",
      "Epoch 399/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1072.1969\n",
      "Epoch 00399: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1072.0745 - val_loss: 1084.4655\n",
      "Epoch 400/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.9582\n",
      "Epoch 00400: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1059.3525 - val_loss: 1089.9680\n",
      "Epoch 401/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1062.3165\n",
      "Epoch 00401: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1065.6939 - val_loss: 1090.3744\n",
      "Epoch 402/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.6324\n",
      "Epoch 00402: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1053.8370 - val_loss: 1085.3834\n",
      "Epoch 403/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.8051\n",
      "Epoch 00403: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1066.7367 - val_loss: 1085.3272\n",
      "Epoch 404/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1064.6072\n",
      "Epoch 00404: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1063.2917 - val_loss: 1083.9576\n",
      "Epoch 405/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1068.5773\n",
      "Epoch 00405: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1064.7901 - val_loss: 1085.8632\n",
      "Epoch 406/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1063.0271\n",
      "Epoch 00406: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1062.6641 - val_loss: 1090.0877\n",
      "Epoch 407/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1071.7467\n",
      "Epoch 00407: val_loss did not improve from 1081.83831\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1070.7575 - val_loss: 1086.1280\n",
      "Epoch 408/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1066.8819\n",
      "Epoch 00408: val_loss improved from 1081.83831 to 1081.53856, saving model to Weights\\Weights-408--1081.53856.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1067.7772 - val_loss: 1081.5386\n",
      "Epoch 409/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1054.8506\n",
      "Epoch 00409: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1056.5451 - val_loss: 1087.7119\n",
      "Epoch 410/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1065.4451\n",
      "Epoch 00410: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1065.6693 - val_loss: 1089.0174\n",
      "Epoch 411/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1062.0597\n",
      "Epoch 00411: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1060.8263 - val_loss: 1087.1393\n",
      "Epoch 412/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1061.3287\n",
      "Epoch 00412: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1062.0918 - val_loss: 1092.2351\n",
      "Epoch 413/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1066.7527\n",
      "Epoch 00413: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1064.5475 - val_loss: 1085.3021\n",
      "Epoch 414/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1059.8075\n",
      "Epoch 00414: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1059.2624 - val_loss: 1086.7842\n",
      "Epoch 415/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1051.0744\n",
      "Epoch 00415: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1055.8364 - val_loss: 1086.4307\n",
      "Epoch 416/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1063.9753\n",
      "Epoch 00416: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1065.6668 - val_loss: 1085.1151\n",
      "Epoch 417/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1065.0218\n",
      "Epoch 00417: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.4676 - val_loss: 1084.4056\n",
      "Epoch 418/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1057.7991\n",
      "Epoch 00418: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.2709 - val_loss: 1088.9151\n",
      "Epoch 419/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.7571\n",
      "Epoch 00419: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1058.2811 - val_loss: 1083.4411\n",
      "Epoch 420/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1058.1549\n",
      "Epoch 00420: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1055.9658 - val_loss: 1084.8916\n",
      "Epoch 421/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1069.2154\n",
      "Epoch 00421: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1067.4442 - val_loss: 1095.9046\n",
      "Epoch 422/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1057.1137\n",
      "Epoch 00422: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1056.1927 - val_loss: 1084.3403\n",
      "Epoch 423/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.5202\n",
      "Epoch 00423: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1065.0762 - val_loss: 1085.3687\n",
      "Epoch 424/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1061.0994\n",
      "Epoch 00424: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1060.9106 - val_loss: 1087.9184\n",
      "Epoch 425/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.1132\n",
      "Epoch 00425: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1061.9835 - val_loss: 1089.8309\n",
      "Epoch 426/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1059.9172\n",
      "Epoch 00426: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1059.9805 - val_loss: 1085.3483\n",
      "Epoch 427/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1062.1881\n",
      "Epoch 00427: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1065.7620 - val_loss: 1086.6884\n",
      "Epoch 428/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1063.5534\n",
      "Epoch 00428: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.1721 - val_loss: 1082.3731\n",
      "Epoch 429/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.8736\n",
      "Epoch 00429: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1055.4469 - val_loss: 1083.5701\n",
      "Epoch 430/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1046.6106\n",
      "Epoch 00430: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1048.4545 - val_loss: 1084.0119\n",
      "Epoch 431/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.2800\n",
      "Epoch 00431: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1058.4911 - val_loss: 1086.9556\n",
      "Epoch 432/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1049.6418 ETA: 0s - loss: 1045.\n",
      "Epoch 00432: val_loss did not improve from 1081.53856\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1054.0702 - val_loss: 1082.5033\n",
      "Epoch 433/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1065.7470\n",
      "Epoch 00433: val_loss improved from 1081.53856 to 1079.95806, saving model to Weights\\Weights-433--1079.95806.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1064.0076 - val_loss: 1079.9581\n",
      "Epoch 434/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1043.1371\n",
      "Epoch 00434: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1044.6574 - val_loss: 1089.3810\n",
      "Epoch 435/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1055.7214\n",
      "Epoch 00435: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1057.1346 - val_loss: 1089.0880\n",
      "Epoch 436/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1055.7904\n",
      "Epoch 00436: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1057.9698 - val_loss: 1090.1903\n",
      "Epoch 437/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1062.4297\n",
      "Epoch 00437: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1060.3813 - val_loss: 1088.1839\n",
      "Epoch 438/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1046.2022\n",
      "Epoch 00438: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.2878 - val_loss: 1081.9974\n",
      "Epoch 439/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1047.1358\n",
      "Epoch 00439: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1054.3328 - val_loss: 1096.5593\n",
      "Epoch 440/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1063.1796\n",
      "Epoch 00440: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1063.0651 - val_loss: 1095.2014\n",
      "Epoch 441/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1061.5555\n",
      "Epoch 00441: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1063.8448 - val_loss: 1085.0493\n",
      "Epoch 442/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1056.8012\n",
      "Epoch 00442: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.3396 - val_loss: 1082.1658\n",
      "Epoch 443/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.4124\n",
      "Epoch 00443: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1054.0004 - val_loss: 1086.6599\n",
      "Epoch 444/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.3601\n",
      "Epoch 00444: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1045.1749 - val_loss: 1092.2888\n",
      "Epoch 445/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.7997\n",
      "Epoch 00445: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1058.1363 - val_loss: 1091.7048\n",
      "Epoch 446/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1054.7310\n",
      "Epoch 00446: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1054.4467 - val_loss: 1086.0554\n",
      "Epoch 447/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1054.3377\n",
      "Epoch 00447: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1051.9959 - val_loss: 1084.7814\n",
      "Epoch 448/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.0068\n",
      "Epoch 00448: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1052.4955 - val_loss: 1082.5085\n",
      "Epoch 449/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.0366\n",
      "Epoch 00449: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1052.1311 - val_loss: 1081.0184\n",
      "Epoch 450/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1048.4809\n",
      "Epoch 00450: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1048.8001 - val_loss: 1084.6677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1056.0070\n",
      "Epoch 00451: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1053.6884 - val_loss: 1080.9800\n",
      "Epoch 452/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.2109\n",
      "Epoch 00452: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1049.7288 - val_loss: 1085.2457\n",
      "Epoch 453/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1043.8546\n",
      "Epoch 00453: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1047.8850 - val_loss: 1089.9407\n",
      "Epoch 454/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.4880\n",
      "Epoch 00454: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1052.0199 - val_loss: 1091.4343\n",
      "Epoch 455/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.1103\n",
      "Epoch 00455: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1054.4931 - val_loss: 1083.0821\n",
      "Epoch 456/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.8728\n",
      "Epoch 00456: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1047.3029 - val_loss: 1083.9893\n",
      "Epoch 457/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1049.9852\n",
      "Epoch 00457: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1045.5524 - val_loss: 1082.3881\n",
      "Epoch 458/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1058.6013\n",
      "Epoch 00458: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1057.4301 - val_loss: 1084.6258\n",
      "Epoch 459/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.8840\n",
      "Epoch 00459: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1046.5082 - val_loss: 1085.2715\n",
      "Epoch 460/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1051.9826\n",
      "Epoch 00460: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1052.4585 - val_loss: 1085.7269\n",
      "Epoch 461/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1050.1042\n",
      "Epoch 00461: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1050.5570 - val_loss: 1082.1071\n",
      "Epoch 462/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1050.9133\n",
      "Epoch 00462: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1052.5561 - val_loss: 1084.4152\n",
      "Epoch 463/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1045.9714\n",
      "Epoch 00463: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1048.0489 - val_loss: 1094.5687\n",
      "Epoch 464/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1048.6301\n",
      "Epoch 00464: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1042.7844 - val_loss: 1085.4383\n",
      "Epoch 465/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.0157\n",
      "Epoch 00465: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1044.5389 - val_loss: 1089.6645\n",
      "Epoch 466/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1048.5711\n",
      "Epoch 00466: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1048.7164 - val_loss: 1090.6176\n",
      "Epoch 467/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1049.6504\n",
      "Epoch 00467: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1051.6496 - val_loss: 1082.3385\n",
      "Epoch 468/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1038.9901\n",
      "Epoch 00468: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1040.2696 - val_loss: 1082.6598\n",
      "Epoch 469/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.0961\n",
      "Epoch 00469: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1042.6100 - val_loss: 1084.5692\n",
      "Epoch 470/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1045.3586\n",
      "Epoch 00470: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1047.5344 - val_loss: 1081.7006\n",
      "Epoch 471/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1049.3642\n",
      "Epoch 00471: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1047.5937 - val_loss: 1081.6551\n",
      "Epoch 472/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1038.5699\n",
      "Epoch 00472: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1041.2380 - val_loss: 1086.1386\n",
      "Epoch 473/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1047.1079\n",
      "Epoch 00473: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1044.1831 - val_loss: 1083.7094\n",
      "Epoch 474/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.0404\n",
      "Epoch 00474: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1044.3987 - val_loss: 1081.2742\n",
      "Epoch 475/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1047.5958\n",
      "Epoch 00475: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1048.8784 - val_loss: 1082.7123\n",
      "Epoch 476/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1044.7438\n",
      "Epoch 00476: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1043.7668 - val_loss: 1081.6474\n",
      "Epoch 477/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1044.3763\n",
      "Epoch 00477: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1043.2826 - val_loss: 1083.9013\n",
      "Epoch 478/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1044.5969\n",
      "Epoch 00478: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1045.3820 - val_loss: 1083.7102\n",
      "Epoch 479/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1049.8461\n",
      "Epoch 00479: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1048.3139 - val_loss: 1081.5433\n",
      "Epoch 480/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1049.1624 ETA: 0s - loss: 1042.\n",
      "Epoch 00480: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1049.2075 - val_loss: 1082.7880\n",
      "Epoch 481/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1041.95 - ETA: 0s - loss: 1044.2159\n",
      "Epoch 00481: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1043.0028 - val_loss: 1080.1761\n",
      "Epoch 482/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1043.2033\n",
      "Epoch 00482: val_loss did not improve from 1079.95806\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1044.4052 - val_loss: 1081.9456\n",
      "Epoch 483/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1035.5780\n",
      "Epoch 00483: val_loss improved from 1079.95806 to 1077.85938, saving model to Weights\\Weights-483--1077.85938.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1033.4636 - val_loss: 1077.8594\n",
      "Epoch 484/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1043.6081\n",
      "Epoch 00484: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1043.0843 - val_loss: 1081.0226\n",
      "Epoch 485/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1044.7068\n",
      "Epoch 00485: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1044.4006 - val_loss: 1084.4479\n",
      "Epoch 486/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1045.4683\n",
      "Epoch 00486: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.0449 - val_loss: 1079.6908\n",
      "Epoch 487/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1040.1904\n",
      "Epoch 00487: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1041.9531 - val_loss: 1083.7413\n",
      "Epoch 488/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1045.6537\n",
      "Epoch 00488: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1047.8919 - val_loss: 1080.9141\n",
      "Epoch 489/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1048.4477\n",
      "Epoch 00489: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1049.5095 - val_loss: 1082.9307\n",
      "Epoch 490/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1044.7032\n",
      "Epoch 00490: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1042.0047 - val_loss: 1080.9853\n",
      "Epoch 491/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.3409\n",
      "Epoch 00491: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1047.8321 - val_loss: 1081.6074\n",
      "Epoch 492/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1039.9798\n",
      "Epoch 00492: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1042.4797 - val_loss: 1082.5834\n",
      "Epoch 493/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1045.3452\n",
      "Epoch 00493: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.0455 - val_loss: 1083.0475\n",
      "Epoch 494/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1046.4186\n",
      "Epoch 00494: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1044.7014 - val_loss: 1079.5191\n",
      "Epoch 495/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1047.9887\n",
      "Epoch 00495: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1047.4503 - val_loss: 1082.0391\n",
      "Epoch 496/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1038.9451\n",
      "Epoch 00496: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1037.9894 - val_loss: 1078.4664\n",
      "Epoch 497/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1038.1223 ETA: 0s - loss: 1040.\n",
      "Epoch 00497: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1039.1272 - val_loss: 1083.2505\n",
      "Epoch 498/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1041.9709\n",
      "Epoch 00498: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1041.7862 - val_loss: 1083.1376\n",
      "Epoch 499/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.5826\n",
      "Epoch 00499: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.0794 - val_loss: 1086.1844\n",
      "Epoch 500/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1036.0450\n",
      "Epoch 00500: val_loss did not improve from 1077.85938\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1035.6598 - val_loss: 1086.0494\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, decay=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=500, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=callbacks_list, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          10061.429842786558,
          9747.512606409477,
          9379.795234797975,
          8953.981211697288,
          8467.398702740405,
          7915.426091431472,
          7292.582545379706,
          6605.040405738514,
          5837.193471091244,
          5015.7334248734205,
          4167.739787705847,
          3340.590755299717,
          2542.0126731356995,
          1926.6900951996677,
          1583.9123908930799,
          1473.2194102126466,
          1431.7824405221154,
          1412.523026910984,
          1401.1521830783474,
          1393.9749416108837,
          1388.8544509180902,
          1371.041540035342,
          1364.3957486731126,
          1364.0659015442102,
          1358.0315510023327,
          1340.2357192910551,
          1343.5694501259916,
          1342.0133905282642,
          1334.702259027769,
          1331.0929734448237,
          1330.106733512993,
          1318.734308640943,
          1315.5775169003864,
          1316.7092286380134,
          1305.175517820944,
          1306.1208343108096,
          1296.9790817943058,
          1303.6744773226796,
          1294.5704784588368,
          1300.1430435930295,
          1298.266150316017,
          1293.728543700927,
          1292.2139896005003,
          1280.986274690171,
          1286.632043043358,
          1285.8442032536548,
          1287.733844054111,
          1281.013241091878,
          1282.844847678182,
          1274.2040818824255,
          1266.0912083330722,
          1278.692069816092,
          1255.9582071589257,
          1271.2633041464449,
          1262.5759436693606,
          1266.582113592998,
          1251.7666039368373,
          1264.1989587478224,
          1256.7477563246473,
          1259.4415279041916,
          1260.165956560299,
          1255.362235856195,
          1243.0830864838424,
          1255.0945338250544,
          1246.3407661188621,
          1241.2350368771306,
          1236.3335043916918,
          1241.227254411274,
          1237.576332008143,
          1244.4751093810705,
          1233.1928499205355,
          1238.0655089952627,
          1238.074321727703,
          1239.418109668145,
          1236.3994060093353,
          1221.2223141251238,
          1232.69836065959,
          1220.1989568630393,
          1222.5013848506117,
          1231.5711504942146,
          1231.3970562476109,
          1227.4570319353757,
          1218.0871192101417,
          1220.306573360268,
          1214.0949301435685,
          1211.8418415468109,
          1215.998153181671,
          1217.0972646067275,
          1214.3866687175719,
          1214.7152857439107,
          1213.6208471126492,
          1218.147405838187,
          1205.7067966556797,
          1210.8211030319455,
          1217.5681138636235,
          1207.9724367584236,
          1202.4003311686556,
          1207.7887099231439,
          1199.438867324575,
          1203.7362676650698,
          1213.531146190055,
          1200.9564096386935,
          1203.126920153532,
          1205.772170901179,
          1202.0849528843353,
          1197.2373076003469,
          1193.6715783057434,
          1192.5346138975008,
          1195.8559146358657,
          1195.5872760632724,
          1192.7469730381022,
          1186.711462644673,
          1184.9157578747713,
          1194.1411281392166,
          1189.9199839504588,
          1186.1107607807644,
          1184.87592719097,
          1182.4400114614405,
          1187.2967844814486,
          1176.2337671696412,
          1191.6290477800876,
          1181.4881221204428,
          1185.8570198776586,
          1178.113189801296,
          1179.0677007044487,
          1173.6991158457301,
          1177.6818765411163,
          1179.1753900718038,
          1179.6216114044955,
          1177.208905263059,
          1181.5835758910669,
          1177.1613543846813,
          1178.6515695446833,
          1179.7991258473203,
          1175.1166862700443,
          1176.0951791797345,
          1176.71908301917,
          1161.9027305711647,
          1170.0659154720242,
          1172.771510884354,
          1165.3575082568193,
          1174.7820835147947,
          1164.8597106627622,
          1166.9325336206168,
          1163.9468794647332,
          1165.115360288312,
          1164.525569165377,
          1154.3859386945119,
          1169.990885286119,
          1165.2038725246186,
          1161.2639636002825,
          1155.9401103758444,
          1159.5555129371521,
          1155.4812748497657,
          1158.584231575338,
          1165.9178282724538,
          1154.592677925497,
          1156.271957333207,
          1153.7497394837906,
          1164.3310417387943,
          1151.373617425815,
          1156.90469003589,
          1153.400639073701,
          1146.6583538342268,
          1151.316548000387,
          1150.7507845104303,
          1149.3011286620604,
          1159.1317466917892,
          1153.188455927795,
          1149.6280790749308,
          1154.1618710111134,
          1148.5638198867837,
          1152.2304923954625,
          1147.2141094678195,
          1143.6809793108566,
          1148.352778552361,
          1149.4279836069875,
          1150.0910263658168,
          1147.9289789194092,
          1137.7430740090251,
          1140.7614000258093,
          1148.743961766413,
          1138.49194096056,
          1142.001353005118,
          1151.017836438216,
          1139.7094916019932,
          1142.482898382474,
          1127.3021478647217,
          1128.9701420715344,
          1134.1350986196924,
          1143.0246643959842,
          1139.3171067284707,
          1141.301008647875,
          1139.4774612233996,
          1141.2192043306738,
          1143.2797038657932,
          1136.3158427242863,
          1133.63049255212,
          1123.5388866766484,
          1136.7377840588656,
          1143.0053014791583,
          1134.7894564441767,
          1125.3566208910745,
          1130.873704076889,
          1128.4062509546304,
          1124.625410074984,
          1131.289443250694,
          1128.4939593675979,
          1125.7969857371354,
          1131.1799821136517,
          1128.1263265936748,
          1127.7669970242944,
          1126.7420652338658,
          1124.614205870811,
          1124.5906756737438,
          1127.6342295632708,
          1130.1226757929992,
          1122.4034578527217,
          1129.3103307123913,
          1122.449168888916,
          1125.8973681914774,
          1124.9857678303433,
          1122.3481607110127,
          1122.3367318749451,
          1130.549483907182,
          1118.3790844232112,
          1117.9838808930563,
          1119.4793499720367,
          1122.1441841806275,
          1116.8554401845188,
          1120.4406551761142,
          1123.979973174394,
          1117.2257559841134,
          1124.0845832709967,
          1116.0230146231766,
          1122.6212847250508,
          1117.4219685782643,
          1114.8330727028947,
          1109.1673572969407,
          1117.4187217331469,
          1111.0811542138467,
          1117.9131541568624,
          1109.4383957371197,
          1111.292668530954,
          1106.9258095707041,
          1113.0336562317887,
          1114.4665492095855,
          1121.4921257672293,
          1111.6017299519767,
          1117.3594435865282,
          1111.7814998194524,
          1101.2063967878985,
          1110.4216768774595,
          1109.5250977247877,
          1114.6392774534102,
          1110.1033782071668,
          1110.0468119943885,
          1101.2830950559344,
          1113.2849334294556,
          1113.889680576726,
          1107.3270742700552,
          1110.9547304436849,
          1103.738433109679,
          1115.8399651104592,
          1105.5558150409815,
          1105.419531259791,
          1100.3505867207866,
          1100.6947611397054,
          1104.2133513443548,
          1101.3189190821918,
          1107.3097918846813,
          1106.0631208280201,
          1098.2711402919074,
          1099.6615377716437,
          1093.3694895791557,
          1100.9148262533367,
          1103.3569827694585,
          1104.1634673001365,
          1104.6200874226113,
          1100.0631564675577,
          1092.4096984924477,
          1096.808840142574,
          1096.8058720006,
          1091.594137457585,
          1094.501504289571,
          1094.7792499267628,
          1093.2412368104337,
          1099.117578041776,
          1099.9082670607643,
          1092.946746869008,
          1093.3003864735813,
          1087.2980450587543,
          1095.940784418394,
          1105.408026175674,
          1090.7899392913764,
          1090.936708195214,
          1092.8926134463277,
          1097.0974065774137,
          1090.6259536024131,
          1089.8682944062766,
          1090.4419645864405,
          1093.729885201524,
          1092.3096123774353,
          1088.300904764497,
          1082.9726827348763,
          1089.0565933585526,
          1090.37445488152,
          1089.6909428870533,
          1091.5735799063893,
          1086.960830385565,
          1093.9387102021897,
          1084.2199451484014,
          1086.9249514191,
          1078.0808454398427,
          1088.241347058015,
          1087.1416587424178,
          1083.425783722248,
          1082.7929623368414,
          1079.0532293386634,
          1080.3135634338735,
          1079.0189422919514,
          1082.3532777996227,
          1084.0675862099858,
          1077.8389736655147,
          1079.5949688428195,
          1085.272122190547,
          1085.6867038871378,
          1080.947204161484,
          1080.746276990096,
          1082.5169062364882,
          1080.875216725596,
          1079.9377471758603,
          1077.4725939052867,
          1080.0074168179062,
          1076.9155750018408,
          1076.3583022890766,
          1075.9855377399206,
          1078.5762234495235,
          1077.6554380255852,
          1078.3017792304915,
          1082.6691013578663,
          1083.2092046988184,
          1074.8192768238437,
          1078.0548196796044,
          1079.4460225002977,
          1078.7849928858,
          1078.3073773304734,
          1069.2081665504138,
          1078.1282324522274,
          1081.00094457014,
          1072.4442177346648,
          1078.6287951946545,
          1073.6117875225586,
          1069.4865919701772,
          1067.7653974063032,
          1071.451993199898,
          1079.1627230310526,
          1072.221165876004,
          1071.0100773118506,
          1069.6934743957336,
          1072.1948671771215,
          1069.8850360057243,
          1072.8713017125776,
          1065.7367723708596,
          1070.0910950624946,
          1078.678641677835,
          1071.0847062837008,
          1068.1232969637072,
          1061.096020331573,
          1066.2754582568975,
          1066.668007694028,
          1066.1786412861918,
          1071.807127168333,
          1064.5960803509045,
          1063.7072431045328,
          1066.4729422719774,
          1071.0149696339388,
          1068.8412234113382,
          1067.9084468495041,
          1061.3399311965995,
          1073.2921919745245,
          1063.778771730327,
          1070.040329441509,
          1068.7708258757928,
          1060.5060914481166,
          1071.810394525767,
          1063.8146727144874,
          1066.6757734171345,
          1072.074480465617,
          1059.3525354397998,
          1065.6939440984058,
          1053.8370295727877,
          1066.7367127186935,
          1063.2917128234583,
          1064.7900507383652,
          1062.66407060212,
          1070.7574992098596,
          1067.7772012163266,
          1056.545073477663,
          1065.6692884409622,
          1060.8263088644735,
          1062.091797609331,
          1064.547536167277,
          1059.2624078218603,
          1055.8364338221759,
          1065.6667561245174,
          1061.4675700644918,
          1061.2708613684642,
          1058.2811427631764,
          1055.9658485108157,
          1067.4442470711938,
          1056.1926683204456,
          1065.0762402657065,
          1060.9106344709135,
          1061.9835147555518,
          1059.9804790795913,
          1065.7620261654913,
          1060.172076720763,
          1055.4468698939509,
          1048.454500867294,
          1058.491071683839,
          1054.070186072656,
          1064.0076456354882,
          1044.6573828526434,
          1057.1345764612993,
          1057.9698334811135,
          1060.3812592427812,
          1045.2877594416382,
          1054.3328256249451,
          1063.065127387849,
          1063.8448372017244,
          1060.3395623787865,
          1054.0003834187637,
          1045.1749428886199,
          1058.136292348387,
          1054.4466770585946,
          1051.9958782482893,
          1052.4955344589291,
          1052.1311287766162,
          1048.8000691690975,
          1053.6883975851863,
          1049.7288154279252,
          1047.8850348552721,
          1052.0198604996037,
          1054.4931434033174,
          1047.3029430474312,
          1045.552393045649,
          1057.4301388688716,
          1046.508183067969,
          1052.4585152842703,
          1050.5570110705803,
          1052.556139003401,
          1048.0488928685286,
          1042.7843537974122,
          1044.5389368069873,
          1048.7163745611638,
          1051.6496091008496,
          1040.2696393924987,
          1042.6100001728125,
          1047.5343955074209,
          1047.5937247512477,
          1041.2379660995541,
          1044.1830674303972,
          1044.3986645062319,
          1048.8783520502916,
          1043.7667840070708,
          1043.2825576601704,
          1045.382042627237,
          1048.313906268603,
          1049.2075338017517,
          1043.0028413964199,
          1044.4052330003321,
          1033.463577897749,
          1043.0842563957306,
          1044.4006028222207,
          1045.0448631897034,
          1041.9531229193951,
          1047.8919341410715,
          1049.5094990138423,
          1042.0047066097438,
          1047.8321092849221,
          1042.4797011047083,
          1045.0455271474389,
          1044.7013852471505,
          1047.4502954997447,
          1037.9894386660199,
          1039.127194108001,
          1041.78619743364,
          1045.079439477211,
          1035.6597925847439
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          9519.096411020215,
          9180.088058639963,
          8717.61901490751,
          8255.427234638184,
          7774.809101613414,
          7044.844570884299,
          6429.232037575039,
          5659.098587088148,
          4875.597215455497,
          3413.2931911544233,
          2871.935494003158,
          2386.771117258387,
          1778.2291154266718,
          1536.2397880974909,
          2068.31106448054,
          2014.2339528085522,
          2460.1051892635696,
          2082.840838964511,
          1742.0740783458868,
          1569.2118997675207,
          1592.5279120243883,
          1472.045863972892,
          1521.049007473141,
          1368.9365830162328,
          1364.0352000406135,
          1302.0587257582606,
          1273.2594766901757,
          1316.8740902861111,
          1247.8695140323825,
          1242.6733582020283,
          1247.0179001781585,
          1233.530262422533,
          1232.7903465269467,
          1218.3600454932825,
          1220.2915351153233,
          1228.004409854293,
          1216.7292280975457,
          1210.8749447782986,
          1208.8220173721206,
          1209.7663842004838,
          1204.5027415763407,
          1196.974304260413,
          1200.764692350502,
          1201.5421553314006,
          1195.3875741233849,
          1193.7589586195593,
          1193.43729654132,
          1202.3000826073574,
          1198.1756272607608,
          1184.758288762697,
          1188.3892022089465,
          1176.9519914130253,
          1185.2117283012003,
          1185.2976001764744,
          1181.7302228469803,
          1169.2635037621253,
          1178.442826935971,
          1190.3658250220103,
          1177.364847088759,
          1172.0375015323043,
          1174.9417524381752,
          1183.585973629092,
          1177.391877915784,
          1173.219940546593,
          1172.996874148176,
          1193.552112225184,
          1179.9658356600207,
          1175.6436981268484,
          1171.7808480760916,
          1163.3595921417168,
          1167.4540654089303,
          1182.4527769515194,
          1167.9984025360468,
          1170.4144973219434,
          1165.494580097707,
          1160.9794743432533,
          1162.7259304367135,
          1163.889003009387,
          1162.6118505036925,
          1172.5580780047655,
          1162.7642402962547,
          1167.9029017437524,
          1157.5781337385406,
          1157.6486192959117,
          1157.2498089025607,
          1155.0815039757667,
          1154.5372970700384,
          1155.4816064002734,
          1157.2243176986728,
          1156.9907446372288,
          1156.5659184338265,
          1159.6367714994533,
          1157.7155537013423,
          1145.3459196058188,
          1156.6686621299364,
          1155.2118573476587,
          1144.6156793551907,
          1147.0690112897048,
          1153.684314740787,
          1153.2886695104537,
          1156.3718722144372,
          1142.853370398779,
          1147.2119066457556,
          1150.2095767354497,
          1148.4037579493793,
          1143.8690786719299,
          1151.8319087880443,
          1141.19911733887,
          1144.074148327644,
          1148.9529253108865,
          1142.4387332601875,
          1158.8677523230322,
          1142.4526570352448,
          1134.5852551595085,
          1150.3734737416894,
          1147.7756243626006,
          1146.9293704402933,
          1140.1311816484579,
          1139.4514823991597,
          1134.701540778485,
          1142.812833949323,
          1142.000835228234,
          1135.6191835099382,
          1146.0345976687827,
          1139.3450571338997,
          1134.9981002608736,
          1133.2615515917557,
          1139.131549377013,
          1129.6322354381157,
          1139.425039134954,
          1133.795819080782,
          1143.669239901674,
          1131.0674816779485,
          1128.6449329780867,
          1133.7870225281042,
          1144.8434832909313,
          1126.2527818421647,
          1134.6772360578912,
          1128.8071795750984,
          1130.423050884448,
          1128.1595329742095,
          1131.4827788333653,
          1124.5762139032188,
          1139.7501054499512,
          1128.3048975187048,
          1129.7011995348062,
          1131.8054398222487,
          1131.6750043031805,
          1124.2678239056313,
          1151.704746819269,
          1131.2059948150347,
          1128.0007987319766,
          1126.4663050689605,
          1123.7592165900874,
          1125.636928842138,
          1135.577234109466,
          1121.374304368115,
          1128.3047928520402,
          1143.4494432350282,
          1132.6031271148736,
          1120.5769566057295,
          1127.8179938249605,
          1117.1713442499326,
          1118.1810028339307,
          1124.8303614054746,
          1121.9368665659622,
          1118.2676293568356,
          1120.9927999811227,
          1123.1583147433328,
          1121.2326842270372,
          1119.568718903524,
          1113.2086084953692,
          1120.9228030232118,
          1116.0755533723618,
          1122.807846763891,
          1119.6035141073824,
          1124.4587975856368,
          1120.8979542856348,
          1123.3347282769184,
          1120.1117234888836,
          1117.8016518974725,
          1117.9427555100292,
          1114.5857625823235,
          1112.4108585398208,
          1112.580232873048,
          1121.879877427405,
          1121.6311932508897,
          1117.0630173362854,
          1119.403893506907,
          1111.8046433911763,
          1115.4950534965128,
          1119.8186837290627,
          1116.1085492201598,
          1112.378791596432,
          1122.6928684991133,
          1122.1761526746886,
          1112.258925329881,
          1116.3066716627293,
          1112.6853273834236,
          1116.7621307832003,
          1111.0670446285342,
          1118.8446167555176,
          1109.4317743711777,
          1114.6891837723392,
          1110.075678698209,
          1111.6134000403001,
          1120.1991596608214,
          1113.2112931120914,
          1123.0888610925515,
          1115.7535551907615,
          1118.01292510919,
          1108.8858495956674,
          1114.8747076627749,
          1111.0120776421038,
          1114.5940730567445,
          1124.2855690175315,
          1110.2489061648175,
          1109.9475664070333,
          1114.9544578354894,
          1111.2368081817413,
          1117.7541953561683,
          1112.7686179134107,
          1112.1380051757617,
          1120.8007520725762,
          1111.5011381398276,
          1102.6113708630721,
          1109.6516049883994,
          1113.6838356631538,
          1103.6898500799343,
          1110.6740035322307,
          1110.2759494509944,
          1105.6968105844728,
          1107.7353672037532,
          1108.5717396725627,
          1106.041552861659,
          1107.1488452740798,
          1123.284907801299,
          1106.2869457660229,
          1102.3974320782863,
          1108.7633741526797,
          1106.8039693731043,
          1110.2246679991024,
          1108.634318197039,
          1105.8447117534888,
          1102.6870335283847,
          1105.650818177065,
          1113.3051776660332,
          1107.2222475702451,
          1099.637440401685,
          1110.3808404782121,
          1106.8339038678298,
          1101.2107163684173,
          1099.8436918164964,
          1100.5091554447815,
          1108.338581214478,
          1107.7249957800436,
          1099.1211003834578,
          1109.7824940793328,
          1101.3361406159922,
          1107.6326874434076,
          1113.804882171184,
          1100.7312945298402,
          1101.466088637097,
          1104.2693972590455,
          1105.5431658689547,
          1104.7483085661393,
          1106.9522519537124,
          1110.8528254271653,
          1101.4091303404477,
          1105.7636749836686,
          1101.3271513013915,
          1106.9181472169437,
          1099.071627050644,
          1103.6607227130382,
          1103.6225170550854,
          1100.2215304714132,
          1100.3681286187839,
          1107.2557396790248,
          1096.8827017873425,
          1098.607317085947,
          1095.9099431226266,
          1104.1404956842869,
          1092.689681355117,
          1100.1597079653193,
          1098.753130747365,
          1095.529319321055,
          1091.8347992867393,
          1099.3396719899472,
          1106.1426819839003,
          1097.3922036161207,
          1090.877603938208,
          1096.9737219113445,
          1107.077121854713,
          1102.1850939150777,
          1098.3308333600955,
          1095.1492160378702,
          1104.732539686192,
          1094.8323633145396,
          1100.1690059437742,
          1094.318204871729,
          1100.7250395461795,
          1093.910050555272,
          1095.3185627847247,
          1096.9463982330622,
          1095.8737264005556,
          1101.4928025757595,
          1099.5587370424823,
          1098.6862375868666,
          1102.4247805280056,
          1095.9427409947505,
          1098.1065700998377,
          1100.1139179150948,
          1093.348736867222,
          1100.4818879183533,
          1093.8018308539704,
          1092.2786639549938,
          1092.665056466148,
          1096.1950547497713,
          1100.8879206542479,
          1094.315002453645,
          1093.453634087299,
          1095.5085421558945,
          1094.5392618709034,
          1096.0090826236105,
          1093.2777763689498,
          1096.3325183563202,
          1094.2381465736696,
          1091.2335741384632,
          1093.631178588745,
          1107.7816138121225,
          1093.3181789743176,
          1097.8952283505473,
          1097.2370624561358,
          1095.0430121000147,
          1088.4918082914014,
          1092.8757495317905,
          1091.4725628430795,
          1097.6127831287129,
          1099.6408032955999,
          1097.3253582850568,
          1096.6644703475129,
          1092.533395250503,
          1099.0904726067072,
          1096.0650121223384,
          1095.7262051499915,
          1093.0985835144031,
          1087.1940148634496,
          1090.586441177726,
          1095.4057038045012,
          1089.6033392141835,
          1091.8345343155877,
          1098.2872888700074,
          1089.4159036639794,
          1090.1116306449503,
          1091.0221626277148,
          1094.5445482717957,
          1087.4395845702734,
          1090.0143779343873,
          1092.3710033293594,
          1089.1009136205305,
          1088.9663711098074,
          1093.4253998139302,
          1085.6053175022637,
          1090.984540102117,
          1091.7972442461728,
          1097.5597068491359,
          1088.163255837247,
          1091.199914235019,
          1085.3663353576721,
          1090.5828301533204,
          1088.9560151560072,
          1087.7379976023599,
          1095.2747216982905,
          1092.0892615722755,
          1092.3077656692558,
          1091.0054763478795,
          1089.3436686605878,
          1093.7558577349937,
          1081.8383052773147,
          1092.748800078446,
          1093.6733096137848,
          1083.9213456451619,
          1089.6563555723396,
          1084.3416593347783,
          1089.5057228628418,
          1089.3171654749615,
          1095.4423855540028,
          1085.053639536036,
          1084.4654748097005,
          1089.9680011046298,
          1090.374424553644,
          1085.3834380277392,
          1085.3272494814644,
          1083.9576199632718,
          1085.8632073273322,
          1090.0877446154925,
          1086.1279740900561,
          1081.5385616372098,
          1087.7118940642154,
          1089.0174019834383,
          1087.1393258703097,
          1092.2351314100254,
          1085.3020858626962,
          1086.784158220553,
          1086.4306789449442,
          1085.1151067727262,
          1084.4055611729168,
          1088.9151427500565,
          1083.4411193223475,
          1084.8916280228987,
          1095.904643503774,
          1084.3403490432547,
          1085.3687367183975,
          1087.9184234967947,
          1089.8308598694496,
          1085.3482643496136,
          1086.688375591969,
          1082.3730632505844,
          1083.5700757516418,
          1084.0119211806166,
          1086.9556306357467,
          1082.503281187337,
          1079.958059925533,
          1089.381041880765,
          1089.088025815362,
          1090.1902915353355,
          1088.1839158521138,
          1081.9974472201943,
          1096.5592767513895,
          1095.2014380455591,
          1085.0492563134853,
          1082.1658290804137,
          1086.659857842304,
          1092.2887712397937,
          1091.7047767310241,
          1086.0554172538052,
          1084.7813871730561,
          1082.5084896267404,
          1081.018420402813,
          1084.6677017961545,
          1080.9799659289936,
          1085.245691801614,
          1089.9406586519483,
          1091.4342892594966,
          1083.0821230658696,
          1083.9893435089436,
          1082.3881495697408,
          1084.6257815241504,
          1085.2714595546076,
          1085.7268702092238,
          1082.1070763477228,
          1084.4151519536733,
          1094.568720323231,
          1085.4383485930607,
          1089.664501164943,
          1090.6175873922589,
          1082.3384609065602,
          1082.6597713136184,
          1084.5692268648486,
          1081.7005585616353,
          1081.6550964392186,
          1086.138644533404,
          1083.7094471015264,
          1081.2742159644372,
          1082.7123463632397,
          1081.6474142095622,
          1083.9012930690108,
          1083.7101645920018,
          1081.543307496953,
          1082.787994488796,
          1080.176081150836,
          1081.9456130127785,
          1077.8593799444964,
          1081.0225693737311,
          1084.4478591603795,
          1079.6908489171456,
          1083.7412738941562,
          1080.9140623776116,
          1082.9306543203736,
          1080.9853096077145,
          1081.6073855256088,
          1082.5834431239973,
          1083.0475046399936,
          1079.5191438168927,
          1082.0390914571244,
          1078.4663705712978,
          1083.2504604501,
          1083.137598136013,
          1086.1843612484256,
          1086.0493609311945
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506,
          1081.8032113475506
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          499
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"55035adf-a392-4622-8cea-c381d9bb28a1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"55035adf-a392-4622-8cea-c381d9bb28a1\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '55035adf-a392-4622-8cea-c381d9bb28a1',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [10061.429842786558, 9747.512606409477, 9379.795234797975, 8953.981211697288, 8467.398702740405, 7915.426091431472, 7292.582545379706, 6605.040405738514, 5837.193471091244, 5015.7334248734205, 4167.739787705847, 3340.590755299717, 2542.0126731356995, 1926.6900951996677, 1583.9123908930799, 1473.2194102126466, 1431.7824405221154, 1412.523026910984, 1401.1521830783474, 1393.9749416108837, 1388.8544509180902, 1371.041540035342, 1364.3957486731126, 1364.0659015442102, 1358.0315510023327, 1340.2357192910551, 1343.5694501259916, 1342.0133905282642, 1334.702259027769, 1331.0929734448237, 1330.106733512993, 1318.734308640943, 1315.5775169003864, 1316.7092286380134, 1305.175517820944, 1306.1208343108096, 1296.9790817943058, 1303.6744773226796, 1294.5704784588368, 1300.1430435930295, 1298.266150316017, 1293.728543700927, 1292.2139896005003, 1280.986274690171, 1286.632043043358, 1285.8442032536548, 1287.733844054111, 1281.013241091878, 1282.844847678182, 1274.2040818824255, 1266.0912083330722, 1278.692069816092, 1255.9582071589257, 1271.2633041464449, 1262.5759436693606, 1266.582113592998, 1251.7666039368373, 1264.1989587478224, 1256.7477563246473, 1259.4415279041916, 1260.165956560299, 1255.362235856195, 1243.0830864838424, 1255.0945338250544, 1246.3407661188621, 1241.2350368771306, 1236.3335043916918, 1241.227254411274, 1237.576332008143, 1244.4751093810705, 1233.1928499205355, 1238.0655089952627, 1238.074321727703, 1239.418109668145, 1236.3994060093353, 1221.2223141251238, 1232.69836065959, 1220.1989568630393, 1222.5013848506117, 1231.5711504942146, 1231.3970562476109, 1227.4570319353757, 1218.0871192101417, 1220.306573360268, 1214.0949301435685, 1211.8418415468109, 1215.998153181671, 1217.0972646067275, 1214.3866687175719, 1214.7152857439107, 1213.6208471126492, 1218.147405838187, 1205.7067966556797, 1210.8211030319455, 1217.5681138636235, 1207.9724367584236, 1202.4003311686556, 1207.7887099231439, 1199.438867324575, 1203.7362676650698, 1213.531146190055, 1200.9564096386935, 1203.126920153532, 1205.772170901179, 1202.0849528843353, 1197.2373076003469, 1193.6715783057434, 1192.5346138975008, 1195.8559146358657, 1195.5872760632724, 1192.7469730381022, 1186.711462644673, 1184.9157578747713, 1194.1411281392166, 1189.9199839504588, 1186.1107607807644, 1184.87592719097, 1182.4400114614405, 1187.2967844814486, 1176.2337671696412, 1191.6290477800876, 1181.4881221204428, 1185.8570198776586, 1178.113189801296, 1179.0677007044487, 1173.6991158457301, 1177.6818765411163, 1179.1753900718038, 1179.6216114044955, 1177.208905263059, 1181.5835758910669, 1177.1613543846813, 1178.6515695446833, 1179.7991258473203, 1175.1166862700443, 1176.0951791797345, 1176.71908301917, 1161.9027305711647, 1170.0659154720242, 1172.771510884354, 1165.3575082568193, 1174.7820835147947, 1164.8597106627622, 1166.9325336206168, 1163.9468794647332, 1165.115360288312, 1164.525569165377, 1154.3859386945119, 1169.990885286119, 1165.2038725246186, 1161.2639636002825, 1155.9401103758444, 1159.5555129371521, 1155.4812748497657, 1158.584231575338, 1165.9178282724538, 1154.592677925497, 1156.271957333207, 1153.7497394837906, 1164.3310417387943, 1151.373617425815, 1156.90469003589, 1153.400639073701, 1146.6583538342268, 1151.316548000387, 1150.7507845104303, 1149.3011286620604, 1159.1317466917892, 1153.188455927795, 1149.6280790749308, 1154.1618710111134, 1148.5638198867837, 1152.2304923954625, 1147.2141094678195, 1143.6809793108566, 1148.352778552361, 1149.4279836069875, 1150.0910263658168, 1147.9289789194092, 1137.7430740090251, 1140.7614000258093, 1148.743961766413, 1138.49194096056, 1142.001353005118, 1151.017836438216, 1139.7094916019932, 1142.482898382474, 1127.3021478647217, 1128.9701420715344, 1134.1350986196924, 1143.0246643959842, 1139.3171067284707, 1141.301008647875, 1139.4774612233996, 1141.2192043306738, 1143.2797038657932, 1136.3158427242863, 1133.63049255212, 1123.5388866766484, 1136.7377840588656, 1143.0053014791583, 1134.7894564441767, 1125.3566208910745, 1130.873704076889, 1128.4062509546304, 1124.625410074984, 1131.289443250694, 1128.4939593675979, 1125.7969857371354, 1131.1799821136517, 1128.1263265936748, 1127.7669970242944, 1126.7420652338658, 1124.614205870811, 1124.5906756737438, 1127.6342295632708, 1130.1226757929992, 1122.4034578527217, 1129.3103307123913, 1122.449168888916, 1125.8973681914774, 1124.9857678303433, 1122.3481607110127, 1122.3367318749451, 1130.549483907182, 1118.3790844232112, 1117.9838808930563, 1119.4793499720367, 1122.1441841806275, 1116.8554401845188, 1120.4406551761142, 1123.979973174394, 1117.2257559841134, 1124.0845832709967, 1116.0230146231766, 1122.6212847250508, 1117.4219685782643, 1114.8330727028947, 1109.1673572969407, 1117.4187217331469, 1111.0811542138467, 1117.9131541568624, 1109.4383957371197, 1111.292668530954, 1106.9258095707041, 1113.0336562317887, 1114.4665492095855, 1121.4921257672293, 1111.6017299519767, 1117.3594435865282, 1111.7814998194524, 1101.2063967878985, 1110.4216768774595, 1109.5250977247877, 1114.6392774534102, 1110.1033782071668, 1110.0468119943885, 1101.2830950559344, 1113.2849334294556, 1113.889680576726, 1107.3270742700552, 1110.9547304436849, 1103.738433109679, 1115.8399651104592, 1105.5558150409815, 1105.419531259791, 1100.3505867207866, 1100.6947611397054, 1104.2133513443548, 1101.3189190821918, 1107.3097918846813, 1106.0631208280201, 1098.2711402919074, 1099.6615377716437, 1093.3694895791557, 1100.9148262533367, 1103.3569827694585, 1104.1634673001365, 1104.6200874226113, 1100.0631564675577, 1092.4096984924477, 1096.808840142574, 1096.8058720006, 1091.594137457585, 1094.501504289571, 1094.7792499267628, 1093.2412368104337, 1099.117578041776, 1099.9082670607643, 1092.946746869008, 1093.3003864735813, 1087.2980450587543, 1095.940784418394, 1105.408026175674, 1090.7899392913764, 1090.936708195214, 1092.8926134463277, 1097.0974065774137, 1090.6259536024131, 1089.8682944062766, 1090.4419645864405, 1093.729885201524, 1092.3096123774353, 1088.300904764497, 1082.9726827348763, 1089.0565933585526, 1090.37445488152, 1089.6909428870533, 1091.5735799063893, 1086.960830385565, 1093.9387102021897, 1084.2199451484014, 1086.9249514191, 1078.0808454398427, 1088.241347058015, 1087.1416587424178, 1083.425783722248, 1082.7929623368414, 1079.0532293386634, 1080.3135634338735, 1079.0189422919514, 1082.3532777996227, 1084.0675862099858, 1077.8389736655147, 1079.5949688428195, 1085.272122190547, 1085.6867038871378, 1080.947204161484, 1080.746276990096, 1082.5169062364882, 1080.875216725596, 1079.9377471758603, 1077.4725939052867, 1080.0074168179062, 1076.9155750018408, 1076.3583022890766, 1075.9855377399206, 1078.5762234495235, 1077.6554380255852, 1078.3017792304915, 1082.6691013578663, 1083.2092046988184, 1074.8192768238437, 1078.0548196796044, 1079.4460225002977, 1078.7849928858, 1078.3073773304734, 1069.2081665504138, 1078.1282324522274, 1081.00094457014, 1072.4442177346648, 1078.6287951946545, 1073.6117875225586, 1069.4865919701772, 1067.7653974063032, 1071.451993199898, 1079.1627230310526, 1072.221165876004, 1071.0100773118506, 1069.6934743957336, 1072.1948671771215, 1069.8850360057243, 1072.8713017125776, 1065.7367723708596, 1070.0910950624946, 1078.678641677835, 1071.0847062837008, 1068.1232969637072, 1061.096020331573, 1066.2754582568975, 1066.668007694028, 1066.1786412861918, 1071.807127168333, 1064.5960803509045, 1063.7072431045328, 1066.4729422719774, 1071.0149696339388, 1068.8412234113382, 1067.9084468495041, 1061.3399311965995, 1073.2921919745245, 1063.778771730327, 1070.040329441509, 1068.7708258757928, 1060.5060914481166, 1071.810394525767, 1063.8146727144874, 1066.6757734171345, 1072.074480465617, 1059.3525354397998, 1065.6939440984058, 1053.8370295727877, 1066.7367127186935, 1063.2917128234583, 1064.7900507383652, 1062.66407060212, 1070.7574992098596, 1067.7772012163266, 1056.545073477663, 1065.6692884409622, 1060.8263088644735, 1062.091797609331, 1064.547536167277, 1059.2624078218603, 1055.8364338221759, 1065.6667561245174, 1061.4675700644918, 1061.2708613684642, 1058.2811427631764, 1055.9658485108157, 1067.4442470711938, 1056.1926683204456, 1065.0762402657065, 1060.9106344709135, 1061.9835147555518, 1059.9804790795913, 1065.7620261654913, 1060.172076720763, 1055.4468698939509, 1048.454500867294, 1058.491071683839, 1054.070186072656, 1064.0076456354882, 1044.6573828526434, 1057.1345764612993, 1057.9698334811135, 1060.3812592427812, 1045.2877594416382, 1054.3328256249451, 1063.065127387849, 1063.8448372017244, 1060.3395623787865, 1054.0003834187637, 1045.1749428886199, 1058.136292348387, 1054.4466770585946, 1051.9958782482893, 1052.4955344589291, 1052.1311287766162, 1048.8000691690975, 1053.6883975851863, 1049.7288154279252, 1047.8850348552721, 1052.0198604996037, 1054.4931434033174, 1047.3029430474312, 1045.552393045649, 1057.4301388688716, 1046.508183067969, 1052.4585152842703, 1050.5570110705803, 1052.556139003401, 1048.0488928685286, 1042.7843537974122, 1044.5389368069873, 1048.7163745611638, 1051.6496091008496, 1040.2696393924987, 1042.6100001728125, 1047.5343955074209, 1047.5937247512477, 1041.2379660995541, 1044.1830674303972, 1044.3986645062319, 1048.8783520502916, 1043.7667840070708, 1043.2825576601704, 1045.382042627237, 1048.313906268603, 1049.2075338017517, 1043.0028413964199, 1044.4052330003321, 1033.463577897749, 1043.0842563957306, 1044.4006028222207, 1045.0448631897034, 1041.9531229193951, 1047.8919341410715, 1049.5094990138423, 1042.0047066097438, 1047.8321092849221, 1042.4797011047083, 1045.0455271474389, 1044.7013852471505, 1047.4502954997447, 1037.9894386660199, 1039.127194108001, 1041.78619743364, 1045.079439477211, 1035.6597925847439]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [9519.096411020215, 9180.088058639963, 8717.61901490751, 8255.427234638184, 7774.809101613414, 7044.844570884299, 6429.232037575039, 5659.098587088148, 4875.597215455497, 3413.2931911544233, 2871.935494003158, 2386.771117258387, 1778.2291154266718, 1536.2397880974909, 2068.31106448054, 2014.2339528085522, 2460.1051892635696, 2082.840838964511, 1742.0740783458868, 1569.2118997675207, 1592.5279120243883, 1472.045863972892, 1521.049007473141, 1368.9365830162328, 1364.0352000406135, 1302.0587257582606, 1273.2594766901757, 1316.8740902861111, 1247.8695140323825, 1242.6733582020283, 1247.0179001781585, 1233.530262422533, 1232.7903465269467, 1218.3600454932825, 1220.2915351153233, 1228.004409854293, 1216.7292280975457, 1210.8749447782986, 1208.8220173721206, 1209.7663842004838, 1204.5027415763407, 1196.974304260413, 1200.764692350502, 1201.5421553314006, 1195.3875741233849, 1193.7589586195593, 1193.43729654132, 1202.3000826073574, 1198.1756272607608, 1184.758288762697, 1188.3892022089465, 1176.9519914130253, 1185.2117283012003, 1185.2976001764744, 1181.7302228469803, 1169.2635037621253, 1178.442826935971, 1190.3658250220103, 1177.364847088759, 1172.0375015323043, 1174.9417524381752, 1183.585973629092, 1177.391877915784, 1173.219940546593, 1172.996874148176, 1193.552112225184, 1179.9658356600207, 1175.6436981268484, 1171.7808480760916, 1163.3595921417168, 1167.4540654089303, 1182.4527769515194, 1167.9984025360468, 1170.4144973219434, 1165.494580097707, 1160.9794743432533, 1162.7259304367135, 1163.889003009387, 1162.6118505036925, 1172.5580780047655, 1162.7642402962547, 1167.9029017437524, 1157.5781337385406, 1157.6486192959117, 1157.2498089025607, 1155.0815039757667, 1154.5372970700384, 1155.4816064002734, 1157.2243176986728, 1156.9907446372288, 1156.5659184338265, 1159.6367714994533, 1157.7155537013423, 1145.3459196058188, 1156.6686621299364, 1155.2118573476587, 1144.6156793551907, 1147.0690112897048, 1153.684314740787, 1153.2886695104537, 1156.3718722144372, 1142.853370398779, 1147.2119066457556, 1150.2095767354497, 1148.4037579493793, 1143.8690786719299, 1151.8319087880443, 1141.19911733887, 1144.074148327644, 1148.9529253108865, 1142.4387332601875, 1158.8677523230322, 1142.4526570352448, 1134.5852551595085, 1150.3734737416894, 1147.7756243626006, 1146.9293704402933, 1140.1311816484579, 1139.4514823991597, 1134.701540778485, 1142.812833949323, 1142.000835228234, 1135.6191835099382, 1146.0345976687827, 1139.3450571338997, 1134.9981002608736, 1133.2615515917557, 1139.131549377013, 1129.6322354381157, 1139.425039134954, 1133.795819080782, 1143.669239901674, 1131.0674816779485, 1128.6449329780867, 1133.7870225281042, 1144.8434832909313, 1126.2527818421647, 1134.6772360578912, 1128.8071795750984, 1130.423050884448, 1128.1595329742095, 1131.4827788333653, 1124.5762139032188, 1139.7501054499512, 1128.3048975187048, 1129.7011995348062, 1131.8054398222487, 1131.6750043031805, 1124.2678239056313, 1151.704746819269, 1131.2059948150347, 1128.0007987319766, 1126.4663050689605, 1123.7592165900874, 1125.636928842138, 1135.577234109466, 1121.374304368115, 1128.3047928520402, 1143.4494432350282, 1132.6031271148736, 1120.5769566057295, 1127.8179938249605, 1117.1713442499326, 1118.1810028339307, 1124.8303614054746, 1121.9368665659622, 1118.2676293568356, 1120.9927999811227, 1123.1583147433328, 1121.2326842270372, 1119.568718903524, 1113.2086084953692, 1120.9228030232118, 1116.0755533723618, 1122.807846763891, 1119.6035141073824, 1124.4587975856368, 1120.8979542856348, 1123.3347282769184, 1120.1117234888836, 1117.8016518974725, 1117.9427555100292, 1114.5857625823235, 1112.4108585398208, 1112.580232873048, 1121.879877427405, 1121.6311932508897, 1117.0630173362854, 1119.403893506907, 1111.8046433911763, 1115.4950534965128, 1119.8186837290627, 1116.1085492201598, 1112.378791596432, 1122.6928684991133, 1122.1761526746886, 1112.258925329881, 1116.3066716627293, 1112.6853273834236, 1116.7621307832003, 1111.0670446285342, 1118.8446167555176, 1109.4317743711777, 1114.6891837723392, 1110.075678698209, 1111.6134000403001, 1120.1991596608214, 1113.2112931120914, 1123.0888610925515, 1115.7535551907615, 1118.01292510919, 1108.8858495956674, 1114.8747076627749, 1111.0120776421038, 1114.5940730567445, 1124.2855690175315, 1110.2489061648175, 1109.9475664070333, 1114.9544578354894, 1111.2368081817413, 1117.7541953561683, 1112.7686179134107, 1112.1380051757617, 1120.8007520725762, 1111.5011381398276, 1102.6113708630721, 1109.6516049883994, 1113.6838356631538, 1103.6898500799343, 1110.6740035322307, 1110.2759494509944, 1105.6968105844728, 1107.7353672037532, 1108.5717396725627, 1106.041552861659, 1107.1488452740798, 1123.284907801299, 1106.2869457660229, 1102.3974320782863, 1108.7633741526797, 1106.8039693731043, 1110.2246679991024, 1108.634318197039, 1105.8447117534888, 1102.6870335283847, 1105.650818177065, 1113.3051776660332, 1107.2222475702451, 1099.637440401685, 1110.3808404782121, 1106.8339038678298, 1101.2107163684173, 1099.8436918164964, 1100.5091554447815, 1108.338581214478, 1107.7249957800436, 1099.1211003834578, 1109.7824940793328, 1101.3361406159922, 1107.6326874434076, 1113.804882171184, 1100.7312945298402, 1101.466088637097, 1104.2693972590455, 1105.5431658689547, 1104.7483085661393, 1106.9522519537124, 1110.8528254271653, 1101.4091303404477, 1105.7636749836686, 1101.3271513013915, 1106.9181472169437, 1099.071627050644, 1103.6607227130382, 1103.6225170550854, 1100.2215304714132, 1100.3681286187839, 1107.2557396790248, 1096.8827017873425, 1098.607317085947, 1095.9099431226266, 1104.1404956842869, 1092.689681355117, 1100.1597079653193, 1098.753130747365, 1095.529319321055, 1091.8347992867393, 1099.3396719899472, 1106.1426819839003, 1097.3922036161207, 1090.877603938208, 1096.9737219113445, 1107.077121854713, 1102.1850939150777, 1098.3308333600955, 1095.1492160378702, 1104.732539686192, 1094.8323633145396, 1100.1690059437742, 1094.318204871729, 1100.7250395461795, 1093.910050555272, 1095.3185627847247, 1096.9463982330622, 1095.8737264005556, 1101.4928025757595, 1099.5587370424823, 1098.6862375868666, 1102.4247805280056, 1095.9427409947505, 1098.1065700998377, 1100.1139179150948, 1093.348736867222, 1100.4818879183533, 1093.8018308539704, 1092.2786639549938, 1092.665056466148, 1096.1950547497713, 1100.8879206542479, 1094.315002453645, 1093.453634087299, 1095.5085421558945, 1094.5392618709034, 1096.0090826236105, 1093.2777763689498, 1096.3325183563202, 1094.2381465736696, 1091.2335741384632, 1093.631178588745, 1107.7816138121225, 1093.3181789743176, 1097.8952283505473, 1097.2370624561358, 1095.0430121000147, 1088.4918082914014, 1092.8757495317905, 1091.4725628430795, 1097.6127831287129, 1099.6408032955999, 1097.3253582850568, 1096.6644703475129, 1092.533395250503, 1099.0904726067072, 1096.0650121223384, 1095.7262051499915, 1093.0985835144031, 1087.1940148634496, 1090.586441177726, 1095.4057038045012, 1089.6033392141835, 1091.8345343155877, 1098.2872888700074, 1089.4159036639794, 1090.1116306449503, 1091.0221626277148, 1094.5445482717957, 1087.4395845702734, 1090.0143779343873, 1092.3710033293594, 1089.1009136205305, 1088.9663711098074, 1093.4253998139302, 1085.6053175022637, 1090.984540102117, 1091.7972442461728, 1097.5597068491359, 1088.163255837247, 1091.199914235019, 1085.3663353576721, 1090.5828301533204, 1088.9560151560072, 1087.7379976023599, 1095.2747216982905, 1092.0892615722755, 1092.3077656692558, 1091.0054763478795, 1089.3436686605878, 1093.7558577349937, 1081.8383052773147, 1092.748800078446, 1093.6733096137848, 1083.9213456451619, 1089.6563555723396, 1084.3416593347783, 1089.5057228628418, 1089.3171654749615, 1095.4423855540028, 1085.053639536036, 1084.4654748097005, 1089.9680011046298, 1090.374424553644, 1085.3834380277392, 1085.3272494814644, 1083.9576199632718, 1085.8632073273322, 1090.0877446154925, 1086.1279740900561, 1081.5385616372098, 1087.7118940642154, 1089.0174019834383, 1087.1393258703097, 1092.2351314100254, 1085.3020858626962, 1086.784158220553, 1086.4306789449442, 1085.1151067727262, 1084.4055611729168, 1088.9151427500565, 1083.4411193223475, 1084.8916280228987, 1095.904643503774, 1084.3403490432547, 1085.3687367183975, 1087.9184234967947, 1089.8308598694496, 1085.3482643496136, 1086.688375591969, 1082.3730632505844, 1083.5700757516418, 1084.0119211806166, 1086.9556306357467, 1082.503281187337, 1079.958059925533, 1089.381041880765, 1089.088025815362, 1090.1902915353355, 1088.1839158521138, 1081.9974472201943, 1096.5592767513895, 1095.2014380455591, 1085.0492563134853, 1082.1658290804137, 1086.659857842304, 1092.2887712397937, 1091.7047767310241, 1086.0554172538052, 1084.7813871730561, 1082.5084896267404, 1081.018420402813, 1084.6677017961545, 1080.9799659289936, 1085.245691801614, 1089.9406586519483, 1091.4342892594966, 1083.0821230658696, 1083.9893435089436, 1082.3881495697408, 1084.6257815241504, 1085.2714595546076, 1085.7268702092238, 1082.1070763477228, 1084.4151519536733, 1094.568720323231, 1085.4383485930607, 1089.664501164943, 1090.6175873922589, 1082.3384609065602, 1082.6597713136184, 1084.5692268648486, 1081.7005585616353, 1081.6550964392186, 1086.138644533404, 1083.7094471015264, 1081.2742159644372, 1082.7123463632397, 1081.6474142095622, 1083.9012930690108, 1083.7101645920018, 1081.543307496953, 1082.787994488796, 1080.176081150836, 1081.9456130127785, 1077.8593799444964, 1081.0225693737311, 1084.4478591603795, 1079.6908489171456, 1083.7412738941562, 1080.9140623776116, 1082.9306543203736, 1080.9853096077145, 1081.6073855256088, 1082.5834431239973, 1083.0475046399936, 1079.5191438168927, 1082.0390914571244, 1078.4663705712978, 1083.2504604501, 1083.137598136013, 1086.1843612484256, 1086.0493609311945]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506, 1081.8032113475506]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 499], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('55035adf-a392-4622-8cea-c381d9bb28a1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('Weights/*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file =  latest_file # choose the best checkpoint \n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.37% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1078   : Mean absolute error \n",
      "\n",
      "9.20% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "528px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
