{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import timeit \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cufflinks wrapper on plotly\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "\n",
    "# Set global theme\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geojsoncontour\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_summary(model, X_test, y_test ):\n",
    "    \n",
    "    y_hat=model.predict(X_test)\n",
    "    \n",
    "    df_summary=pd.DataFrame(y_hat, columns=[\"y_hat\"])\n",
    "    df_summary[\"y_true\"]=y_test\n",
    "    df_summary[\"abs_error\"]=np.abs(df_summary.y_true-df_summary.y_hat)\n",
    "    df_summary[\"error\"]=df_summary.y_hat-df_summary.y_true\n",
    "    df_summary[\"relative_error\"]= df_summary[\"error\"]/df_summary.y_true\n",
    "    df_summary[\"relative_abs_error\"]= df_summary[\"abs_error\"]/df_summary.y_true\n",
    "    \n",
    "    share_within_5pct=(df_summary.query(\"relative_abs_error<0.05\").shape[0]/df_summary.shape[0])*100\n",
    "    \n",
    "    print(\"{:.2f}% : Share of forecasts within 5% absolute error\\n\".format(share_within_5pct))\n",
    "    print(\"{:.0f}   : Mean absolute error \\n\".format(df_summary.abs_error.mean()))\n",
    "    print(\"{:.2f}% : Mean absolute percentage error\\n\".format(df_summary.relative_abs_error.mean()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_plot_loss(history, starting_epoch,previous_val_loss):\n",
    "\n",
    "        trace0=go.Scatter(\n",
    "                y=history.history['loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"blue\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Training Loss\"\n",
    "            )\n",
    "\n",
    "\n",
    "        trace1=go.Scatter(\n",
    "                y=history.history['val_loss'][starting_epoch:],\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"red\",\n",
    "                size=5,\n",
    "                opacity=0.5\n",
    "                ),\n",
    "                name=\"Validation Loss\"\n",
    "            )\n",
    "        \n",
    "        trace2=go.Scatter(\n",
    "                y=list(np.ones([len(history.epoch[starting_epoch:])])*np.asarray(previous_val_loss).min()),\n",
    "                x=history.epoch[starting_epoch:],\n",
    "                mode='lines',\n",
    "                marker=dict(\n",
    "                color=\"grey\",\n",
    "                size=5,\n",
    "\n",
    "                ),\n",
    "                name=\"Lowest error from previous models\"\n",
    "            )\n",
    "\n",
    "        data=[trace0, trace1,trace2]\n",
    "        figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning curve\",\n",
    "                yaxis=dict(title=\"Loss\",range=(900,1500)),\n",
    "                xaxis=dict(title=\"Epoch\",range=(starting_epoch,history.epoch[-1])),\n",
    "                legend=dict(\n",
    "                    x=0.57,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None,\n",
    "\n",
    "\n",
    "\n",
    "            )))\n",
    "        iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a central city point to center all graphs around - Swietokrzyska Subway \n",
    "center_coors=52.235176, 21.008393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24935, 46)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"https://raw.githubusercontent.com/Jan-Majewski/Project_Portfolio/master/03_Real_Estate_pricing_in_Warsaw/top_features_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.unit_price\n",
    "X=df.drop(columns=[\"unit_price\",\"lat_mod\",\"lon_mod\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx=np.asarray(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_coors=df[[ \"unit_price\",\"lat_mod\",\"lon_mod\"]].iloc[test_idx]\n",
    "X_test_coors.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['build_year', 'building_floors_num', 'rooms_num',\n",
       "       'Equipment_types_dishwasher', 'Equipment_types_fridge',\n",
       "       'Equipment_types_furniture', 'Equipment_types_tv',\n",
       "       'Equipment_types_washing_machine', 'Extras_types_air_conditioning',\n",
       "       'Extras_types_balcony', 'Extras_types_garden', 'Extras_types_lift',\n",
       "       'floor_num', 'east_bank', 'distance_driving', 'distance_transit',\n",
       "       'time_driving', 'time_transit', 'restaurant_price_level',\n",
       "       'restaurant_mean_rating', 'restaurant_mean_popularity',\n",
       "       'restaurant_count', 'restaurant_ratings_count', 'district_Bemowo',\n",
       "       'district_Bialoleka', 'district_Downtown', 'district_Subburbs',\n",
       "       'district_Targowek', 'district_Wawer', 'district_Wola',\n",
       "       'district_Zoliborz', 'market_primary', 'Building_material_brick',\n",
       "       'Building_ownership_full_ownership', 'Building_type_apartment',\n",
       "       'Building_type_block', 'Building_type_tenement',\n",
       "       'Construction_status_ready_to_use', 'Construction_status_to_completion',\n",
       "       'Heating_urban', 'Windows_type_aluminium', 'Windows_type_plastic',\n",
       "       'Windows_type_wooden', 'unit_price', 'lat_mod', 'lon_mod'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and scaling data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19948, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np.asarray(y_train).reshape(-1,1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11195.887256867856"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=np.asarray(y_test).reshape(-1,1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11128.766392620813"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19948, 43)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4987, 43)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),  \n",
    "    keras.layers.Dense(units=256,activation='relu'),    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Initial_model\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Initial_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 2s 120us/sample - loss: 11118.3712 - val_loss: 10586.6312\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 7066.7532 - val_loss: 3763.7942\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 2464.6214 - val_loss: 1684.2396\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1628.4329 - val_loss: 1496.9290\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1463.1655 - val_loss: 1407.6187\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1387.9316 - val_loss: 1362.7425\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1347.6676 - val_loss: 1336.8407\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1319.9523 - val_loss: 1317.9880\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1298.9916 - val_loss: 1306.9283\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1286.6858 - val_loss: 1288.8704\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1266.8040 - val_loss: 1279.4128\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1256.6694 - val_loss: 1271.6448\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1245.9511 - val_loss: 1266.3097\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1237.1841 - val_loss: 1258.2871\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1230.7500 - val_loss: 1257.5023\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1223.4087 - val_loss: 1249.9370\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1215.9765 - val_loss: 1256.2207\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1211.6661 - val_loss: 1244.0647\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1204.9755 - val_loss: 1249.5841\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1202.4088 - val_loss: 1239.5859\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1196.8165 - val_loss: 1248.8297\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1197.3049 - val_loss: 1235.9305\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1192.5993 - val_loss: 1231.9216\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1186.0912 - val_loss: 1229.1004\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1180.6775 - val_loss: 1225.9160\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1177.1427 - val_loss: 1231.5199\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1175.5162 - val_loss: 1233.9612\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1171.2238 - val_loss: 1223.9249\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1166.3704 - val_loss: 1221.0827\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1162.4829 - val_loss: 1219.5647\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1160.2501 - val_loss: 1217.6306\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1156.4742 - val_loss: 1221.0852\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1156.6641 - val_loss: 1216.4021\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1152.9806 - val_loss: 1214.6491\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1150.6007 - val_loss: 1219.1764\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1153.9309 - val_loss: 1214.5239\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1146.1914 - val_loss: 1247.7888\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1159.2666 - val_loss: 1213.4075\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1143.4233 - val_loss: 1218.0178\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1137.7066 - val_loss: 1208.7369\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1134.1124 - val_loss: 1207.2179\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1131.8415 - val_loss: 1204.9409\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1130.3245 - val_loss: 1208.8396\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1130.8772 - val_loss: 1201.9245\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1129.1417 - val_loss: 1212.0412\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1125.0756 - val_loss: 1204.3166\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1119.0867 - val_loss: 1200.1024\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1114.3355 - val_loss: 1208.9757\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1112.9744 - val_loss: 1202.9052\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1113.0176 - val_loss: 1198.6039\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1114.2916 - val_loss: 1201.4043\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1109.3831 - val_loss: 1194.6965\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1103.0212 - val_loss: 1211.9289\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1112.4735 - val_loss: 1204.8148\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1102.6531 - val_loss: 1203.2134\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1095.9947 - val_loss: 1201.3105\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1103.0027 - val_loss: 1209.9772\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1103.7810 - val_loss: 1189.8448\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1084.8380 - val_loss: 1193.1177\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1088.1904 - val_loss: 1188.1846\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1079.3261 - val_loss: 1190.3044\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1088.5572 - val_loss: 1205.2590\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1090.6995 - val_loss: 1206.4373\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1077.6967 - val_loss: 1187.0819\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1068.5014 - val_loss: 1192.3847\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1080.6453 - val_loss: 1184.4074\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1067.9027 - val_loss: 1183.2887\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1066.2194 - val_loss: 1184.6275\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1055.8465 - val_loss: 1180.4819\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1053.8628 - val_loss: 1177.9208\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1062.6712 - val_loss: 1193.4336\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1047.9060 - val_loss: 1177.2465\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1045.4430 - val_loss: 1186.7017\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1040.1967 - val_loss: 1177.5787\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1038.5179 - val_loss: 1178.2507\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1030.5437 - val_loss: 1170.2154\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1024.0594 - val_loss: 1168.6879\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1018.7631 - val_loss: 1170.7921\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1016.7170 - val_loss: 1177.4242\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1015.7998 - val_loss: 1167.7696\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1019.5594 - val_loss: 1184.9285\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1035.6214 - val_loss: 1194.3963\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1012.6661 - val_loss: 1166.5519\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 1007.4578 - val_loss: 1190.1006\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 999.3660 - val_loss: 1181.9463\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 994.0409 - val_loss: 1164.2006\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 996.1258 - val_loss: 1176.2110\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 989.0128 - val_loss: 1166.8355\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 987.8888 - val_loss: 1177.8375\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 986.2034 - val_loss: 1168.6371\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 981.0336 - val_loss: 1178.3812\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 976.3983 - val_loss: 1187.2194\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 970.2810 - val_loss: 1161.3726\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 968.7701 - val_loss: 1193.2266\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 972.5230 - val_loss: 1161.5869\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 953.8851 - val_loss: 1163.4426\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 966.6605 - val_loss: 1196.3308\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 958.0064 - val_loss: 1180.4697\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 949.1572 - val_loss: 1166.6265\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 951.3597 - val_loss: 1166.5127\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 941.4700 - val_loss: 1171.1392\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 935.7797 - val_loss: 1165.4739\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 934.2427 - val_loss: 1171.3945\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 926.3108 - val_loss: 1172.8238\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 928.0847 - val_loss: 1160.1907\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 925.9335 - val_loss: 1174.7467\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 926.4335 - val_loss: 1180.9669\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 913.9169 - val_loss: 1172.7698\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 912.6291 - val_loss: 1186.3124\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 907.5372 - val_loss: 1169.8012\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 903.9843 - val_loss: 1168.2708\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 913.5660 - val_loss: 1182.7840\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 928.2056 - val_loss: 1171.3949\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 903.8249 - val_loss: 1171.8911\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 899.7088 - val_loss: 1176.1500\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 894.4252 - val_loss: 1167.1506\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 878.7970 - val_loss: 1176.6156\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 882.3991 - val_loss: 1162.6723\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 875.2305 - val_loss: 1168.9251\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 872.8034 - val_loss: 1162.1531\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 872.4220 - val_loss: 1176.3635\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 876.8659 - val_loss: 1179.1208\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 855.9138 - val_loss: 1169.1012\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 858.7377 - val_loss: 1169.5208\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 855.4012 - val_loss: 1167.2792\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 847.6120 - val_loss: 1192.1425\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 852.1901 - val_loss: 1167.0288\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 848.4021 - val_loss: 1171.3669\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 845.0190 - val_loss: 1188.6340\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 857.3133 - val_loss: 1167.9122\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 847.8690 - val_loss: 1179.6688\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 829.1889 - val_loss: 1168.1753\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 826.2222 - val_loss: 1178.5639\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 829.8423 - val_loss: 1176.2804\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 837.6601 - val_loss: 1186.3361\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 838.0350 - val_loss: 1179.3955\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 823.9259 - val_loss: 1180.6264\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 814.7483 - val_loss: 1174.3352\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 815.5731 - val_loss: 1179.8749\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 828.0807 - val_loss: 1179.1270\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 824.9272 - val_loss: 1167.7097\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 810.8800 - val_loss: 1181.8913\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 806.1591 - val_loss: 1165.9492\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 7us/sample - loss: 796.0964 - val_loss: 1177.8279\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 796.2166 - val_loss: 1181.7916\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 789.8379 - val_loss: 1195.0252\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 795.7245 - val_loss: 1179.7856\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 806.3644 - val_loss: 1184.7226\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 784.6511 - val_loss: 1175.4893\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 780.3454 - val_loss: 1174.3319\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 768.5436 - val_loss: 1196.8327\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 786.0099 - val_loss: 1210.7255\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 769.0535 - val_loss: 1185.2680\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 775.5439 - val_loss: 1201.3853\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 6us/sample - loss: 771.2625 - val_loss: 1172.1013\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 764.4116 - val_loss: 1190.9315\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 760.3366 - val_loss: 1186.2816\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 754.4369 - val_loss: 1174.8966\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 774.2774 - val_loss: 1181.4914\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 771.0935 - val_loss: 1191.3084\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 752.7656 - val_loss: 1176.7775\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 762.2502 - val_loss: 1197.9636\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 778.5083 - val_loss: 1189.7582\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 766.6309 - val_loss: 1170.9947\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 740.7123 - val_loss: 1173.7738\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 747.7700 - val_loss: 1185.1188\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 750.6257 - val_loss: 1180.4619\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 742.0824 - val_loss: 1189.4805\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 763.3376 - val_loss: 1193.4710\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 749.5717 - val_loss: 1190.6327\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 731.2499 - val_loss: 1182.6670\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 731.0248 - val_loss: 1181.7239\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 731.7813 - val_loss: 1186.6526\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 725.5260 - val_loss: 1183.1439\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 730.5244 - val_loss: 1188.2573\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 727.3429 - val_loss: 1202.4975\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 731.5422 - val_loss: 1187.7990\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 711.9607 - val_loss: 1194.3874\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 715.0666 - val_loss: 1184.4694\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 704.0128 - val_loss: 1191.1022\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 723.1282 - val_loss: 1202.2627\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 728.0069 - val_loss: 1197.9249\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 715.5118 - val_loss: 1194.6245\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 707.5709 - val_loss: 1199.4529\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 705.9652 - val_loss: 1198.1587\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 716.3020 - val_loss: 1195.3845\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 711.3574 - val_loss: 1190.3491\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 704.6385 - val_loss: 1204.5357\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 703.2444 - val_loss: 1190.2779\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 689.4690 - val_loss: 1190.5303\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 706.8015 - val_loss: 1194.3049\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 695.9441 - val_loss: 1218.8955\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 709.2482 - val_loss: 1209.6443\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 694.7268 - val_loss: 1196.1863\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 696.6978 - val_loss: 1215.5756\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 690.5089 - val_loss: 1211.4867\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 691.3429 - val_loss: 1200.8822\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 693.8839 - val_loss: 1198.1842\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 692.6664 - val_loss: 1201.0012\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 681.7279 - val_loss: 1200.5697\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1266.8039790417977,
          1256.6694486964936,
          1245.9511019666759,
          1237.1840695965761,
          1230.749972413627,
          1223.4086691804944,
          1215.9764996657325,
          1211.6660556215693,
          1204.9755199211309,
          1202.4087943105199,
          1196.8164624752874,
          1197.304900407074,
          1192.599331430665,
          1186.0911901461377,
          1180.677543767115,
          1177.1427467519065,
          1175.5162483002682,
          1171.2238494891405,
          1166.370414836387,
          1162.4829375468014,
          1160.250086406297,
          1156.4741650557073,
          1156.6641199736503,
          1152.9806490282938,
          1150.6007408324142,
          1153.9308940794895,
          1146.1913601829601,
          1159.2665823964574,
          1143.4232762506736,
          1137.7066239654744,
          1134.112436989493,
          1131.8414802069365,
          1130.3244771611267,
          1130.8772057104331,
          1129.1417032428847,
          1125.0756221424729,
          1119.0866867870134,
          1114.3355315127926,
          1112.974366800948,
          1113.017561969715,
          1114.2916152308503,
          1109.3831210659432,
          1103.0211613182555,
          1112.4734715533825,
          1102.6531037827258,
          1095.9947117388022,
          1103.0027346001948,
          1103.7810092862537,
          1084.8379851089394,
          1088.1903824577691,
          1079.3260582494956,
          1088.557154999483,
          1090.6995330437262,
          1077.6966584310615,
          1068.501440402762,
          1080.6452656056138,
          1067.9026786784311,
          1066.2194074956215,
          1055.8464709783523,
          1053.8627931890494,
          1062.671215644073,
          1047.9059685553532,
          1045.4430013885712,
          1040.196664428099,
          1038.5179119274567,
          1030.5436921885575,
          1024.059437667819,
          1018.7630761611048,
          1016.7170057554916,
          1015.7997793653146,
          1019.5594130922038,
          1035.6214085088027,
          1012.6661198388271,
          1007.4578342410972,
          999.3659548395163,
          994.0409371250016,
          996.1258014734794,
          989.0127809844658,
          987.8887763091263,
          986.2034073503218,
          981.0336313991573,
          976.3982967409601,
          970.2809555454534,
          968.7700698451716,
          972.5229669283692,
          953.8850644855335,
          966.6605109931329,
          958.0063564804625,
          949.1571854889118,
          951.3596796617495,
          941.4699572771041,
          935.7796686301376,
          934.2427243082992,
          926.3107683982261,
          928.0847028813,
          925.9335024359233,
          926.4334842489887,
          913.9169440276163,
          912.6290940429492,
          907.5371744856941,
          903.9842949946227,
          913.5660479576,
          928.2055936376963,
          903.8248783629429,
          899.7087615911722,
          894.4251629799,
          878.797032403879,
          882.3991158628645,
          875.2304592893672,
          872.8033696938211,
          872.4219705731973,
          876.8658694245664,
          855.9138308797209,
          858.7376740193448,
          855.401176396032,
          847.6120242709169,
          852.1901358204183,
          848.4020613140062,
          845.0190020053114,
          857.3132978752569,
          847.8689735157112,
          829.1889270134576,
          826.2221997897659,
          829.8422878120809,
          837.660113426256,
          838.0349741970725,
          823.9259308454913,
          814.7483440220942,
          815.5731332127594,
          828.0806734105354,
          824.9272407208939,
          810.880039225032,
          806.1590765874085,
          796.0964329277606,
          796.21661826119,
          789.8379024966868,
          795.7245111337329,
          806.3643962339191,
          784.6511350042337,
          780.3453608287798,
          768.5436328668405,
          786.0098500608418,
          769.053536301317,
          775.5438739967078,
          771.2625241766287,
          764.411609332213,
          760.3366002705668,
          754.436914211814,
          774.277386659416,
          771.0934624848826,
          752.7655785413168,
          762.2501631561396,
          778.5082545918216,
          766.6308742941609,
          740.7122984481331,
          747.7700189315462,
          750.6256776530112,
          742.0824111562132,
          763.3375714602106,
          749.571744617059,
          731.2498508083909,
          731.0247574969335,
          731.7813359779371,
          725.5259917117513,
          730.5244058257524,
          727.3429171583422,
          731.5421961112564,
          711.9606757599055,
          715.066555037727,
          704.0128300500246,
          723.1281894571393,
          728.0069345459327,
          715.511847906608,
          707.5709270005334,
          705.9652315257952,
          716.3019581625116,
          711.3573512323447,
          704.6385436852613,
          703.2443887554908,
          689.4689514147917,
          706.8014593436097,
          695.9440628275117,
          709.2481890782245,
          694.726805147485,
          696.6978335077451,
          690.5088645884573,
          691.3429360184135,
          693.8838541854329,
          692.6664262262547,
          681.7279299347248
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1279.4128320792263,
          1271.644808362093,
          1266.3096539772941,
          1258.2871170365215,
          1257.502252438371,
          1249.9369775478744,
          1256.220694508848,
          1244.0647481792505,
          1249.5840903389828,
          1239.5858565032756,
          1248.829679064983,
          1235.9304602856098,
          1231.921639573438,
          1229.1003884758575,
          1225.915989189079,
          1231.5198956495874,
          1233.9612163255324,
          1223.924908154757,
          1221.0826576099812,
          1219.564666619539,
          1217.630562387011,
          1221.0851869646237,
          1216.4020980183243,
          1214.6491419438587,
          1219.1764252339285,
          1214.5239283269314,
          1247.7887520492734,
          1213.4074816201812,
          1218.0177863568324,
          1208.7369356371487,
          1207.2179006236527,
          1204.9409433766073,
          1208.8395936857705,
          1201.924484739424,
          1212.0411542667184,
          1204.3165956829553,
          1200.1023814260827,
          1208.9756869814644,
          1202.905245851127,
          1198.603947240364,
          1201.4042602318764,
          1194.6965120543882,
          1211.9289399019483,
          1204.8147960576405,
          1203.2133747205626,
          1201.3105132671117,
          1209.9771576019525,
          1189.8447863370545,
          1193.1176735537788,
          1188.1845872755493,
          1190.3043918827623,
          1205.2589900244543,
          1206.4372610241708,
          1187.0819370842707,
          1192.3847322618888,
          1184.4073519373026,
          1183.2886660346196,
          1184.6274768010107,
          1180.481851250752,
          1177.9207626107175,
          1193.433601460477,
          1177.2465367474965,
          1186.7017084997165,
          1177.5786551381248,
          1178.2507221901944,
          1170.2153752343986,
          1168.6879163412764,
          1170.7921426029943,
          1177.4241959269882,
          1167.7695546506854,
          1184.9284602123726,
          1194.3962905850133,
          1166.5518766028001,
          1190.100558179783,
          1181.9463036022564,
          1164.2006224239665,
          1176.2109541203613,
          1166.8354665000095,
          1177.8374525719996,
          1168.6371356053082,
          1178.3812187077026,
          1187.2193763355035,
          1161.372613766496,
          1193.2265570414718,
          1161.5868713733832,
          1163.4426458254352,
          1196.3308272406696,
          1180.4697155230554,
          1166.6264893948876,
          1166.5127302177066,
          1171.1392312884593,
          1165.4739004517214,
          1171.3944515261164,
          1172.8237772456434,
          1160.190717667694,
          1174.7466882155322,
          1180.9668949816398,
          1172.7698349693578,
          1186.3124402744008,
          1169.8012066725048,
          1168.270802573018,
          1182.78399478292,
          1171.3948575133238,
          1171.8911479172018,
          1176.1499827774871,
          1167.1506337620392,
          1176.615609495822,
          1162.672326417827,
          1168.925120082723,
          1162.1531154585907,
          1176.3635433572601,
          1179.1207740711789,
          1169.1011656919318,
          1169.520796943146,
          1167.279205817939,
          1192.1424677550303,
          1167.0287716079386,
          1171.3669475940178,
          1188.6340393715066,
          1167.9122408937064,
          1179.6687878963821,
          1168.175272162701,
          1178.5639266340531,
          1176.280355046167,
          1186.3360723476742,
          1179.3954682810072,
          1180.6264472932357,
          1174.3351505173216,
          1179.8749476177122,
          1179.1270155431466,
          1167.709687717362,
          1181.8912700854253,
          1165.949195643047,
          1177.8279355915224,
          1181.7915512216528,
          1195.025212598655,
          1179.7855502676098,
          1184.7226359335052,
          1175.4893365572755,
          1174.3318960106824,
          1196.832739145215,
          1210.7254549083477,
          1185.2680152478474,
          1201.3852527068425,
          1172.101333608985,
          1190.931475596316,
          1186.281627764414,
          1174.89663963708,
          1181.4914254356247,
          1191.3084085027322,
          1176.7774894902527,
          1197.9635511803344,
          1189.7582317296456,
          1170.9947008462236,
          1173.7738155826241,
          1185.1188385211706,
          1180.4618993269219,
          1189.480525660663,
          1193.4710253739802,
          1190.6327136590292,
          1182.6669992126012,
          1181.7238504927263,
          1186.6525993706684,
          1183.1438548575907,
          1188.257347497047,
          1202.4974612214414,
          1187.798980816921,
          1194.387433450017,
          1184.469448618165,
          1191.1022163239656,
          1202.2626564419052,
          1197.924922988246,
          1194.6244928464398,
          1199.4528973818256,
          1198.1587124081204,
          1195.3844551742734,
          1190.3491086835538,
          1204.5357246957324,
          1190.277862677336,
          1190.530253635037,
          1194.3048822201395,
          1218.895518925378,
          1209.6442779547135,
          1196.18634183741,
          1215.5755800775376,
          1211.4866687077808,
          1200.8821995782785,
          1198.1842492629273,
          1201.0012212171569,
          1200.5697104463793
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"e62a7033-34c7-42dd-a7e5-9b59bc91329e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"e62a7033-34c7-42dd-a7e5-9b59bc91329e\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'e62a7033-34c7-42dd-a7e5-9b59bc91329e',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1266.8039790417977, 1256.6694486964936, 1245.9511019666759, 1237.1840695965761, 1230.749972413627, 1223.4086691804944, 1215.9764996657325, 1211.6660556215693, 1204.9755199211309, 1202.4087943105199, 1196.8164624752874, 1197.304900407074, 1192.599331430665, 1186.0911901461377, 1180.677543767115, 1177.1427467519065, 1175.5162483002682, 1171.2238494891405, 1166.370414836387, 1162.4829375468014, 1160.250086406297, 1156.4741650557073, 1156.6641199736503, 1152.9806490282938, 1150.6007408324142, 1153.9308940794895, 1146.1913601829601, 1159.2665823964574, 1143.4232762506736, 1137.7066239654744, 1134.112436989493, 1131.8414802069365, 1130.3244771611267, 1130.8772057104331, 1129.1417032428847, 1125.0756221424729, 1119.0866867870134, 1114.3355315127926, 1112.974366800948, 1113.017561969715, 1114.2916152308503, 1109.3831210659432, 1103.0211613182555, 1112.4734715533825, 1102.6531037827258, 1095.9947117388022, 1103.0027346001948, 1103.7810092862537, 1084.8379851089394, 1088.1903824577691, 1079.3260582494956, 1088.557154999483, 1090.6995330437262, 1077.6966584310615, 1068.501440402762, 1080.6452656056138, 1067.9026786784311, 1066.2194074956215, 1055.8464709783523, 1053.8627931890494, 1062.671215644073, 1047.9059685553532, 1045.4430013885712, 1040.196664428099, 1038.5179119274567, 1030.5436921885575, 1024.059437667819, 1018.7630761611048, 1016.7170057554916, 1015.7997793653146, 1019.5594130922038, 1035.6214085088027, 1012.6661198388271, 1007.4578342410972, 999.3659548395163, 994.0409371250016, 996.1258014734794, 989.0127809844658, 987.8887763091263, 986.2034073503218, 981.0336313991573, 976.3982967409601, 970.2809555454534, 968.7700698451716, 972.5229669283692, 953.8850644855335, 966.6605109931329, 958.0063564804625, 949.1571854889118, 951.3596796617495, 941.4699572771041, 935.7796686301376, 934.2427243082992, 926.3107683982261, 928.0847028813, 925.9335024359233, 926.4334842489887, 913.9169440276163, 912.6290940429492, 907.5371744856941, 903.9842949946227, 913.5660479576, 928.2055936376963, 903.8248783629429, 899.7087615911722, 894.4251629799, 878.797032403879, 882.3991158628645, 875.2304592893672, 872.8033696938211, 872.4219705731973, 876.8658694245664, 855.9138308797209, 858.7376740193448, 855.401176396032, 847.6120242709169, 852.1901358204183, 848.4020613140062, 845.0190020053114, 857.3132978752569, 847.8689735157112, 829.1889270134576, 826.2221997897659, 829.8422878120809, 837.660113426256, 838.0349741970725, 823.9259308454913, 814.7483440220942, 815.5731332127594, 828.0806734105354, 824.9272407208939, 810.880039225032, 806.1590765874085, 796.0964329277606, 796.21661826119, 789.8379024966868, 795.7245111337329, 806.3643962339191, 784.6511350042337, 780.3453608287798, 768.5436328668405, 786.0098500608418, 769.053536301317, 775.5438739967078, 771.2625241766287, 764.411609332213, 760.3366002705668, 754.436914211814, 774.277386659416, 771.0934624848826, 752.7655785413168, 762.2501631561396, 778.5082545918216, 766.6308742941609, 740.7122984481331, 747.7700189315462, 750.6256776530112, 742.0824111562132, 763.3375714602106, 749.571744617059, 731.2498508083909, 731.0247574969335, 731.7813359779371, 725.5259917117513, 730.5244058257524, 727.3429171583422, 731.5421961112564, 711.9606757599055, 715.066555037727, 704.0128300500246, 723.1281894571393, 728.0069345459327, 715.511847906608, 707.5709270005334, 705.9652315257952, 716.3019581625116, 711.3573512323447, 704.6385436852613, 703.2443887554908, 689.4689514147917, 706.8014593436097, 695.9440628275117, 709.2481890782245, 694.726805147485, 696.6978335077451, 690.5088645884573, 691.3429360184135, 693.8838541854329, 692.6664262262547, 681.7279299347248]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1279.4128320792263, 1271.644808362093, 1266.3096539772941, 1258.2871170365215, 1257.502252438371, 1249.9369775478744, 1256.220694508848, 1244.0647481792505, 1249.5840903389828, 1239.5858565032756, 1248.829679064983, 1235.9304602856098, 1231.921639573438, 1229.1003884758575, 1225.915989189079, 1231.5198956495874, 1233.9612163255324, 1223.924908154757, 1221.0826576099812, 1219.564666619539, 1217.630562387011, 1221.0851869646237, 1216.4020980183243, 1214.6491419438587, 1219.1764252339285, 1214.5239283269314, 1247.7887520492734, 1213.4074816201812, 1218.0177863568324, 1208.7369356371487, 1207.2179006236527, 1204.9409433766073, 1208.8395936857705, 1201.924484739424, 1212.0411542667184, 1204.3165956829553, 1200.1023814260827, 1208.9756869814644, 1202.905245851127, 1198.603947240364, 1201.4042602318764, 1194.6965120543882, 1211.9289399019483, 1204.8147960576405, 1203.2133747205626, 1201.3105132671117, 1209.9771576019525, 1189.8447863370545, 1193.1176735537788, 1188.1845872755493, 1190.3043918827623, 1205.2589900244543, 1206.4372610241708, 1187.0819370842707, 1192.3847322618888, 1184.4073519373026, 1183.2886660346196, 1184.6274768010107, 1180.481851250752, 1177.9207626107175, 1193.433601460477, 1177.2465367474965, 1186.7017084997165, 1177.5786551381248, 1178.2507221901944, 1170.2153752343986, 1168.6879163412764, 1170.7921426029943, 1177.4241959269882, 1167.7695546506854, 1184.9284602123726, 1194.3962905850133, 1166.5518766028001, 1190.100558179783, 1181.9463036022564, 1164.2006224239665, 1176.2109541203613, 1166.8354665000095, 1177.8374525719996, 1168.6371356053082, 1178.3812187077026, 1187.2193763355035, 1161.372613766496, 1193.2265570414718, 1161.5868713733832, 1163.4426458254352, 1196.3308272406696, 1180.4697155230554, 1166.6264893948876, 1166.5127302177066, 1171.1392312884593, 1165.4739004517214, 1171.3944515261164, 1172.8237772456434, 1160.190717667694, 1174.7466882155322, 1180.9668949816398, 1172.7698349693578, 1186.3124402744008, 1169.8012066725048, 1168.270802573018, 1182.78399478292, 1171.3948575133238, 1171.8911479172018, 1176.1499827774871, 1167.1506337620392, 1176.615609495822, 1162.672326417827, 1168.925120082723, 1162.1531154585907, 1176.3635433572601, 1179.1207740711789, 1169.1011656919318, 1169.520796943146, 1167.279205817939, 1192.1424677550303, 1167.0287716079386, 1171.3669475940178, 1188.6340393715066, 1167.9122408937064, 1179.6687878963821, 1168.175272162701, 1178.5639266340531, 1176.280355046167, 1186.3360723476742, 1179.3954682810072, 1180.6264472932357, 1174.3351505173216, 1179.8749476177122, 1179.1270155431466, 1167.709687717362, 1181.8912700854253, 1165.949195643047, 1177.8279355915224, 1181.7915512216528, 1195.025212598655, 1179.7855502676098, 1184.7226359335052, 1175.4893365572755, 1174.3318960106824, 1196.832739145215, 1210.7254549083477, 1185.2680152478474, 1201.3852527068425, 1172.101333608985, 1190.931475596316, 1186.281627764414, 1174.89663963708, 1181.4914254356247, 1191.3084085027322, 1176.7774894902527, 1197.9635511803344, 1189.7582317296456, 1170.9947008462236, 1173.7738155826241, 1185.1188385211706, 1180.4618993269219, 1189.480525660663, 1193.4710253739802, 1190.6327136590292, 1182.6669992126012, 1181.7238504927263, 1186.6525993706684, 1183.1438548575907, 1188.257347497047, 1202.4974612214414, 1187.798980816921, 1194.387433450017, 1184.469448618165, 1191.1022163239656, 1202.2626564419052, 1197.924922988246, 1194.6244928464398, 1199.4528973818256, 1198.1587124081204, 1195.3844551742734, 1190.3491086835538, 1204.5357246957324, 1190.277862677336, 1190.530253635037, 1194.3048822201395, 1218.895518925378, 1209.6442779547135, 1196.18634183741, 1215.5755800775376, 1211.4866687077808, 1200.8821995782785, 1198.1842492629273, 1201.0012212171569, 1200.5697104463793]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('e62a7033-34c7-42dd-a7e5-9b59bc91329e');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.02% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1201   : Mean absolute error \n",
      "\n",
      "10.32% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss=[history.history[\"val_loss\"][-1]]\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Dropout\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Dropout\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,329\n",
      "Trainable params: 515,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 1s 51us/sample - loss: 11104.9790 - val_loss: 10453.7489\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 6566.8866 - val_loss: 2844.8539\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 2426.2823 - val_loss: 1743.4345\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1758.0013 - val_loss: 1499.9643\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1648.7799 - val_loss: 1415.0016\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1603.1399 - val_loss: 1372.3619\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1572.5706 - val_loss: 1345.1741\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1534.9148 - val_loss: 1323.9660\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1533.1877 - val_loss: 1316.1693\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1520.0408 - val_loss: 1302.6522\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1498.4426 - val_loss: 1295.2491\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1486.7691 - val_loss: 1276.2119\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1485.7212 - val_loss: 1286.1024\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1479.8191 - val_loss: 1271.1752\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1480.3789 - val_loss: 1266.0011\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1463.0550 - val_loss: 1269.1567\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1468.8634 - val_loss: 1260.2492\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1456.2608 - val_loss: 1263.0301\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1454.8075 - val_loss: 1253.1224\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1445.6685 - val_loss: 1249.4080\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1450.3202 - val_loss: 1250.0484\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1440.5088 - val_loss: 1252.4648\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1431.9636 - val_loss: 1243.7739\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1433.1650 - val_loss: 1240.5835\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1425.7619 - val_loss: 1252.8666\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1427.9689 - val_loss: 1249.0613\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1416.1786 - val_loss: 1239.5190\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1431.2791 - val_loss: 1235.7134\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1423.6418 - val_loss: 1239.0101\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1417.3009 - val_loss: 1231.5934\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1416.9034 - val_loss: 1233.0305\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1414.2914 - val_loss: 1229.5651\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1427.0842 - val_loss: 1241.6313\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1419.1883 - val_loss: 1239.2474\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1399.4278 - val_loss: 1222.3552\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1406.7061 - val_loss: 1231.8537\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1409.4484 - val_loss: 1225.6180\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1397.8253 - val_loss: 1230.1190\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1409.0953 - val_loss: 1230.0635\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1397.7083 - val_loss: 1227.5121\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1400.3129 - val_loss: 1220.5953\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1384.9384 - val_loss: 1232.2618\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1393.3049 - val_loss: 1236.3372\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1397.1868 - val_loss: 1224.8259\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1395.0458 - val_loss: 1217.5614\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1387.4325 - val_loss: 1230.1550\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1395.1549 - val_loss: 1212.8854\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1382.8583 - val_loss: 1230.4260\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1388.3548 - val_loss: 1221.7739\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1388.4375 - val_loss: 1220.5329\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1379.7458 - val_loss: 1220.3705\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1377.6044 - val_loss: 1216.6177\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1383.3705 - val_loss: 1228.0631\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1379.7967 - val_loss: 1210.9562\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1378.5005 - val_loss: 1211.8229\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1367.7614 - val_loss: 1213.5220\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1379.0610 - val_loss: 1206.1577\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1368.6132 - val_loss: 1207.4544\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1369.6684 - val_loss: 1202.6705\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1379.8712 - val_loss: 1210.3278\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1364.5164 - val_loss: 1206.7184\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1372.6096 - val_loss: 1204.9740\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1372.9652 - val_loss: 1247.9278\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1378.6815 - val_loss: 1219.0820\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1363.9497 - val_loss: 1209.2430\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1363.5618 - val_loss: 1227.2245\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1377.1826 - val_loss: 1216.3068\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1365.5087 - val_loss: 1203.9797\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1373.3204 - val_loss: 1214.0719\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1356.7564 - val_loss: 1212.3993\n",
      "Epoch 71/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1358.5384 - val_loss: 1199.0835\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1345.5289 - val_loss: 1202.5442\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1359.0984 - val_loss: 1214.5745\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1360.6952 - val_loss: 1192.5583\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1355.5503 - val_loss: 1196.9080\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1367.3959 - val_loss: 1204.0640\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1348.3049 - val_loss: 1212.7422\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1356.7574 - val_loss: 1200.0463\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1345.9441 - val_loss: 1199.1317\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1360.1341 - val_loss: 1206.1787\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1355.4576 - val_loss: 1192.0393\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1349.0969 - val_loss: 1197.3175\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1355.7074 - val_loss: 1197.0923\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1336.7936 - val_loss: 1193.7866\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1361.2372 - val_loss: 1207.9959\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1351.1748 - val_loss: 1189.8217\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1353.2286 - val_loss: 1192.2221\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1337.7472 - val_loss: 1194.5978\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1339.3860 - val_loss: 1184.9565\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1334.0444 - val_loss: 1193.7248\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1346.1943 - val_loss: 1185.7914\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1350.4164 - val_loss: 1195.5980\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1346.7363 - val_loss: 1192.0285\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1346.3072 - val_loss: 1188.3863\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1348.5269 - val_loss: 1188.2210\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1342.6517 - val_loss: 1187.9459\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1341.5732 - val_loss: 1225.6422\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1336.2030 - val_loss: 1186.9382\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1333.5496 - val_loss: 1190.5741\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1341.2992 - val_loss: 1189.4937\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1323.8505 - val_loss: 1183.9919\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1333.8107 - val_loss: 1194.8666\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1331.6306 - val_loss: 1185.0936\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1333.1311 - val_loss: 1175.0939\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1330.2079 - val_loss: 1191.0156\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1342.6805 - val_loss: 1182.4441\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1327.1628 - val_loss: 1185.9764\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1328.0093 - val_loss: 1176.1489\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1324.2468 - val_loss: 1189.2250\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1318.1575 - val_loss: 1170.0674\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1333.1170 - val_loss: 1176.2424\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1331.1874 - val_loss: 1182.0624\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1326.9343 - val_loss: 1181.6832\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1326.2703 - val_loss: 1183.9983\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1326.9332 - val_loss: 1180.4637\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1321.3217 - val_loss: 1172.8332\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1318.3266 - val_loss: 1190.8262\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1322.2278 - val_loss: 1189.6668\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1321.0271 - val_loss: 1177.6149\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1321.7530 - val_loss: 1170.0225\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1320.0880 - val_loss: 1172.2961\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1324.2463 - val_loss: 1198.1869\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1327.3559 - val_loss: 1179.7931\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1318.4365 - val_loss: 1170.8150\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1312.6422 - val_loss: 1188.4621\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1311.3748 - val_loss: 1175.4571\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1311.9795 - val_loss: 1176.5244\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1304.7134 - val_loss: 1179.8359\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1316.2792 - val_loss: 1167.0924\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1323.0755 - val_loss: 1179.7508\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1302.9306 - val_loss: 1196.1807\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1319.7642 - val_loss: 1167.4679\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1316.6123 - val_loss: 1173.6899\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1308.9072 - val_loss: 1163.4868\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1309.5409 - val_loss: 1167.3497\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1307.5244 - val_loss: 1173.6041\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1312.2147 - val_loss: 1175.7067\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1306.2750 - val_loss: 1176.3031\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1321.7119 - val_loss: 1177.1884\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1302.9523 - val_loss: 1175.7945\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1305.6284 - val_loss: 1168.0113\n",
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1305.9406 - val_loss: 1181.4482\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1319.1233 - val_loss: 1163.9970\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1295.9626 - val_loss: 1161.6499\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1305.3658 - val_loss: 1183.7885\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1326.9102 - val_loss: 1198.1618\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1304.7840 - val_loss: 1167.9697\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1307.4831 - val_loss: 1192.3912\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1298.4878 - val_loss: 1168.7396\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1310.5902 - val_loss: 1169.6830\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1297.6384 - val_loss: 1160.5371\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1297.4192 - val_loss: 1164.4484\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1291.1664 - val_loss: 1157.3735\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1301.4622 - val_loss: 1163.1834\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1291.3015 - val_loss: 1155.2088\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1298.2679 - val_loss: 1171.4921\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1292.9797 - val_loss: 1184.4734\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1295.8081 - val_loss: 1172.5314\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1300.1229 - val_loss: 1155.5513\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1295.5817 - val_loss: 1156.5358\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1300.9848 - val_loss: 1160.1508\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1294.8279 - val_loss: 1151.4668\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1282.3797 - val_loss: 1154.8018\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1288.7168 - val_loss: 1159.2447\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1285.5962 - val_loss: 1155.8714\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1285.4840 - val_loss: 1164.4457\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1283.1423 - val_loss: 1153.9844\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1289.3284 - val_loss: 1154.6291\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1283.7829 - val_loss: 1176.3368\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1283.0020 - val_loss: 1166.6905\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1290.0869 - val_loss: 1166.2298\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 0s 8us/sample - loss: 1304.3408 - val_loss: 1158.4135\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 7us/sample - loss: 1276.7452 - val_loss: 1153.6313\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1282.4330 - val_loss: 1152.1035\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1281.2922 - val_loss: 1152.4347\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1285.1788 - val_loss: 1158.2190\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1279.3681 - val_loss: 1181.2827\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1281.6678 - val_loss: 1161.4754\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1283.2934 - val_loss: 1151.2268\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1278.8207 - val_loss: 1162.7971\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1283.0515 - val_loss: 1160.7681\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1277.9534 - val_loss: 1154.7402\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1275.2799 - val_loss: 1152.4764\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1268.2565 - val_loss: 1150.8041\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1288.6460 - val_loss: 1148.3526\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1290.8344 - val_loss: 1181.2718\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1265.4406 - val_loss: 1152.8025\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1274.5377 - val_loss: 1147.0404\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1270.3752 - val_loss: 1151.8771\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1270.9845 - val_loss: 1164.5119\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1273.7069 - val_loss: 1148.1010\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1269.97 - 0s 9us/sample - loss: 1270.7489 - val_loss: 1165.7323\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1274.6472 - val_loss: 1152.2790\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1270.5345 - val_loss: 1152.8164\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1269.1797 - val_loss: 1146.7215\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1264.8413 - val_loss: 1148.9110\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 10us/sample - loss: 1268.2501 - val_loss: 1149.5029\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 12us/sample - loss: 1267.2494 - val_loss: 1165.8497\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 11us/sample - loss: 1278.0869 - val_loss: 1165.4000\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 0s 9us/sample - loss: 1261.5282 - val_loss: 1152.6631\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1498.442628372832,
          1486.7690631383784,
          1485.7212395538947,
          1479.8191345349471,
          1480.3788885036643,
          1463.0550000068538,
          1468.8633862593524,
          1456.2607934193356,
          1454.807486378647,
          1445.6684643011283,
          1450.320184506083,
          1440.5088093300394,
          1431.963567702785,
          1433.1650222952724,
          1425.7618960909692,
          1427.9688943695014,
          1416.1785767752015,
          1431.2791012491855,
          1423.6417789769025,
          1417.3008730169142,
          1416.9034310496354,
          1414.2914269728246,
          1427.0842390899934,
          1419.188261746165,
          1399.4277734169386,
          1406.7061478985988,
          1409.4483802907089,
          1397.825311703985,
          1409.095344820361,
          1397.7082950583626,
          1400.3128756838091,
          1384.9383594856392,
          1393.304856175862,
          1397.1867614586988,
          1395.045820965565,
          1387.4325463979785,
          1395.1548626771403,
          1382.8582685587999,
          1388.3547917211704,
          1388.4375187499218,
          1379.7458497317634,
          1377.6044062952349,
          1383.3704632532867,
          1379.796705491896,
          1378.500535253965,
          1367.761396305198,
          1379.0610449962871,
          1368.6132372390873,
          1369.6683648482147,
          1379.8712438962395,
          1364.5163685837083,
          1372.6096228612362,
          1372.9652396768709,
          1378.6815108442106,
          1363.9497421078006,
          1363.5618476446966,
          1377.1825534965128,
          1365.5086882143573,
          1373.3203652494533,
          1356.7563977865889,
          1358.5384453681133,
          1345.5289009481291,
          1359.0984457294044,
          1360.695248294981,
          1355.5503117186718,
          1367.3959324027628,
          1348.3049294376317,
          1356.7574259725677,
          1345.9440612486997,
          1360.1340844594383,
          1355.4576470600907,
          1349.0968803067663,
          1355.7073855647732,
          1336.7935740591554,
          1361.237203863835,
          1351.1748213078613,
          1353.2285563559003,
          1337.7471714054197,
          1339.3859932553153,
          1334.044437657049,
          1346.1942553568967,
          1350.4163940747899,
          1346.7363102073202,
          1346.3071707876024,
          1348.5269158797248,
          1342.6517447560925,
          1341.5732224095148,
          1336.203032792487,
          1333.5495635576326,
          1341.2992361928123,
          1323.8504842571153,
          1333.8107073508309,
          1331.630590634282,
          1333.1311054493638,
          1330.2079147972229,
          1342.6804609709855,
          1327.1627910545935,
          1328.0093208651087,
          1324.246765516123,
          1318.1574652690747,
          1333.1169788030911,
          1331.1873631941094,
          1326.9343242870918,
          1326.270322002245,
          1326.9332294728013,
          1321.3216830801098,
          1318.3265673123167,
          1322.2278041070847,
          1321.0270575077232,
          1321.753012030498,
          1320.0879856719266,
          1324.2462952015082,
          1327.3558886650212,
          1318.436528088264,
          1312.6421996262548,
          1311.3748112279427,
          1311.9794830328385,
          1304.7133692620343,
          1316.2791511347473,
          1323.0754902933172,
          1302.9306049733214,
          1319.7642095522579,
          1316.6122971728448,
          1308.9071715610978,
          1309.540943562638,
          1307.5244454184394,
          1312.2147266486616,
          1306.2750031429373,
          1321.7119271825495,
          1302.9522519537124,
          1305.628423525189,
          1305.9405918278148,
          1319.1233156646736,
          1295.962587909228,
          1305.3657698247866,
          1326.9102250568274,
          1304.7840010492123,
          1307.4831208848084,
          1298.4878044977488,
          1310.5901551064017,
          1297.6383528815545,
          1297.4191955480735,
          1291.1663978443562,
          1301.4621548252019,
          1291.3015164378555,
          1298.2679081089348,
          1292.979714126847,
          1295.8080582267803,
          1300.1228810630296,
          1295.581730933043,
          1300.9848224769166,
          1294.8279478548523,
          1282.379691029685,
          1288.7167569763417,
          1285.596245918098,
          1285.4840426662056,
          1283.1422685554708,
          1289.328437482376,
          1283.782878721982,
          1283.0019560133692,
          1290.0868753387715,
          1304.3408003631707,
          1276.7451871447795,
          1282.433044078667,
          1281.2922313836286,
          1285.1788086524964,
          1279.3681005673345,
          1281.6678481483498,
          1283.293438550583,
          1278.8206718571803,
          1283.0515083357354,
          1277.9533989055137,
          1275.2799194898926,
          1268.2564677193907,
          1288.6459692417081,
          1290.8343527154977,
          1265.4405946427507,
          1274.5377112572767,
          1270.3751517862459,
          1270.9845034834711,
          1273.7069139773175,
          1270.7488590452363,
          1274.6472336375357,
          1270.5344512186764,
          1269.1796619452764,
          1264.8412932462293,
          1268.250080947769,
          1267.2494268545483,
          1278.086913401602,
          1261.5281642671337
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1295.2491310904445,
          1276.2119001591639,
          1286.1023584170405,
          1271.175185682971,
          1266.0011353493694,
          1269.1566782619184,
          1260.2492295152945,
          1263.0301014816257,
          1253.1224334637245,
          1249.4079824829714,
          1250.0484176828504,
          1252.4648361374338,
          1243.7738551630723,
          1240.5835164836778,
          1252.866630362967,
          1249.0612792479196,
          1239.518979767317,
          1235.713393984516,
          1239.0101176143912,
          1231.593435510452,
          1233.0304644615062,
          1229.5651107185322,
          1241.6312965957584,
          1239.2474344428326,
          1222.355238121068,
          1231.8536590643484,
          1225.6180143079036,
          1230.119035493259,
          1230.0635111984518,
          1227.512116806431,
          1220.5952681806682,
          1232.261814678124,
          1236.3372159215523,
          1224.8258756458197,
          1217.5614407518142,
          1230.1550466270899,
          1212.8854223618125,
          1230.4259532058743,
          1221.7738584186072,
          1220.5329323281549,
          1220.370466802554,
          1216.617711738998,
          1228.0630546892623,
          1210.9562111245098,
          1211.8228909569177,
          1213.5219912348277,
          1206.1576620453413,
          1207.4544359768993,
          1202.6705035142152,
          1210.3278380968252,
          1206.7184351432866,
          1204.974025067715,
          1247.9277881035614,
          1219.0820428524319,
          1209.2430228506225,
          1227.22454923328,
          1216.306816839995,
          1203.97971569342,
          1214.0719082749915,
          1212.3992975241488,
          1199.0834640524347,
          1202.54418225668,
          1214.5745014674874,
          1192.5582732144592,
          1196.9080028484216,
          1204.0639732885581,
          1212.7422333222628,
          1200.0462745863856,
          1199.1316530156141,
          1206.1786585062569,
          1192.0393377517876,
          1197.3175488202921,
          1197.0923005772038,
          1193.7866020745735,
          1207.9959182693362,
          1189.8216897380455,
          1192.2221101279342,
          1194.5978377277013,
          1184.9565179280582,
          1193.7248087459034,
          1185.7914064115528,
          1195.598045861623,
          1192.0284518556841,
          1188.3862583733332,
          1188.221048676755,
          1187.9459184700534,
          1225.6421516254763,
          1186.938230928735,
          1190.5741344487856,
          1189.4936546936096,
          1183.991937484726,
          1194.8665871108633,
          1185.0935937343343,
          1175.0938884703746,
          1191.01561124353,
          1182.4441459660352,
          1185.9764087800156,
          1176.148880791229,
          1189.2249762713132,
          1170.0673703778261,
          1176.2423975676602,
          1182.0623813565662,
          1181.6832364734246,
          1183.9982537116032,
          1180.4637107994458,
          1172.8331711522223,
          1190.826218015473,
          1189.6668078948626,
          1177.6148678948077,
          1170.022518435628,
          1172.2960755092147,
          1198.1869094264227,
          1179.7930893271725,
          1170.814989627328,
          1188.4620743914647,
          1175.4570735474733,
          1176.5243876755346,
          1179.8358514363908,
          1167.0923568024912,
          1179.7508021588553,
          1196.180726284825,
          1167.4678882624119,
          1173.6899145052528,
          1163.4867701433884,
          1167.349669590153,
          1173.6041248261104,
          1175.706716197465,
          1176.3031126044905,
          1177.1884366638417,
          1175.7944702613513,
          1168.0113040732076,
          1181.44816231675,
          1163.9970121779518,
          1161.6499147294685,
          1183.7885321905312,
          1198.1618369136709,
          1167.9697047283876,
          1192.3912360124605,
          1168.7396031929109,
          1169.6829671697194,
          1160.537126949992,
          1164.4484206544437,
          1157.3734520054877,
          1163.1833912459504,
          1155.2088250006657,
          1171.4920514284406,
          1184.4733741321186,
          1172.5314011498256,
          1155.551333564925,
          1156.5358004297502,
          1160.1508014098374,
          1151.466767305933,
          1154.8017513993414,
          1159.2446657941507,
          1155.8713892693268,
          1164.4457004226615,
          1153.9843618554726,
          1154.6290632744747,
          1176.3368441541759,
          1166.6905457852527,
          1166.2297760799172,
          1158.4135192173478,
          1153.6313074638592,
          1152.1035497958756,
          1152.4347478003356,
          1158.219020576546,
          1181.2826829248233,
          1161.4754387041073,
          1151.2267500726498,
          1162.7970786545018,
          1160.7681152392706,
          1154.7401822374893,
          1152.4764474792664,
          1150.8040579579172,
          1148.3525562213708,
          1181.271823709418,
          1152.802470431912,
          1147.040436947587,
          1151.8771339173256,
          1164.5118523248336,
          1148.1010051671456,
          1165.7322639203726,
          1152.279049748095,
          1152.8163688235898,
          1146.721546651176,
          1148.9110080006847,
          1149.5029303728757,
          1165.8497323754652,
          1165.399956684254,
          1152.6631266439226
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793,
          1200.5697104463793
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"19916b8d-1b2e-4ca8-bfb6-1dbd261b532c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"19916b8d-1b2e-4ca8-bfb6-1dbd261b532c\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '19916b8d-1b2e-4ca8-bfb6-1dbd261b532c',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1498.442628372832, 1486.7690631383784, 1485.7212395538947, 1479.8191345349471, 1480.3788885036643, 1463.0550000068538, 1468.8633862593524, 1456.2607934193356, 1454.807486378647, 1445.6684643011283, 1450.320184506083, 1440.5088093300394, 1431.963567702785, 1433.1650222952724, 1425.7618960909692, 1427.9688943695014, 1416.1785767752015, 1431.2791012491855, 1423.6417789769025, 1417.3008730169142, 1416.9034310496354, 1414.2914269728246, 1427.0842390899934, 1419.188261746165, 1399.4277734169386, 1406.7061478985988, 1409.4483802907089, 1397.825311703985, 1409.095344820361, 1397.7082950583626, 1400.3128756838091, 1384.9383594856392, 1393.304856175862, 1397.1867614586988, 1395.045820965565, 1387.4325463979785, 1395.1548626771403, 1382.8582685587999, 1388.3547917211704, 1388.4375187499218, 1379.7458497317634, 1377.6044062952349, 1383.3704632532867, 1379.796705491896, 1378.500535253965, 1367.761396305198, 1379.0610449962871, 1368.6132372390873, 1369.6683648482147, 1379.8712438962395, 1364.5163685837083, 1372.6096228612362, 1372.9652396768709, 1378.6815108442106, 1363.9497421078006, 1363.5618476446966, 1377.1825534965128, 1365.5086882143573, 1373.3203652494533, 1356.7563977865889, 1358.5384453681133, 1345.5289009481291, 1359.0984457294044, 1360.695248294981, 1355.5503117186718, 1367.3959324027628, 1348.3049294376317, 1356.7574259725677, 1345.9440612486997, 1360.1340844594383, 1355.4576470600907, 1349.0968803067663, 1355.7073855647732, 1336.7935740591554, 1361.237203863835, 1351.1748213078613, 1353.2285563559003, 1337.7471714054197, 1339.3859932553153, 1334.044437657049, 1346.1942553568967, 1350.4163940747899, 1346.7363102073202, 1346.3071707876024, 1348.5269158797248, 1342.6517447560925, 1341.5732224095148, 1336.203032792487, 1333.5495635576326, 1341.2992361928123, 1323.8504842571153, 1333.8107073508309, 1331.630590634282, 1333.1311054493638, 1330.2079147972229, 1342.6804609709855, 1327.1627910545935, 1328.0093208651087, 1324.246765516123, 1318.1574652690747, 1333.1169788030911, 1331.1873631941094, 1326.9343242870918, 1326.270322002245, 1326.9332294728013, 1321.3216830801098, 1318.3265673123167, 1322.2278041070847, 1321.0270575077232, 1321.753012030498, 1320.0879856719266, 1324.2462952015082, 1327.3558886650212, 1318.436528088264, 1312.6421996262548, 1311.3748112279427, 1311.9794830328385, 1304.7133692620343, 1316.2791511347473, 1323.0754902933172, 1302.9306049733214, 1319.7642095522579, 1316.6122971728448, 1308.9071715610978, 1309.540943562638, 1307.5244454184394, 1312.2147266486616, 1306.2750031429373, 1321.7119271825495, 1302.9522519537124, 1305.628423525189, 1305.9405918278148, 1319.1233156646736, 1295.962587909228, 1305.3657698247866, 1326.9102250568274, 1304.7840010492123, 1307.4831208848084, 1298.4878044977488, 1310.5901551064017, 1297.6383528815545, 1297.4191955480735, 1291.1663978443562, 1301.4621548252019, 1291.3015164378555, 1298.2679081089348, 1292.979714126847, 1295.8080582267803, 1300.1228810630296, 1295.581730933043, 1300.9848224769166, 1294.8279478548523, 1282.379691029685, 1288.7167569763417, 1285.596245918098, 1285.4840426662056, 1283.1422685554708, 1289.328437482376, 1283.782878721982, 1283.0019560133692, 1290.0868753387715, 1304.3408003631707, 1276.7451871447795, 1282.433044078667, 1281.2922313836286, 1285.1788086524964, 1279.3681005673345, 1281.6678481483498, 1283.293438550583, 1278.8206718571803, 1283.0515083357354, 1277.9533989055137, 1275.2799194898926, 1268.2564677193907, 1288.6459692417081, 1290.8343527154977, 1265.4405946427507, 1274.5377112572767, 1270.3751517862459, 1270.9845034834711, 1273.7069139773175, 1270.7488590452363, 1274.6472336375357, 1270.5344512186764, 1269.1796619452764, 1264.8412932462293, 1268.250080947769, 1267.2494268545483, 1278.086913401602, 1261.5281642671337]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1295.2491310904445, 1276.2119001591639, 1286.1023584170405, 1271.175185682971, 1266.0011353493694, 1269.1566782619184, 1260.2492295152945, 1263.0301014816257, 1253.1224334637245, 1249.4079824829714, 1250.0484176828504, 1252.4648361374338, 1243.7738551630723, 1240.5835164836778, 1252.866630362967, 1249.0612792479196, 1239.518979767317, 1235.713393984516, 1239.0101176143912, 1231.593435510452, 1233.0304644615062, 1229.5651107185322, 1241.6312965957584, 1239.2474344428326, 1222.355238121068, 1231.8536590643484, 1225.6180143079036, 1230.119035493259, 1230.0635111984518, 1227.512116806431, 1220.5952681806682, 1232.261814678124, 1236.3372159215523, 1224.8258756458197, 1217.5614407518142, 1230.1550466270899, 1212.8854223618125, 1230.4259532058743, 1221.7738584186072, 1220.5329323281549, 1220.370466802554, 1216.617711738998, 1228.0630546892623, 1210.9562111245098, 1211.8228909569177, 1213.5219912348277, 1206.1576620453413, 1207.4544359768993, 1202.6705035142152, 1210.3278380968252, 1206.7184351432866, 1204.974025067715, 1247.9277881035614, 1219.0820428524319, 1209.2430228506225, 1227.22454923328, 1216.306816839995, 1203.97971569342, 1214.0719082749915, 1212.3992975241488, 1199.0834640524347, 1202.54418225668, 1214.5745014674874, 1192.5582732144592, 1196.9080028484216, 1204.0639732885581, 1212.7422333222628, 1200.0462745863856, 1199.1316530156141, 1206.1786585062569, 1192.0393377517876, 1197.3175488202921, 1197.0923005772038, 1193.7866020745735, 1207.9959182693362, 1189.8216897380455, 1192.2221101279342, 1194.5978377277013, 1184.9565179280582, 1193.7248087459034, 1185.7914064115528, 1195.598045861623, 1192.0284518556841, 1188.3862583733332, 1188.221048676755, 1187.9459184700534, 1225.6421516254763, 1186.938230928735, 1190.5741344487856, 1189.4936546936096, 1183.991937484726, 1194.8665871108633, 1185.0935937343343, 1175.0938884703746, 1191.01561124353, 1182.4441459660352, 1185.9764087800156, 1176.148880791229, 1189.2249762713132, 1170.0673703778261, 1176.2423975676602, 1182.0623813565662, 1181.6832364734246, 1183.9982537116032, 1180.4637107994458, 1172.8331711522223, 1190.826218015473, 1189.6668078948626, 1177.6148678948077, 1170.022518435628, 1172.2960755092147, 1198.1869094264227, 1179.7930893271725, 1170.814989627328, 1188.4620743914647, 1175.4570735474733, 1176.5243876755346, 1179.8358514363908, 1167.0923568024912, 1179.7508021588553, 1196.180726284825, 1167.4678882624119, 1173.6899145052528, 1163.4867701433884, 1167.349669590153, 1173.6041248261104, 1175.706716197465, 1176.3031126044905, 1177.1884366638417, 1175.7944702613513, 1168.0113040732076, 1181.44816231675, 1163.9970121779518, 1161.6499147294685, 1183.7885321905312, 1198.1618369136709, 1167.9697047283876, 1192.3912360124605, 1168.7396031929109, 1169.6829671697194, 1160.537126949992, 1164.4484206544437, 1157.3734520054877, 1163.1833912459504, 1155.2088250006657, 1171.4920514284406, 1184.4733741321186, 1172.5314011498256, 1155.551333564925, 1156.5358004297502, 1160.1508014098374, 1151.466767305933, 1154.8017513993414, 1159.2446657941507, 1155.8713892693268, 1164.4457004226615, 1153.9843618554726, 1154.6290632744747, 1176.3368441541759, 1166.6905457852527, 1166.2297760799172, 1158.4135192173478, 1153.6313074638592, 1152.1035497958756, 1152.4347478003356, 1158.219020576546, 1181.2826829248233, 1161.4754387041073, 1151.2267500726498, 1162.7970786545018, 1160.7681152392706, 1154.7401822374893, 1152.4764474792664, 1150.8040579579172, 1148.3525562213708, 1181.271823709418, 1152.802470431912, 1147.040436947587, 1151.8771339173256, 1164.5118523248336, 1148.1010051671456, 1165.7322639203726, 1152.279049748095, 1152.8163688235898, 1146.721546651176, 1148.9110080006847, 1149.5029303728757, 1165.8497323754652, 1165.399956684254, 1152.6631266439226]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793, 1200.5697104463793]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('19916b8d-1b2e-4ca8-bfb6-1dbd261b532c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.32% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1153   : Mean absolute error \n",
      "\n",
      "9.87% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512, activation='relu'),  \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256,activation='relu'), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128,activation='relu'),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Batchnorm\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Batchnorm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/200\n",
      "19948/19948 [==============================] - 2s 120us/sample - loss: 11193.2647 - val_loss: 11118.9994\n",
      "Epoch 2/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11176.2126 - val_loss: 11081.1899\n",
      "Epoch 3/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 11140.8920 - val_loss: 11036.2633\n",
      "Epoch 4/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 11086.9997 - val_loss: 10963.5562\n",
      "Epoch 5/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 11010.1029 - val_loss: 10843.1285\n",
      "Epoch 6/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10903.1746 - val_loss: 10675.0468\n",
      "Epoch 7/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 10760.0189 - val_loss: 10464.9079\n",
      "Epoch 8/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 10573.2153 - val_loss: 10203.9690\n",
      "Epoch 9/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10335.5889 - val_loss: 9889.4180\n",
      "Epoch 10/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 10036.9312 - val_loss: 9517.0654\n",
      "Epoch 11/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 9670.6838 - val_loss: 9071.2532\n",
      "Epoch 12/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 9225.1104 - val_loss: 8560.2671\n",
      "Epoch 13/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 8692.5848 - val_loss: 7952.1821\n",
      "Epoch 14/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 8062.8914 - val_loss: 7254.5627\n",
      "Epoch 15/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 7330.8239 - val_loss: 6436.2457\n",
      "Epoch 16/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 6491.4287 - val_loss: 5616.3784\n",
      "Epoch 17/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 5586.9870 - val_loss: 4653.1959\n",
      "Epoch 18/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 4747.7051 - val_loss: 4770.9986\n",
      "Epoch 19/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 4225.4139 - val_loss: 3871.2983\n",
      "Epoch 20/200\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 3933.4231 - val_loss: 3366.5130\n",
      "Epoch 21/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 3600.2720 - val_loss: 2987.6251\n",
      "Epoch 22/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 3250.1200 - val_loss: 2383.3906\n",
      "Epoch 23/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 2844.6370 - val_loss: 1879.1172\n",
      "Epoch 24/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 2418.3456 - val_loss: 1604.2971\n",
      "Epoch 25/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1995.7980 - val_loss: 1676.2520\n",
      "Epoch 26/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1641.9101 - val_loss: 1897.1625\n",
      "Epoch 27/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1464.8043 - val_loss: 2092.3395\n",
      "Epoch 28/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1407.1681 - val_loss: 1873.7667\n",
      "Epoch 29/200\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1395.2991 - val_loss: 1883.0595\n",
      "Epoch 30/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1375.5261 - val_loss: 1624.5270\n",
      "Epoch 31/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1383.7364 - val_loss: 1508.5554\n",
      "Epoch 32/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1354.1242 - val_loss: 1448.0682\n",
      "Epoch 33/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1359.3816 - val_loss: 1429.1920\n",
      "Epoch 34/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1350.8603 - val_loss: 1378.5761\n",
      "Epoch 35/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1345.8548 - val_loss: 1273.4256\n",
      "Epoch 36/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1330.3400 - val_loss: 1263.2702\n",
      "Epoch 37/200\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1333.1762 - val_loss: 1279.6331\n",
      "Epoch 38/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1291.0187 - val_loss: 1193.1611\n",
      "Epoch 39/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1319.3592 - val_loss: 1186.0209\n",
      "Epoch 40/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1284.4341 - val_loss: 1205.4327\n",
      "Epoch 41/200\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1283.6262 - val_loss: 1178.4019\n",
      "Epoch 42/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1276.7090 - val_loss: 1175.2976\n",
      "Epoch 43/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1260.7224 - val_loss: 1173.9429\n",
      "Epoch 44/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1294.3383 - val_loss: 1163.2263\n",
      "Epoch 45/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1296.1996 - val_loss: 1165.2160\n",
      "Epoch 46/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1255.1970 - val_loss: 1160.5785\n",
      "Epoch 47/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1261.7221 - val_loss: 1150.7810\n",
      "Epoch 48/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1276.6312 - val_loss: 1161.7753\n",
      "Epoch 49/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1246.6014 - val_loss: 1162.2231\n",
      "Epoch 50/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1252.4995 - val_loss: 1156.9568\n",
      "Epoch 51/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1289.3440 - val_loss: 1164.4549\n",
      "Epoch 52/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1235.3772 - val_loss: 1145.0168\n",
      "Epoch 53/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1260.1218 - val_loss: 1144.7127\n",
      "Epoch 54/200\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1227.2376 - val_loss: 1134.2026\n",
      "Epoch 55/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1231.3304 - val_loss: 1135.9740\n",
      "Epoch 56/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1244.2457 - val_loss: 1138.9616\n",
      "Epoch 57/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1228.0024 - val_loss: 1135.0722\n",
      "Epoch 58/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1200.9999 - val_loss: 1135.5179\n",
      "Epoch 59/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1207.3558 - val_loss: 1140.1757\n",
      "Epoch 60/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1231.4693 - val_loss: 1149.6816\n",
      "Epoch 61/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1198.1811 - val_loss: 1139.8816\n",
      "Epoch 62/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1220.3733 - val_loss: 1136.8199\n",
      "Epoch 63/200\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1182.5300 - val_loss: 1134.7746\n",
      "Epoch 64/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1206.2581 - val_loss: 1142.8218\n",
      "Epoch 65/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1205.7236 - val_loss: 1123.7751\n",
      "Epoch 66/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1186.5896 - val_loss: 1133.2384\n",
      "Epoch 67/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1178.5422 - val_loss: 1127.8617\n",
      "Epoch 68/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1168.9576 - val_loss: 1127.9222\n",
      "Epoch 69/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1149.7308 - val_loss: 1120.9900\n",
      "Epoch 70/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1175.5403 - val_loss: 1124.1042\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1189.7584 - val_loss: 1128.6164\n",
      "Epoch 72/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1193.6923 - val_loss: 1120.0759\n",
      "Epoch 73/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1175.0648 - val_loss: 1117.6392\n",
      "Epoch 74/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1202.2625 - val_loss: 1122.8787\n",
      "Epoch 75/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1173.5467 - val_loss: 1118.2371\n",
      "Epoch 76/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1154.2019 - val_loss: 1108.5054\n",
      "Epoch 77/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.4335 - val_loss: 1120.8856\n",
      "Epoch 78/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1176.1779 - val_loss: 1138.6234\n",
      "Epoch 79/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1160.9439 - val_loss: 1143.4297\n",
      "Epoch 80/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1138.1799 - val_loss: 1134.3958\n",
      "Epoch 81/200\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1146.2687 - val_loss: 1125.5343\n",
      "Epoch 82/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1155.8023 - val_loss: 1122.1969\n",
      "Epoch 83/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1168.7835 - val_loss: 1134.4714\n",
      "Epoch 84/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1172.9060 - val_loss: 1106.8236\n",
      "Epoch 85/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1148.1515 - val_loss: 1123.4333\n",
      "Epoch 86/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1141.7162 - val_loss: 1131.7141\n",
      "Epoch 87/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1141.9034 - val_loss: 1125.2911\n",
      "Epoch 88/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1145.2284 - val_loss: 1115.6518\n",
      "Epoch 89/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1116.3491 - val_loss: 1109.7884\n",
      "Epoch 90/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1153.5132 - val_loss: 1117.7300\n",
      "Epoch 91/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1133.3145 - val_loss: 1111.4711\n",
      "Epoch 92/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1101.5722 - val_loss: 1123.8284\n",
      "Epoch 93/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1113.8747 - val_loss: 1122.3669\n",
      "Epoch 94/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1141.3728 - val_loss: 1106.4592\n",
      "Epoch 95/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1126.1503 - val_loss: 1109.9910\n",
      "Epoch 96/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1106.3993 - val_loss: 1105.4742\n",
      "Epoch 97/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1126.0310 - val_loss: 1148.1362\n",
      "Epoch 98/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1107.9189 - val_loss: 1113.3196\n",
      "Epoch 99/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1088.4030 - val_loss: 1120.2461\n",
      "Epoch 100/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1125.8870 - val_loss: 1103.6826\n",
      "Epoch 101/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1130.3054 - val_loss: 1113.2226\n",
      "Epoch 102/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1112.4276 - val_loss: 1114.1556\n",
      "Epoch 103/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1099.5021 - val_loss: 1118.3098\n",
      "Epoch 104/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1092.0231 - val_loss: 1106.0421\n",
      "Epoch 105/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1121.8046 - val_loss: 1105.0727\n",
      "Epoch 106/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1093.2817 - val_loss: 1115.2942\n",
      "Epoch 107/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1098.4190 - val_loss: 1108.4660\n",
      "Epoch 108/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1097.2684 - val_loss: 1108.2100\n",
      "Epoch 109/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1086.4000 - val_loss: 1106.1888\n",
      "Epoch 110/200\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1061.4053 - val_loss: 1103.0449\n",
      "Epoch 111/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1078.9269 - val_loss: 1106.3491\n",
      "Epoch 112/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1066.3019 - val_loss: 1114.4593\n",
      "Epoch 113/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1087.4942 - val_loss: 1112.6518\n",
      "Epoch 114/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1069.3525 - val_loss: 1104.4592\n",
      "Epoch 115/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1074.1007 - val_loss: 1101.3016\n",
      "Epoch 116/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1080.4507 - val_loss: 1109.3443\n",
      "Epoch 117/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1072.5433 - val_loss: 1109.6159\n",
      "Epoch 118/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1064.0448 - val_loss: 1095.8529\n",
      "Epoch 119/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1058.3053 - val_loss: 1113.8509\n",
      "Epoch 120/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1070.9816 - val_loss: 1096.9833\n",
      "Epoch 121/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1066.8383 - val_loss: 1091.9085\n",
      "Epoch 122/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1082.2683 - val_loss: 1089.2966\n",
      "Epoch 123/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1048.8163 - val_loss: 1122.0640\n",
      "Epoch 124/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1084.4248 - val_loss: 1096.7957\n",
      "Epoch 125/200\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1042.1835 - val_loss: 1097.1798\n",
      "Epoch 126/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1035.4439 - val_loss: 1095.4493\n",
      "Epoch 127/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1039.1440 - val_loss: 1103.0736\n",
      "Epoch 128/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1054.1421 - val_loss: 1104.6291\n",
      "Epoch 129/200\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1069.0679 - val_loss: 1121.2922\n",
      "Epoch 130/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1033.6921 - val_loss: 1095.4530\n",
      "Epoch 131/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1037.6131 - val_loss: 1101.0389\n",
      "Epoch 132/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1033.6453 - val_loss: 1091.9361\n",
      "Epoch 133/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1056.4334 - val_loss: 1094.3942\n",
      "Epoch 134/200\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1049.5471 - val_loss: 1095.5341\n",
      "Epoch 135/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1037.4062 - val_loss: 1092.9663\n",
      "Epoch 136/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1029.3352 - val_loss: 1091.8546\n",
      "Epoch 137/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1015.3253 - val_loss: 1112.2646\n",
      "Epoch 138/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1020.0005 - val_loss: 1100.7911\n",
      "Epoch 139/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1019.2210 - val_loss: 1109.1088\n",
      "Epoch 140/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1046.7382 - val_loss: 1095.3375\n",
      "Epoch 141/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1049.3870 - val_loss: 1096.9932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1026.2975 - val_loss: 1107.8643\n",
      "Epoch 143/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1013.4394 - val_loss: 1101.5852\n",
      "Epoch 144/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1009.9328 - val_loss: 1116.1110\n",
      "Epoch 145/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1018.7281 - val_loss: 1104.3793\n",
      "Epoch 146/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1009.7395 - val_loss: 1098.4590\n",
      "Epoch 147/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1015.9798 - val_loss: 1095.2978\n",
      "Epoch 148/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1002.6646 - val_loss: 1101.0200\n",
      "Epoch 149/200\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1086.8367 - val_loss: 1088.4710\n",
      "Epoch 150/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1031.2911 - val_loss: 1093.3851\n",
      "Epoch 151/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1020.4288 - val_loss: 1092.5382\n",
      "Epoch 152/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1001.4705 - val_loss: 1092.3185\n",
      "Epoch 153/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1016.6848 - val_loss: 1093.0788\n",
      "Epoch 154/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1036.0262 - val_loss: 1095.0446\n",
      "Epoch 155/200\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1027.06 - 0s 16us/sample - loss: 1028.2661 - val_loss: 1093.3437\n",
      "Epoch 156/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1014.9247 - val_loss: 1093.2624\n",
      "Epoch 157/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 987.2850 - val_loss: 1082.6257\n",
      "Epoch 158/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1016.3930 - val_loss: 1079.4659\n",
      "Epoch 159/200\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1013.3377 - val_loss: 1085.1814\n",
      "Epoch 160/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1008.7842 - val_loss: 1116.4361\n",
      "Epoch 161/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1027.1395 - val_loss: 1097.2733\n",
      "Epoch 162/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 999.5406 - val_loss: 1092.6695\n",
      "Epoch 163/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 989.8336 - val_loss: 1093.7099\n",
      "Epoch 164/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 987.5129 - val_loss: 1088.8084\n",
      "Epoch 165/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1000.5296 - val_loss: 1107.3667\n",
      "Epoch 166/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1014.9665 - val_loss: 1094.6407\n",
      "Epoch 167/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 980.7786 - val_loss: 1089.4234\n",
      "Epoch 168/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 992.1832 - val_loss: 1095.7312\n",
      "Epoch 169/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1018.0581 - val_loss: 1096.2981\n",
      "Epoch 170/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1004.1634 - val_loss: 1099.6557\n",
      "Epoch 171/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 979.5975 - val_loss: 1097.8209\n",
      "Epoch 172/200\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 999.2067 - val_loss: 1097.8577\n",
      "Epoch 173/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 972.0392 - val_loss: 1084.9647\n",
      "Epoch 174/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 977.4379 - val_loss: 1106.2663\n",
      "Epoch 175/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 992.1830 - val_loss: 1090.8645\n",
      "Epoch 176/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 982.5387 - val_loss: 1095.0535\n",
      "Epoch 177/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 993.4017 - val_loss: 1108.7536\n",
      "Epoch 178/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 993.2937 - val_loss: 1095.8268\n",
      "Epoch 179/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 972.0144 - val_loss: 1091.8439\n",
      "Epoch 180/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 982.3668 - val_loss: 1100.1176\n",
      "Epoch 181/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 973.2843 - val_loss: 1085.5615\n",
      "Epoch 182/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 978.2662 - val_loss: 1095.0610\n",
      "Epoch 183/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 976.0469 - val_loss: 1094.5866\n",
      "Epoch 184/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 961.3673 - val_loss: 1106.0059\n",
      "Epoch 185/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 966.7036 - val_loss: 1084.2180\n",
      "Epoch 186/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 950.8529 - val_loss: 1102.7277\n",
      "Epoch 187/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 966.5327 - val_loss: 1084.6802\n",
      "Epoch 188/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 973.3773 - val_loss: 1111.5827\n",
      "Epoch 189/200\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 967.6975 - val_loss: 1087.6912\n",
      "Epoch 190/200\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 963.9722 - val_loss: 1081.7836\n",
      "Epoch 191/200\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 955.3760 - val_loss: 1078.2367\n",
      "Epoch 192/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 953.8300 - val_loss: 1084.2766\n",
      "Epoch 193/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 947.0423 - val_loss: 1083.9803\n",
      "Epoch 194/200\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 948.6804 - val_loss: 1102.3103\n",
      "Epoch 195/200\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 957.8253 - val_loss: 1089.7543\n",
      "Epoch 196/200\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 966.7591 - val_loss: 1085.2277\n",
      "Epoch 197/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 952.3916 - val_loss: 1084.2568\n",
      "Epoch 198/200\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 949.2507 - val_loss: 1086.2202\n",
      "Epoch 199/200\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 953.6482 - val_loss: 1078.0600\n",
      "Epoch 200/200\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 945.0197 - val_loss: 1092.2157\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.6631266439226"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_val_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9670.683761764963,
          9225.110385439642,
          8692.584822883246,
          8062.891395264406,
          7330.823875083029,
          6491.428736002669,
          5586.986952110644,
          4747.7050817477,
          4225.413944811196,
          3933.423072596172,
          3600.2719618371048,
          3250.12002260565,
          2844.6369870256417,
          2418.3455995843096,
          1995.798037544099,
          1641.9100543982715,
          1464.804332108208,
          1407.1680583383986,
          1395.2991179410217,
          1375.5261249316582,
          1383.7363883156754,
          1354.124234019192,
          1359.381628072833,
          1350.860277003412,
          1345.8547664357015,
          1330.3399555396766,
          1333.1761754634315,
          1291.018726251731,
          1319.3592022363614,
          1284.434149883447,
          1283.626235120493,
          1276.709018252143,
          1260.7224168090943,
          1294.3383126451038,
          1296.1995877513957,
          1255.1969900305246,
          1261.7221035434318,
          1276.6312342755225,
          1246.6014190851292,
          1252.4994761526452,
          1289.3439705441178,
          1235.377225855584,
          1260.1218409808862,
          1227.2375958497953,
          1231.330396408592,
          1244.245690210563,
          1228.0023803833326,
          1200.9999299692872,
          1207.355751173755,
          1231.4693392273034,
          1198.1811277926124,
          1220.3732594149085,
          1182.5300282932892,
          1206.258058133765,
          1205.7236027538788,
          1186.58962685306,
          1178.5421957685685,
          1168.957573969665,
          1149.7308442379874,
          1175.5403133107015,
          1189.7584131339138,
          1193.6922908987194,
          1175.064759071829,
          1202.2625086455253,
          1173.5466649568175,
          1154.201890466972,
          1158.4335114314797,
          1176.177861047121,
          1160.9439258380382,
          1138.1798682923256,
          1146.2687021656698,
          1155.8023240552388,
          1168.7835496803407,
          1172.9060443628043,
          1148.1515390454636,
          1141.7161827783486,
          1141.9033575430885,
          1145.2284225362896,
          1116.3490914512497,
          1153.5132238350961,
          1133.3145298626036,
          1101.572158841014,
          1113.8746935391393,
          1141.3728093678328,
          1126.1503461245331,
          1106.3992876596337,
          1126.031030630812,
          1107.9188647808521,
          1088.4030261149692,
          1125.887030008491,
          1130.3054032280804,
          1112.427596932689,
          1099.5021216539958,
          1092.023050042415,
          1121.8045711329926,
          1093.281694294815,
          1098.4189509913274,
          1097.2683834904344,
          1086.399952963643,
          1061.4053165304988,
          1078.9268994894537,
          1066.301907922023,
          1087.494162997622,
          1069.3525499061232,
          1074.1006662243708,
          1080.4507312518408,
          1072.543326467174,
          1064.04484200425,
          1058.305302942925,
          1070.9816064981844,
          1066.8383399377444,
          1082.2683112567283,
          1048.8162627861736,
          1084.424775607987,
          1042.1835393605406,
          1035.4439366571837,
          1039.1439969751432,
          1054.142059442441,
          1069.067898508779,
          1033.6921272652646,
          1037.6131445175424,
          1033.6452500622713,
          1056.4333535258077,
          1049.5471095943203,
          1037.4062208960092,
          1029.3351695854535,
          1015.3252551604876,
          1020.0005156228459,
          1019.2210218491907,
          1046.7381909027927,
          1049.3870310855098,
          1026.2974757440634,
          1013.4393601953948,
          1009.9328365322103,
          1018.728096517743,
          1009.7395398078951,
          1015.9797529484864,
          1002.6646399045723,
          1086.8367398301991,
          1031.2911034142874,
          1020.4287830977729,
          1001.4705046842494,
          1016.6848073423519,
          1036.0261818912768,
          1028.2660593691057,
          1014.924749098633,
          987.2850147199124,
          1016.3930250389293,
          1013.337738116662,
          1008.7841575841328,
          1027.139505854871,
          999.540635706548,
          989.8336139783751,
          987.5129351083325,
          1000.5295726897747,
          1014.9664865956173,
          980.778597424593,
          992.1832250300978,
          1018.0580794734278,
          1004.1634006963026,
          979.5975222345677,
          999.2067495850539,
          972.039218912532,
          977.4378785477006,
          992.1829611482041,
          982.5387464194014,
          993.4017331169439,
          993.2936722759448,
          972.0144049210564,
          982.3668345559784,
          973.2843149145787,
          978.266189994376,
          976.0469491919224,
          961.3672879320217,
          966.7036315416176,
          950.8529217346938,
          966.5327383913018,
          973.3772699644074,
          967.697459390509,
          963.9722362370679,
          955.3759760362293,
          953.8299687341384,
          947.0423208129222,
          948.6804263105558,
          957.8253465602363,
          966.7591267569118,
          952.3915869860269,
          949.2506783628645,
          953.6482319729539,
          945.0196697448522
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          9071.253213237229,
          8560.267114615188,
          7952.182148782146,
          7254.562721082627,
          6436.245732850724,
          5616.378376258742,
          4653.195930415173,
          4770.998635221106,
          3871.2983164430643,
          3366.513040839777,
          2987.6250591870894,
          2383.3906223074523,
          1879.1172342034602,
          1604.2971007333913,
          1676.2520205121207,
          1897.1624830271596,
          2092.339512126059,
          1873.7667247832646,
          1883.0595374878983,
          1624.5270410097503,
          1508.5553911360944,
          1448.068175890127,
          1429.1919589320905,
          1378.5760709289466,
          1273.4256031012665,
          1263.2702361589352,
          1279.6331171250329,
          1193.1610716916719,
          1186.0208874127418,
          1205.4327193819165,
          1178.401916516145,
          1175.2975987812454,
          1173.942851805319,
          1163.2263348573479,
          1165.2159598843712,
          1160.5784712126529,
          1150.7810043172797,
          1161.775323115491,
          1162.2231145215842,
          1156.9567726430516,
          1164.4549101345137,
          1145.0167694550753,
          1144.7126793089767,
          1134.2025579250192,
          1135.973986711152,
          1138.9616000625062,
          1135.0721599547496,
          1135.5179301878243,
          1140.1757069112114,
          1149.6816418733629,
          1139.8815754212906,
          1136.819890504374,
          1134.7746448872538,
          1142.8217578105418,
          1123.7751413930125,
          1133.2383934313198,
          1127.8616902236831,
          1127.922219890857,
          1120.989982352554,
          1124.1042197506486,
          1128.6164214212813,
          1120.0758743974568,
          1117.6391685031472,
          1122.8786668580497,
          1118.2370846329363,
          1108.505357679968,
          1120.8856370068036,
          1138.623429534955,
          1143.429720153258,
          1134.395752687456,
          1125.534341387216,
          1122.1969218111622,
          1134.4713953544842,
          1106.8236022055783,
          1123.433274389389,
          1131.7140629112255,
          1125.2911401186052,
          1115.6518110906716,
          1109.7883600633131,
          1117.730022962045,
          1111.4710954536483,
          1123.8284358178921,
          1122.3668717131338,
          1106.459151729066,
          1109.9909803330456,
          1105.4741777841136,
          1148.1361818805065,
          1113.3195535932487,
          1120.246104936311,
          1103.6825975808588,
          1113.2225984091842,
          1114.155584010615,
          1118.3098359445496,
          1106.042052255787,
          1105.0727082063756,
          1115.2942350550886,
          1108.4659654652878,
          1108.209990604478,
          1106.1887588393886,
          1103.0448992820786,
          1106.3490998960579,
          1114.4593241744942,
          1112.6518133670982,
          1104.4592361037135,
          1101.3016232585583,
          1109.3442989614796,
          1109.6159262373185,
          1095.852900010731,
          1113.8508814568268,
          1096.9833393238437,
          1091.9085204784392,
          1089.2966474797363,
          1122.0640306398197,
          1096.7957109138056,
          1097.1798049195486,
          1095.4493413343444,
          1103.0736099405328,
          1104.629105645381,
          1121.292240146647,
          1095.4530225363287,
          1101.0388924778642,
          1091.936125625846,
          1094.3942315694635,
          1095.5341185421942,
          1092.9663270499393,
          1091.8546118834752,
          1112.2646221974007,
          1100.7910683499756,
          1109.1087642323166,
          1095.3374708813226,
          1096.9931668774361,
          1107.8643288223209,
          1101.5852402525863,
          1116.1109984592754,
          1104.3792521444427,
          1098.4589710836065,
          1095.2978328615338,
          1101.0200174261674,
          1088.4710111279562,
          1093.3851375637203,
          1092.5382135397738,
          1092.3185358837275,
          1093.0788280241518,
          1095.044603346631,
          1093.3437134058317,
          1093.2624096821658,
          1082.6257241973663,
          1079.4658779085387,
          1085.1813679923268,
          1116.4360733904246,
          1097.2733417921752,
          1092.6694865635027,
          1093.7098547042624,
          1088.8084204968072,
          1107.366744037427,
          1094.64065133801,
          1089.4233588384486,
          1095.73116976698,
          1096.29811004706,
          1099.655670588256,
          1097.8209040282077,
          1097.8576888043679,
          1084.9647325477883,
          1106.2662698161705,
          1090.864476276013,
          1095.0534893408408,
          1108.7535584952518,
          1095.826805382471,
          1091.843877088242,
          1100.1175643097836,
          1085.5614904170766,
          1095.0610127346727,
          1094.5865683883567,
          1106.0059132749054,
          1084.2180222044112,
          1102.7276878379882,
          1084.6802288978688,
          1111.5827418622403,
          1087.6911964271167,
          1081.783583288229,
          1078.2366743131752,
          1084.2765892247578,
          1083.980309767309,
          1102.3102538034436,
          1089.7542935851966,
          1085.2277328524867,
          1084.2567664942521,
          1086.2201829003457,
          1078.0599644769761,
          1092.2156825275013
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199
         ],
         "y": [
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226,
          1152.6631266439226
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          199
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"2d223894-005f-4288-bf75-146a32c83a7d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"2d223894-005f-4288-bf75-146a32c83a7d\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '2d223894-005f-4288-bf75-146a32c83a7d',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9670.683761764963, 9225.110385439642, 8692.584822883246, 8062.891395264406, 7330.823875083029, 6491.428736002669, 5586.986952110644, 4747.7050817477, 4225.413944811196, 3933.423072596172, 3600.2719618371048, 3250.12002260565, 2844.6369870256417, 2418.3455995843096, 1995.798037544099, 1641.9100543982715, 1464.804332108208, 1407.1680583383986, 1395.2991179410217, 1375.5261249316582, 1383.7363883156754, 1354.124234019192, 1359.381628072833, 1350.860277003412, 1345.8547664357015, 1330.3399555396766, 1333.1761754634315, 1291.018726251731, 1319.3592022363614, 1284.434149883447, 1283.626235120493, 1276.709018252143, 1260.7224168090943, 1294.3383126451038, 1296.1995877513957, 1255.1969900305246, 1261.7221035434318, 1276.6312342755225, 1246.6014190851292, 1252.4994761526452, 1289.3439705441178, 1235.377225855584, 1260.1218409808862, 1227.2375958497953, 1231.330396408592, 1244.245690210563, 1228.0023803833326, 1200.9999299692872, 1207.355751173755, 1231.4693392273034, 1198.1811277926124, 1220.3732594149085, 1182.5300282932892, 1206.258058133765, 1205.7236027538788, 1186.58962685306, 1178.5421957685685, 1168.957573969665, 1149.7308442379874, 1175.5403133107015, 1189.7584131339138, 1193.6922908987194, 1175.064759071829, 1202.2625086455253, 1173.5466649568175, 1154.201890466972, 1158.4335114314797, 1176.177861047121, 1160.9439258380382, 1138.1798682923256, 1146.2687021656698, 1155.8023240552388, 1168.7835496803407, 1172.9060443628043, 1148.1515390454636, 1141.7161827783486, 1141.9033575430885, 1145.2284225362896, 1116.3490914512497, 1153.5132238350961, 1133.3145298626036, 1101.572158841014, 1113.8746935391393, 1141.3728093678328, 1126.1503461245331, 1106.3992876596337, 1126.031030630812, 1107.9188647808521, 1088.4030261149692, 1125.887030008491, 1130.3054032280804, 1112.427596932689, 1099.5021216539958, 1092.023050042415, 1121.8045711329926, 1093.281694294815, 1098.4189509913274, 1097.2683834904344, 1086.399952963643, 1061.4053165304988, 1078.9268994894537, 1066.301907922023, 1087.494162997622, 1069.3525499061232, 1074.1006662243708, 1080.4507312518408, 1072.543326467174, 1064.04484200425, 1058.305302942925, 1070.9816064981844, 1066.8383399377444, 1082.2683112567283, 1048.8162627861736, 1084.424775607987, 1042.1835393605406, 1035.4439366571837, 1039.1439969751432, 1054.142059442441, 1069.067898508779, 1033.6921272652646, 1037.6131445175424, 1033.6452500622713, 1056.4333535258077, 1049.5471095943203, 1037.4062208960092, 1029.3351695854535, 1015.3252551604876, 1020.0005156228459, 1019.2210218491907, 1046.7381909027927, 1049.3870310855098, 1026.2974757440634, 1013.4393601953948, 1009.9328365322103, 1018.728096517743, 1009.7395398078951, 1015.9797529484864, 1002.6646399045723, 1086.8367398301991, 1031.2911034142874, 1020.4287830977729, 1001.4705046842494, 1016.6848073423519, 1036.0261818912768, 1028.2660593691057, 1014.924749098633, 987.2850147199124, 1016.3930250389293, 1013.337738116662, 1008.7841575841328, 1027.139505854871, 999.540635706548, 989.8336139783751, 987.5129351083325, 1000.5295726897747, 1014.9664865956173, 980.778597424593, 992.1832250300978, 1018.0580794734278, 1004.1634006963026, 979.5975222345677, 999.2067495850539, 972.039218912532, 977.4378785477006, 992.1829611482041, 982.5387464194014, 993.4017331169439, 993.2936722759448, 972.0144049210564, 982.3668345559784, 973.2843149145787, 978.266189994376, 976.0469491919224, 961.3672879320217, 966.7036315416176, 950.8529217346938, 966.5327383913018, 973.3772699644074, 967.697459390509, 963.9722362370679, 955.3759760362293, 953.8299687341384, 947.0423208129222, 948.6804263105558, 957.8253465602363, 966.7591267569118, 952.3915869860269, 949.2506783628645, 953.6482319729539, 945.0196697448522]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [9071.253213237229, 8560.267114615188, 7952.182148782146, 7254.562721082627, 6436.245732850724, 5616.378376258742, 4653.195930415173, 4770.998635221106, 3871.2983164430643, 3366.513040839777, 2987.6250591870894, 2383.3906223074523, 1879.1172342034602, 1604.2971007333913, 1676.2520205121207, 1897.1624830271596, 2092.339512126059, 1873.7667247832646, 1883.0595374878983, 1624.5270410097503, 1508.5553911360944, 1448.068175890127, 1429.1919589320905, 1378.5760709289466, 1273.4256031012665, 1263.2702361589352, 1279.6331171250329, 1193.1610716916719, 1186.0208874127418, 1205.4327193819165, 1178.401916516145, 1175.2975987812454, 1173.942851805319, 1163.2263348573479, 1165.2159598843712, 1160.5784712126529, 1150.7810043172797, 1161.775323115491, 1162.2231145215842, 1156.9567726430516, 1164.4549101345137, 1145.0167694550753, 1144.7126793089767, 1134.2025579250192, 1135.973986711152, 1138.9616000625062, 1135.0721599547496, 1135.5179301878243, 1140.1757069112114, 1149.6816418733629, 1139.8815754212906, 1136.819890504374, 1134.7746448872538, 1142.8217578105418, 1123.7751413930125, 1133.2383934313198, 1127.8616902236831, 1127.922219890857, 1120.989982352554, 1124.1042197506486, 1128.6164214212813, 1120.0758743974568, 1117.6391685031472, 1122.8786668580497, 1118.2370846329363, 1108.505357679968, 1120.8856370068036, 1138.623429534955, 1143.429720153258, 1134.395752687456, 1125.534341387216, 1122.1969218111622, 1134.4713953544842, 1106.8236022055783, 1123.433274389389, 1131.7140629112255, 1125.2911401186052, 1115.6518110906716, 1109.7883600633131, 1117.730022962045, 1111.4710954536483, 1123.8284358178921, 1122.3668717131338, 1106.459151729066, 1109.9909803330456, 1105.4741777841136, 1148.1361818805065, 1113.3195535932487, 1120.246104936311, 1103.6825975808588, 1113.2225984091842, 1114.155584010615, 1118.3098359445496, 1106.042052255787, 1105.0727082063756, 1115.2942350550886, 1108.4659654652878, 1108.209990604478, 1106.1887588393886, 1103.0448992820786, 1106.3490998960579, 1114.4593241744942, 1112.6518133670982, 1104.4592361037135, 1101.3016232585583, 1109.3442989614796, 1109.6159262373185, 1095.852900010731, 1113.8508814568268, 1096.9833393238437, 1091.9085204784392, 1089.2966474797363, 1122.0640306398197, 1096.7957109138056, 1097.1798049195486, 1095.4493413343444, 1103.0736099405328, 1104.629105645381, 1121.292240146647, 1095.4530225363287, 1101.0388924778642, 1091.936125625846, 1094.3942315694635, 1095.5341185421942, 1092.9663270499393, 1091.8546118834752, 1112.2646221974007, 1100.7910683499756, 1109.1087642323166, 1095.3374708813226, 1096.9931668774361, 1107.8643288223209, 1101.5852402525863, 1116.1109984592754, 1104.3792521444427, 1098.4589710836065, 1095.2978328615338, 1101.0200174261674, 1088.4710111279562, 1093.3851375637203, 1092.5382135397738, 1092.3185358837275, 1093.0788280241518, 1095.044603346631, 1093.3437134058317, 1093.2624096821658, 1082.6257241973663, 1079.4658779085387, 1085.1813679923268, 1116.4360733904246, 1097.2733417921752, 1092.6694865635027, 1093.7098547042624, 1088.8084204968072, 1107.366744037427, 1094.64065133801, 1089.4233588384486, 1095.73116976698, 1096.29811004706, 1099.655670588256, 1097.8209040282077, 1097.8576888043679, 1084.9647325477883, 1106.2662698161705, 1090.864476276013, 1095.0534893408408, 1108.7535584952518, 1095.826805382471, 1091.843877088242, 1100.1175643097836, 1085.5614904170766, 1095.0610127346727, 1094.5865683883567, 1106.0059132749054, 1084.2180222044112, 1102.7276878379882, 1084.6802288978688, 1111.5827418622403, 1087.6911964271167, 1081.783583288229, 1078.2366743131752, 1084.2765892247578, 1083.980309767309, 1102.3102538034436, 1089.7542935851966, 1085.2277328524867, 1084.2567664942521, 1086.2201829003457, 1078.0599644769761, 1092.2156825275013]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226, 1152.6631266439226]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 199], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2d223894-005f-4288-bf75-146a32c83a7d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.25% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1092   : Mean absolute error \n",
      "\n",
      "9.42% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(512, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"LeakyRELU\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeakyRELU\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               22528     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 521,473\n",
      "Trainable params: 518,401\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 2s 102us/sample - loss: 11193.0127 - val_loss: 11115.5698\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 11179.0485 - val_loss: 11076.8315\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11154.1231 - val_loss: 11051.6712\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 11116.2460 - val_loss: 11000.3210\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 11061.6303 - val_loss: 10916.0251\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 10985.2781 - val_loss: 10823.6700\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 10882.4478 - val_loss: 10676.5590\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10747.9906 - val_loss: 10486.9070\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 10576.7048 - val_loss: 10249.2206\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 10362.4498 - val_loss: 10042.6964\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 10101.0267 - val_loss: 9625.0623\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 9782.7242 - val_loss: 9241.6529\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 9402.7126 - val_loss: 8811.1780\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 8956.2505 - val_loss: 8232.0926\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 8437.4450 - val_loss: 7691.1685\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 7833.0335 - val_loss: 6745.8952\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 7165.7087 - val_loss: 6163.3554\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 6443.5811 - val_loss: 5371.3568\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 5732.4329 - val_loss: 4796.3182\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 5037.2768 - val_loss: 4203.2533\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 4339.2408 - val_loss: 4246.9137\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 3754.1152 - val_loss: 3893.6419\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 3252.2073 - val_loss: 3027.0482\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 2709.4420 - val_loss: 2151.9892\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 2112.9057 - val_loss: 2132.2476\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1669.1694 - val_loss: 2321.4111\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1502.1890 - val_loss: 2287.3524\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1447.7487 - val_loss: 2092.3731\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1449.0540 - val_loss: 2123.7364\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1439.9087 - val_loss: 2029.4665\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1428.4208 - val_loss: 1669.2993\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1406.3125 - val_loss: 1643.7299\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1415.1262 - val_loss: 1482.3846\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1400.8867 - val_loss: 1400.6500\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1399.5140 - val_loss: 1385.1454\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1380.5399 - val_loss: 1332.0260\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1380.1321 - val_loss: 1303.5089\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1383.6279 - val_loss: 1327.6056\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1365.6600 - val_loss: 1297.2493\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1368.5076 - val_loss: 1257.3673\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1360.9166 - val_loss: 1256.1912\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1357.3175 - val_loss: 1228.5546\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1339.1299 - val_loss: 1236.4686\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1350.3274 - val_loss: 1247.6505\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1350.1369 - val_loss: 1237.7206\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1339.8474 - val_loss: 1212.6851\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1334.5111 - val_loss: 1222.5844\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1342.3796 - val_loss: 1234.3003\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1343.2164 - val_loss: 1205.3266\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1339.0420 - val_loss: 1208.6059\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1331.5974 - val_loss: 1221.1268\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1322.9461 - val_loss: 1221.1269\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1324.2761 - val_loss: 1211.3813\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1312.5883 - val_loss: 1216.9407\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1316.7267 - val_loss: 1217.1500\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1324.6868 - val_loss: 1221.4783\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1313.3795 - val_loss: 1208.2271\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1302.9388 - val_loss: 1206.5492\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1305.6362 - val_loss: 1195.6107\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1309.6280 - val_loss: 1194.7371\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1296.7489 - val_loss: 1197.5848\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1310.2763 - val_loss: 1195.2860\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1308.2755 - val_loss: 1190.3054\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1306.8797 - val_loss: 1196.6276\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1301.8508 - val_loss: 1198.0306\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1290.5049 - val_loss: 1200.1785\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1299.9735 - val_loss: 1184.4199\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1291.5087 - val_loss: 1203.2457\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1282.6841 - val_loss: 1194.8394\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1298.0531 - val_loss: 1198.8875\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1297.9863 - val_loss: 1191.5758\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1298.5611 - val_loss: 1218.7372\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1292.1154 - val_loss: 1181.7329\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1287.8507 - val_loss: 1194.7818\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1285.8532 - val_loss: 1176.6545\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1282.9132 - val_loss: 1194.8673\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1280.3733 - val_loss: 1190.3676\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1276.3582 - val_loss: 1178.7731\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1265.9267 - val_loss: 1192.6601\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1274.9310 - val_loss: 1179.9138\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1276.2884 - val_loss: 1175.4010\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1278.5614 - val_loss: 1169.8091\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1273.2740 - val_loss: 1177.2147\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1267.4878 - val_loss: 1174.7678\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1285.7337 - val_loss: 1171.1957\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1274.4474 - val_loss: 1181.9886\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1269.0036 - val_loss: 1177.8745\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1261.7397 - val_loss: 1184.4206\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1262.2088 - val_loss: 1170.9759\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1270.2523 - val_loss: 1177.6772\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1266.3899 - val_loss: 1192.8491\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1256.7152 - val_loss: 1172.2117\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1250.1207 - val_loss: 1169.7826\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1260.4643 - val_loss: 1164.2425\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1266.6410 - val_loss: 1179.4456\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1268.0581 - val_loss: 1191.3968\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1258.6531 - val_loss: 1166.8404\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1259.2524 - val_loss: 1182.7254\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1255.7143 - val_loss: 1164.5224\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1268.1807 - val_loss: 1152.0935\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1258.3713 - val_loss: 1161.0175\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1255.5710 - val_loss: 1177.2770\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1243.9833 - val_loss: 1174.9361\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1253.7425 - val_loss: 1180.6764\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1251.6908 - val_loss: 1163.0654\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1245.9281 - val_loss: 1157.9226\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1250.7315 - val_loss: 1173.0591\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1253.2123 - val_loss: 1155.3830\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1241.1805 - val_loss: 1167.0113\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1251.1329 - val_loss: 1175.9168\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1250.7452 - val_loss: 1154.3849\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1247.6128 - val_loss: 1167.7159\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1249.5014 - val_loss: 1166.4960\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1246.0094 - val_loss: 1155.0827\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1249.4180 - val_loss: 1183.9248\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1249.8760 - val_loss: 1167.7850\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1232.7336 - val_loss: 1150.5079\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1240.2663 - val_loss: 1163.1608\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1231.4467 - val_loss: 1156.4753\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1245.0754 - val_loss: 1147.7470\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1239.2691 - val_loss: 1161.3710\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1223.3968 - val_loss: 1166.8528\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1220.7221 - val_loss: 1150.5486\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1237.5805 - val_loss: 1150.4247\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1229.4031 - val_loss: 1163.9999\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1220.9996 - val_loss: 1155.4645\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1228.3180 - val_loss: 1176.6479\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1232.8720 - val_loss: 1158.2819\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1241.8130 - val_loss: 1198.3900\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1220.7636 - val_loss: 1146.2971\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1230.0832 - val_loss: 1149.2345\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1229.0748 - val_loss: 1149.5753\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1238.8574 - val_loss: 1159.5785\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1217.8543 - val_loss: 1150.8527\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1231.3070 - val_loss: 1152.9019\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1216.5698 - val_loss: 1135.6916\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1226.9654 - val_loss: 1151.9073\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1215.5541 - val_loss: 1150.8010\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1223.1631 - val_loss: 1168.4076\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1228.9540 - val_loss: 1161.7310\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1227.1104 - val_loss: 1142.7039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1222.1971 - val_loss: 1152.2460\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1218.2522 - val_loss: 1175.2635\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1233.3974 - val_loss: 1151.7260\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1216.8632 - val_loss: 1141.8869\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1214.8327 - val_loss: 1149.2506\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1229.5001 - val_loss: 1154.5285\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1220.1851 - val_loss: 1148.3292\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1220.3347 - val_loss: 1147.5061\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1221.1023 - val_loss: 1145.2734\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1208.0425 - val_loss: 1153.5323\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1210.9684 - val_loss: 1142.8475\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1215.7772 - val_loss: 1147.5340\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1216.4186 - val_loss: 1145.4553\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1208.3921 - val_loss: 1133.7568\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1216.1985 - val_loss: 1139.7538\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1211.7768 - val_loss: 1131.4587\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1199.1232 - val_loss: 1141.5173\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1203.3708 - val_loss: 1131.9324\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1206.1585 - val_loss: 1141.5068\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1205.3008 - val_loss: 1137.7498\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1195.8394 - val_loss: 1138.8999\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1209.3814 - val_loss: 1149.9371\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1213.4789 - val_loss: 1140.6624\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1202.1683 - val_loss: 1137.7466\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1205.0012 - val_loss: 1138.3105\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1193.0223 - val_loss: 1137.2133\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1199.8483 - val_loss: 1146.0542\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1198.6735 - val_loss: 1134.5001\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1204.6576 - val_loss: 1129.5748\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1199.9683 - val_loss: 1133.4744\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1201.9880 - val_loss: 1137.8533\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1197.9298 - val_loss: 1132.1443\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1197.2527 - val_loss: 1131.2802\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1193.4713 - val_loss: 1137.2314\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1196.3062 - val_loss: 1131.6081\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1191.8690 - val_loss: 1136.3096\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1189.7969 - val_loss: 1134.2881\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1194.4127 - val_loss: 1125.8388\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1195.6900 - val_loss: 1148.0939\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1201.8210 - val_loss: 1123.2739\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - ETA: 0s - loss: 1193.46 - 0s 18us/sample - loss: 1197.5734 - val_loss: 1126.5859\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1192.1088 - val_loss: 1133.7753\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1187.7391 - val_loss: 1128.5317\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1197.3579 - val_loss: 1153.0525\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1195.7200 - val_loss: 1130.5285\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1191.4276 - val_loss: 1132.7883\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1188.7658 - val_loss: 1138.6092\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1192.6236 - val_loss: 1146.3914\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1188.3081 - val_loss: 1119.5407\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1184.0778 - val_loss: 1129.0509\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1186.0020 - val_loss: 1122.7461\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1181.0872 - val_loss: 1127.7126\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1185.1083 - val_loss: 1139.8633\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1183.7619 - val_loss: 1139.9184\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1189.6441 - val_loss: 1120.3082\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1182.9676 - val_loss: 1117.6132\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1177.4225 - val_loss: 1122.1455\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1181.4845 - val_loss: 1148.9628\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1183.0277 - val_loss: 1124.2213\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1185.4660 - val_loss: 1125.8767\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1180.9074 - val_loss: 1147.4147\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1184.3707 - val_loss: 1122.6358\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1176.6321 - val_loss: 1117.9536\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1188.7369 - val_loss: 1116.9246\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1184.8718 - val_loss: 1127.3256\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1181.0430 - val_loss: 1115.8413\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1185.0602 - val_loss: 1122.5599\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1171.5061 - val_loss: 1120.0444\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1185.7418 - val_loss: 1135.8673\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1168.1424 - val_loss: 1130.3656\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1176.5006 - val_loss: 1124.2316\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1178.5668 - val_loss: 1121.3620\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1177.3751 - val_loss: 1139.5700\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1175.1228 - val_loss: 1130.1917\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1171.0865 - val_loss: 1128.8980\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1173.1549 - val_loss: 1130.3500\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1183.5341 - val_loss: 1118.5328\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1171.2143 - val_loss: 1119.8638\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1170.6368 - val_loss: 1116.9868\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1182.6857 - val_loss: 1109.4958\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1160.3409 - val_loss: 1127.0981\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1172.8551 - val_loss: 1119.5049\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1165.4992 - val_loss: 1116.3098\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1173.2088 - val_loss: 1119.6507\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1180.1385 - val_loss: 1121.1494\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1172.8363 - val_loss: 1120.0312\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1169.7993 - val_loss: 1117.0289\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1171.9413 - val_loss: 1120.6853\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1169.4166 - val_loss: 1118.8300\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1165.0842 - val_loss: 1115.8845\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1165.2241 - val_loss: 1138.5024\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 13us/sample - loss: 1167.5817 - val_loss: 1127.9853\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1172.7236 - val_loss: 1128.2653\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.9416 - val_loss: 1106.2379\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1162.3510 - val_loss: 1106.0474\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1164.4516 - val_loss: 1122.5047\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1161.0660 - val_loss: 1120.7388\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1165.5073 - val_loss: 1117.0320\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1167.8368 - val_loss: 1117.1998\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1164.2286 - val_loss: 1117.9837\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1160.3933 - val_loss: 1125.9916\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1164.5583 - val_loss: 1128.4159\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1168.5742 - val_loss: 1110.5811\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1157.2296 - val_loss: 1131.4880\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1166.3244 - val_loss: 1114.8907\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1158.0044 - val_loss: 1115.0190\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.6135 - val_loss: 1111.9269\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.2309 - val_loss: 1134.4677\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1159.3286 - val_loss: 1111.4891\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1158.5480 - val_loss: 1130.1699\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1169.4896 - val_loss: 1126.9444\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.6745 - val_loss: 1109.7588\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1157.9995 - val_loss: 1113.0551\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1159.4598 - val_loss: 1121.2562\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1160.9849 - val_loss: 1112.8044\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1158.8839 - val_loss: 1107.8368\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1155.2932 - val_loss: 1114.0408\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1158.6255 - val_loss: 1121.1915\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1158.1188 - val_loss: 1116.7902\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1154.7757 - val_loss: 1118.2503\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1149.2330 - val_loss: 1108.9824\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1152.1913 - val_loss: 1115.6540\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.6999 - val_loss: 1108.8204\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1157.1866 - val_loss: 1106.2024\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1151.8656 - val_loss: 1114.0410\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1146.0206 - val_loss: 1112.3643\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1153.7524 - val_loss: 1113.1306\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1148.1519 - val_loss: 1113.1545\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1150.6476 - val_loss: 1104.6228\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1145.0360 - val_loss: 1110.0547\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1143.8183 - val_loss: 1115.4714\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1150.6910 - val_loss: 1113.2696\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1158.6992 - val_loss: 1117.1838\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1154.2106 - val_loss: 1133.8302\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1148.0886 - val_loss: 1110.4373\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.9077 - val_loss: 1111.9154\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1145.3283 - val_loss: 1110.8313\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1145.6317 - val_loss: 1121.3621\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1148.0003 - val_loss: 1107.3460\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1140.2352 - val_loss: 1115.5169\n",
      "Epoch 282/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1145.3176 - val_loss: 1121.2578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1143.6011 - val_loss: 1106.4081\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1149.0149 - val_loss: 1113.8873\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1138.9183 - val_loss: 1104.9291\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1136.4113 - val_loss: 1107.5634\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1142.2239 - val_loss: 1113.8885\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1141.9110 - val_loss: 1107.5968\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1149.4104 - val_loss: 1111.9556\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1138.4921 - val_loss: 1104.6994\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1148.8138 - val_loss: 1103.1774\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1138.8394 - val_loss: 1096.3054\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1144.9505 - val_loss: 1099.5432\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 14us/sample - loss: 1149.4952 - val_loss: 1105.1204\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 15us/sample - loss: 1142.5337 - val_loss: 1103.6606\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1145.8939 - val_loss: 1110.9277\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1141.4260 - val_loss: 1107.4551\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1142.7808 - val_loss: 1120.5572\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1142.9270 - val_loss: 1104.7578\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1137.3329 - val_loss: 1099.1921\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10101.026746102367,
          9782.724174259325,
          9402.712611187526,
          8956.250506394752,
          8437.444990079675,
          7833.033485303978,
          7165.7087343107705,
          6443.581113923545,
          5732.432930306296,
          5037.276838040624,
          4339.240753693979,
          3754.115237703968,
          3252.20733345174,
          2709.442038362242,
          2112.905664028231,
          1669.169445759169,
          1502.1890238105402,
          1447.7486617059824,
          1449.053954001106,
          1439.90873000759,
          1428.420832910685,
          1406.3125023009043,
          1415.1262067263558,
          1400.8866877857038,
          1399.5140269730596,
          1380.5399367701727,
          1380.132081889475,
          1383.6278546878134,
          1365.660022454867,
          1368.507644411603,
          1360.91657332502,
          1357.317548575515,
          1339.1299383768892,
          1350.3274312773758,
          1350.136943014729,
          1339.8473624196347,
          1334.5111471466437,
          1342.3796125541642,
          1343.2163882442005,
          1339.0420470909912,
          1331.5974297333223,
          1322.9460798270816,
          1324.2761311000397,
          1312.5883433156675,
          1316.7267237346398,
          1324.6868425288562,
          1313.3795118039325,
          1302.9387914681688,
          1305.6362141176435,
          1309.627968876305,
          1296.748868101987,
          1310.2762676366756,
          1308.2754880952195,
          1306.879703709136,
          1301.8508440059388,
          1290.5049391356981,
          1299.9735348772042,
          1291.5086725731041,
          1282.6841447676068,
          1298.0530692055204,
          1297.9863269011148,
          1298.5610866083848,
          1292.1153948263532,
          1287.8506631401801,
          1285.8531693878695,
          1282.9131855862352,
          1280.373345356129,
          1276.3581540765756,
          1265.9266861417811,
          1274.9309869479005,
          1276.2884106097729,
          1278.5613502577405,
          1273.2740490264923,
          1267.4878454734262,
          1285.7337053879148,
          1274.447408109718,
          1269.003630214926,
          1261.739748663909,
          1262.2087743562952,
          1270.252264775134,
          1266.3898690305732,
          1256.7151726813936,
          1250.120679783061,
          1260.4642911168648,
          1266.6410416594865,
          1268.0580580554363,
          1258.6531342819455,
          1259.2524382241484,
          1255.714276185465,
          1268.1807317678308,
          1258.3712745668033,
          1255.571022622686,
          1243.983316926744,
          1253.7424764103466,
          1251.6908344263445,
          1245.9280870252892,
          1250.7314786756112,
          1253.212317430593,
          1241.180540719347,
          1251.1328819922032,
          1250.7452246691007,
          1247.612817740187,
          1249.5014264382316,
          1246.0093564312133,
          1249.4179994205638,
          1249.8759812377416,
          1232.7336187368408,
          1240.2663229083116,
          1231.4466537802975,
          1245.0754200178276,
          1239.2690697963142,
          1223.3968096836932,
          1220.7221241047034,
          1237.5804547144842,
          1229.4031392509196,
          1220.9995920056208,
          1228.3180075324751,
          1232.8719785945411,
          1241.813004363102,
          1220.7635575151644,
          1230.0831731593942,
          1229.0747827163125,
          1238.8573672652412,
          1217.8542843473108,
          1231.306988061538,
          1216.5697854215882,
          1226.9653966279122,
          1215.5541417695774,
          1223.1631185662802,
          1228.9540292064055,
          1227.1104442106116,
          1222.1970905115018,
          1218.2521910238104,
          1233.3973581800963,
          1216.8632075721093,
          1214.8327090621162,
          1229.5000525536316,
          1220.1851487696135,
          1220.3347073974364,
          1221.1022546315733,
          1208.042523965631,
          1210.9683737776813,
          1215.777163740961,
          1216.4185887702558,
          1208.3921308194274,
          1216.1985237055799,
          1211.7768231826578,
          1199.123170316064,
          1203.3708286075046,
          1206.1584547313248,
          1205.3007880058465,
          1195.8394201143676,
          1209.3813552688162,
          1213.4789454045363,
          1202.1683221590981,
          1205.0011729960788,
          1193.0223205333868,
          1199.8483061330553,
          1198.6735409280927,
          1204.657571600223,
          1199.9683296688577,
          1201.988003060888,
          1197.929770014342,
          1197.252689022709,
          1193.4712698817707,
          1196.3061510709094,
          1191.8689533705603,
          1189.7969195004669,
          1194.41266227739,
          1195.6900240919358,
          1201.8209657854563,
          1197.573419797724,
          1192.1087626657434,
          1187.739084828562,
          1197.3578992881492,
          1195.7200343696345,
          1191.4276422164423,
          1188.7657876788244,
          1192.6235627181452,
          1188.3081399578357,
          1184.077819371381,
          1186.0020099867077,
          1181.0871664276337,
          1185.1083477686516,
          1183.7619279364628,
          1189.6441234024871,
          1182.9676471815,
          1177.4225184698967,
          1181.484526272214,
          1183.0276618622481,
          1185.4659799560889,
          1180.9074119566342,
          1184.3707022780713,
          1176.6321001008873,
          1188.7369333852,
          1184.8717843639556,
          1181.042987059323,
          1185.0601927070932,
          1171.5060503255731,
          1185.7417899576008,
          1168.14240996317,
          1176.5005889825266,
          1178.5668275357727,
          1177.375069663547,
          1175.1228343106138,
          1171.0864869020781,
          1173.1549009113148,
          1183.5341494085794,
          1171.2143381140575,
          1170.6367890254896,
          1182.6856758969807,
          1160.3408962668173,
          1172.8551483613253,
          1165.4992444956497,
          1173.2088489888163,
          1180.1385196481556,
          1172.8363425815323,
          1169.7993234068736,
          1171.941308730825,
          1169.416634054205,
          1165.084209300627,
          1165.22407801299,
          1167.5817210930059,
          1172.7235731848118,
          1159.9415676804613,
          1162.3510132203103,
          1164.4515596507952,
          1161.0660049869896,
          1165.5073192987313,
          1167.8367510654655,
          1164.228601272488,
          1160.3932990178762,
          1164.5583217047918,
          1168.5741822292648,
          1157.2295781287205,
          1166.3243621893484,
          1158.0044407941116,
          1158.613473522369,
          1159.2309295427879,
          1159.3286177606699,
          1158.547965310393,
          1169.4895569790049,
          1159.6744924009456,
          1157.999517348622,
          1159.459750502674,
          1160.9849308886699,
          1158.8839103250482,
          1155.2932429002908,
          1158.6254646602652,
          1158.1188178864656,
          1154.7757225181958,
          1149.232986967287,
          1152.1913048633478,
          1153.6998798927993,
          1157.186567595279,
          1151.865638893545,
          1146.0205700103472,
          1153.7524474522431,
          1148.1519047423692,
          1150.6475773534628,
          1145.0359922370428,
          1143.81833122564,
          1150.6910423889221,
          1158.6991767952145,
          1154.2105576549811,
          1148.0885883130122,
          1153.907677808983,
          1145.3283395226026,
          1145.631655585773,
          1148.000275300743,
          1140.2351876597904,
          1145.3175967231598,
          1143.6010917692643,
          1149.0149132617853,
          1138.918328865989,
          1136.4112889068217,
          1142.2239184428342,
          1141.911032160179,
          1149.4104187244257,
          1138.4921182770515,
          1148.8137981261045,
          1138.8394134319544,
          1144.950501846794,
          1149.4952206792348,
          1142.5336740515575,
          1145.8938801675372,
          1141.4259771940249,
          1142.7807552811137,
          1142.9269715273383,
          1137.3328748006536
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9625.062301436861,
          9241.652854452828,
          8811.177988551484,
          8232.092584861264,
          7691.168539080516,
          6745.895159152076,
          6163.3554441743845,
          5371.356797693535,
          4796.318230230631,
          4203.253340521293,
          4246.913718245563,
          3893.641872873377,
          3027.048165513538,
          2151.989216494135,
          2132.2476075981144,
          2321.4110653519465,
          2287.3523779992042,
          2092.373085011264,
          2123.7364434149886,
          2029.46645670834,
          1669.2993076677094,
          1643.7299014057644,
          1482.3845709782936,
          1400.6499603559098,
          1385.145402435982,
          1332.0260327975784,
          1303.5089140946147,
          1327.6056034752858,
          1297.249294993154,
          1257.367340265354,
          1256.1912049208802,
          1228.5546182280962,
          1236.4686130227653,
          1247.6505315186673,
          1237.7206483194195,
          1212.6851005974909,
          1222.5843501942943,
          1234.300341605949,
          1205.3266447108185,
          1208.6059449147863,
          1221.1268149973134,
          1221.1269102155838,
          1211.38129412351,
          1216.9406994807593,
          1217.1499737941695,
          1221.4783233771086,
          1208.2270977784426,
          1206.5491832230596,
          1195.6107106504255,
          1194.737058392837,
          1197.584793583434,
          1195.2859926776416,
          1190.305394979094,
          1196.627640483421,
          1198.0305608791843,
          1200.1784584989332,
          1184.4198500329371,
          1203.2456860003979,
          1194.8393736067292,
          1198.887518289741,
          1191.5757758502184,
          1218.7371514815472,
          1181.732907586091,
          1194.781849777194,
          1176.6544929169356,
          1194.867296597129,
          1190.36763899126,
          1178.7731383824507,
          1192.6601257018247,
          1179.913833511074,
          1175.4010435677683,
          1169.8091390398238,
          1177.2147415311058,
          1174.7678213109946,
          1171.195654967564,
          1181.9886189688893,
          1177.8745215587871,
          1184.4205783670354,
          1170.9759311955224,
          1177.677229424433,
          1192.8491401374042,
          1172.211701302292,
          1169.782574170382,
          1164.2425131269033,
          1179.4456478690297,
          1191.3967796495497,
          1166.8403942780526,
          1182.7254149117784,
          1164.522415384492,
          1152.0935221860038,
          1161.0174988661927,
          1177.2769950895768,
          1174.9360757158065,
          1180.6764083932678,
          1163.0654412654542,
          1157.9225511231546,
          1173.0591068516424,
          1155.3830151753934,
          1167.0112774169872,
          1175.916811174875,
          1154.3849291360664,
          1167.715881776016,
          1166.4960337551463,
          1155.0826723455593,
          1183.9247940641762,
          1167.7849862033866,
          1150.5079377279365,
          1163.160848650828,
          1156.4753081400313,
          1147.74704578584,
          1161.3709614235293,
          1166.852793728538,
          1150.5486078256592,
          1150.424682984353,
          1163.9998748944522,
          1155.4645222843064,
          1176.647942756636,
          1158.2818987326032,
          1198.3900493578226,
          1146.2970676884902,
          1149.2344688230414,
          1149.575289600618,
          1159.5784624985902,
          1150.8526904326247,
          1152.9018694210415,
          1135.6916327666777,
          1151.9073115001347,
          1150.8009726411717,
          1168.4075928713482,
          1161.7309830020945,
          1142.7038796721083,
          1152.2460133162629,
          1175.2634649160082,
          1151.7259594672712,
          1141.8868794216526,
          1149.2506021270538,
          1154.5284766760765,
          1148.3291517173166,
          1147.5060612181517,
          1145.273430964453,
          1153.53230525832,
          1142.8474942565515,
          1147.5339920658948,
          1145.4553169784408,
          1133.756836598398,
          1139.7537582577984,
          1131.458673703974,
          1141.5173477898004,
          1131.9324059938654,
          1141.5068089630697,
          1137.7497636922405,
          1138.8999133831946,
          1149.937126788439,
          1140.6624167415357,
          1137.7465769154098,
          1138.3105272438809,
          1137.2133206982687,
          1146.0542203674868,
          1134.5000763704381,
          1129.574779191523,
          1133.474381291749,
          1137.8532918008698,
          1132.1442769511277,
          1131.2802320701794,
          1137.23144350115,
          1131.608083874519,
          1136.3096311640734,
          1134.288105397275,
          1125.8388459898079,
          1148.0939452831267,
          1123.273923382435,
          1126.5858625982241,
          1133.7753096527533,
          1128.5317282943465,
          1153.0524968188774,
          1130.5284765781657,
          1132.7883391348757,
          1138.6092106566919,
          1146.391383270331,
          1119.540702555159,
          1129.0508681213735,
          1122.746146597364,
          1127.712613512907,
          1139.8632729031028,
          1139.9184039880643,
          1120.308247953468,
          1117.6131999595432,
          1122.145511214901,
          1148.9627782478585,
          1124.2212972967998,
          1125.8766695263153,
          1147.4147178415835,
          1122.6357582938297,
          1117.9535959020798,
          1116.9245504620608,
          1127.325623623374,
          1115.8413105519664,
          1122.559936964036,
          1120.0443878938756,
          1135.8673078568731,
          1130.3655996557848,
          1124.2316108062603,
          1121.3620368279794,
          1139.5699727494612,
          1130.1916562407964,
          1128.8980394925243,
          1130.3500341366068,
          1118.5327537633002,
          1119.8637883790825,
          1116.9868397335024,
          1109.4958213376262,
          1127.0980988558533,
          1119.504930617457,
          1116.3097822649436,
          1119.6506724368123,
          1121.1493729399565,
          1120.0311575966655,
          1117.0289279959927,
          1120.6853024161649,
          1118.8299606075404,
          1115.8845231978144,
          1138.5024020461012,
          1127.9852879939012,
          1128.265292054263,
          1106.2378999118412,
          1106.0474125303915,
          1122.5046595268793,
          1120.7387642440658,
          1117.0319794111174,
          1117.1998379624915,
          1117.9837089861373,
          1125.9915983706073,
          1128.4158740704347,
          1110.5811335791418,
          1131.4880118973392,
          1114.8907023985016,
          1115.0190158474534,
          1111.9269371116857,
          1134.4676898461391,
          1111.4890509269412,
          1130.1698708634638,
          1126.944435928923,
          1109.758753031319,
          1113.0551380611073,
          1121.256213273652,
          1112.8044446322156,
          1107.8368091265806,
          1114.040829961611,
          1121.191479658636,
          1116.7902255091753,
          1118.2503041599566,
          1108.9823582084905,
          1115.6539952852024,
          1108.8203578327089,
          1106.2023905709532,
          1114.0409635609224,
          1112.3643394701223,
          1113.1305832175374,
          1113.1544575221749,
          1104.6227780853264,
          1110.0547204225127,
          1115.4713534731318,
          1113.269568529544,
          1117.1837676151354,
          1133.830158974858,
          1110.4373162948275,
          1111.9154376574406,
          1110.8313363279683,
          1121.3621336373008,
          1107.34603102833,
          1115.5168777933957,
          1121.2577534597767,
          1106.4081190685627,
          1113.8872945390438,
          1104.929088799825,
          1107.5633695459758,
          1113.8885149484363,
          1107.5968450833495,
          1111.955599793839,
          1104.699446980117,
          1103.177408797052,
          1096.3053872441394,
          1099.5431942386142,
          1105.1204097450245,
          1103.6605992964521,
          1110.9276893605013,
          1107.455086153687,
          1120.5572157531458,
          1104.757760974432,
          1099.1921439590592
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"13e565af-a298-4090-ba3a-da94a8d95250\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"13e565af-a298-4090-ba3a-da94a8d95250\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '13e565af-a298-4090-ba3a-da94a8d95250',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10101.026746102367, 9782.724174259325, 9402.712611187526, 8956.250506394752, 8437.444990079675, 7833.033485303978, 7165.7087343107705, 6443.581113923545, 5732.432930306296, 5037.276838040624, 4339.240753693979, 3754.115237703968, 3252.20733345174, 2709.442038362242, 2112.905664028231, 1669.169445759169, 1502.1890238105402, 1447.7486617059824, 1449.053954001106, 1439.90873000759, 1428.420832910685, 1406.3125023009043, 1415.1262067263558, 1400.8866877857038, 1399.5140269730596, 1380.5399367701727, 1380.132081889475, 1383.6278546878134, 1365.660022454867, 1368.507644411603, 1360.91657332502, 1357.317548575515, 1339.1299383768892, 1350.3274312773758, 1350.136943014729, 1339.8473624196347, 1334.5111471466437, 1342.3796125541642, 1343.2163882442005, 1339.0420470909912, 1331.5974297333223, 1322.9460798270816, 1324.2761311000397, 1312.5883433156675, 1316.7267237346398, 1324.6868425288562, 1313.3795118039325, 1302.9387914681688, 1305.6362141176435, 1309.627968876305, 1296.748868101987, 1310.2762676366756, 1308.2754880952195, 1306.879703709136, 1301.8508440059388, 1290.5049391356981, 1299.9735348772042, 1291.5086725731041, 1282.6841447676068, 1298.0530692055204, 1297.9863269011148, 1298.5610866083848, 1292.1153948263532, 1287.8506631401801, 1285.8531693878695, 1282.9131855862352, 1280.373345356129, 1276.3581540765756, 1265.9266861417811, 1274.9309869479005, 1276.2884106097729, 1278.5613502577405, 1273.2740490264923, 1267.4878454734262, 1285.7337053879148, 1274.447408109718, 1269.003630214926, 1261.739748663909, 1262.2087743562952, 1270.252264775134, 1266.3898690305732, 1256.7151726813936, 1250.120679783061, 1260.4642911168648, 1266.6410416594865, 1268.0580580554363, 1258.6531342819455, 1259.2524382241484, 1255.714276185465, 1268.1807317678308, 1258.3712745668033, 1255.571022622686, 1243.983316926744, 1253.7424764103466, 1251.6908344263445, 1245.9280870252892, 1250.7314786756112, 1253.212317430593, 1241.180540719347, 1251.1328819922032, 1250.7452246691007, 1247.612817740187, 1249.5014264382316, 1246.0093564312133, 1249.4179994205638, 1249.8759812377416, 1232.7336187368408, 1240.2663229083116, 1231.4466537802975, 1245.0754200178276, 1239.2690697963142, 1223.3968096836932, 1220.7221241047034, 1237.5804547144842, 1229.4031392509196, 1220.9995920056208, 1228.3180075324751, 1232.8719785945411, 1241.813004363102, 1220.7635575151644, 1230.0831731593942, 1229.0747827163125, 1238.8573672652412, 1217.8542843473108, 1231.306988061538, 1216.5697854215882, 1226.9653966279122, 1215.5541417695774, 1223.1631185662802, 1228.9540292064055, 1227.1104442106116, 1222.1970905115018, 1218.2521910238104, 1233.3973581800963, 1216.8632075721093, 1214.8327090621162, 1229.5000525536316, 1220.1851487696135, 1220.3347073974364, 1221.1022546315733, 1208.042523965631, 1210.9683737776813, 1215.777163740961, 1216.4185887702558, 1208.3921308194274, 1216.1985237055799, 1211.7768231826578, 1199.123170316064, 1203.3708286075046, 1206.1584547313248, 1205.3007880058465, 1195.8394201143676, 1209.3813552688162, 1213.4789454045363, 1202.1683221590981, 1205.0011729960788, 1193.0223205333868, 1199.8483061330553, 1198.6735409280927, 1204.657571600223, 1199.9683296688577, 1201.988003060888, 1197.929770014342, 1197.252689022709, 1193.4712698817707, 1196.3061510709094, 1191.8689533705603, 1189.7969195004669, 1194.41266227739, 1195.6900240919358, 1201.8209657854563, 1197.573419797724, 1192.1087626657434, 1187.739084828562, 1197.3578992881492, 1195.7200343696345, 1191.4276422164423, 1188.7657876788244, 1192.6235627181452, 1188.3081399578357, 1184.077819371381, 1186.0020099867077, 1181.0871664276337, 1185.1083477686516, 1183.7619279364628, 1189.6441234024871, 1182.9676471815, 1177.4225184698967, 1181.484526272214, 1183.0276618622481, 1185.4659799560889, 1180.9074119566342, 1184.3707022780713, 1176.6321001008873, 1188.7369333852, 1184.8717843639556, 1181.042987059323, 1185.0601927070932, 1171.5060503255731, 1185.7417899576008, 1168.14240996317, 1176.5005889825266, 1178.5668275357727, 1177.375069663547, 1175.1228343106138, 1171.0864869020781, 1173.1549009113148, 1183.5341494085794, 1171.2143381140575, 1170.6367890254896, 1182.6856758969807, 1160.3408962668173, 1172.8551483613253, 1165.4992444956497, 1173.2088489888163, 1180.1385196481556, 1172.8363425815323, 1169.7993234068736, 1171.941308730825, 1169.416634054205, 1165.084209300627, 1165.22407801299, 1167.5817210930059, 1172.7235731848118, 1159.9415676804613, 1162.3510132203103, 1164.4515596507952, 1161.0660049869896, 1165.5073192987313, 1167.8367510654655, 1164.228601272488, 1160.3932990178762, 1164.5583217047918, 1168.5741822292648, 1157.2295781287205, 1166.3243621893484, 1158.0044407941116, 1158.613473522369, 1159.2309295427879, 1159.3286177606699, 1158.547965310393, 1169.4895569790049, 1159.6744924009456, 1157.999517348622, 1159.459750502674, 1160.9849308886699, 1158.8839103250482, 1155.2932429002908, 1158.6254646602652, 1158.1188178864656, 1154.7757225181958, 1149.232986967287, 1152.1913048633478, 1153.6998798927993, 1157.186567595279, 1151.865638893545, 1146.0205700103472, 1153.7524474522431, 1148.1519047423692, 1150.6475773534628, 1145.0359922370428, 1143.81833122564, 1150.6910423889221, 1158.6991767952145, 1154.2105576549811, 1148.0885883130122, 1153.907677808983, 1145.3283395226026, 1145.631655585773, 1148.000275300743, 1140.2351876597904, 1145.3175967231598, 1143.6010917692643, 1149.0149132617853, 1138.918328865989, 1136.4112889068217, 1142.2239184428342, 1141.911032160179, 1149.4104187244257, 1138.4921182770515, 1148.8137981261045, 1138.8394134319544, 1144.950501846794, 1149.4952206792348, 1142.5336740515575, 1145.8938801675372, 1141.4259771940249, 1142.7807552811137, 1142.9269715273383, 1137.3328748006536]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9625.062301436861, 9241.652854452828, 8811.177988551484, 8232.092584861264, 7691.168539080516, 6745.895159152076, 6163.3554441743845, 5371.356797693535, 4796.318230230631, 4203.253340521293, 4246.913718245563, 3893.641872873377, 3027.048165513538, 2151.989216494135, 2132.2476075981144, 2321.4110653519465, 2287.3523779992042, 2092.373085011264, 2123.7364434149886, 2029.46645670834, 1669.2993076677094, 1643.7299014057644, 1482.3845709782936, 1400.6499603559098, 1385.145402435982, 1332.0260327975784, 1303.5089140946147, 1327.6056034752858, 1297.249294993154, 1257.367340265354, 1256.1912049208802, 1228.5546182280962, 1236.4686130227653, 1247.6505315186673, 1237.7206483194195, 1212.6851005974909, 1222.5843501942943, 1234.300341605949, 1205.3266447108185, 1208.6059449147863, 1221.1268149973134, 1221.1269102155838, 1211.38129412351, 1216.9406994807593, 1217.1499737941695, 1221.4783233771086, 1208.2270977784426, 1206.5491832230596, 1195.6107106504255, 1194.737058392837, 1197.584793583434, 1195.2859926776416, 1190.305394979094, 1196.627640483421, 1198.0305608791843, 1200.1784584989332, 1184.4198500329371, 1203.2456860003979, 1194.8393736067292, 1198.887518289741, 1191.5757758502184, 1218.7371514815472, 1181.732907586091, 1194.781849777194, 1176.6544929169356, 1194.867296597129, 1190.36763899126, 1178.7731383824507, 1192.6601257018247, 1179.913833511074, 1175.4010435677683, 1169.8091390398238, 1177.2147415311058, 1174.7678213109946, 1171.195654967564, 1181.9886189688893, 1177.8745215587871, 1184.4205783670354, 1170.9759311955224, 1177.677229424433, 1192.8491401374042, 1172.211701302292, 1169.782574170382, 1164.2425131269033, 1179.4456478690297, 1191.3967796495497, 1166.8403942780526, 1182.7254149117784, 1164.522415384492, 1152.0935221860038, 1161.0174988661927, 1177.2769950895768, 1174.9360757158065, 1180.6764083932678, 1163.0654412654542, 1157.9225511231546, 1173.0591068516424, 1155.3830151753934, 1167.0112774169872, 1175.916811174875, 1154.3849291360664, 1167.715881776016, 1166.4960337551463, 1155.0826723455593, 1183.9247940641762, 1167.7849862033866, 1150.5079377279365, 1163.160848650828, 1156.4753081400313, 1147.74704578584, 1161.3709614235293, 1166.852793728538, 1150.5486078256592, 1150.424682984353, 1163.9998748944522, 1155.4645222843064, 1176.647942756636, 1158.2818987326032, 1198.3900493578226, 1146.2970676884902, 1149.2344688230414, 1149.575289600618, 1159.5784624985902, 1150.8526904326247, 1152.9018694210415, 1135.6916327666777, 1151.9073115001347, 1150.8009726411717, 1168.4075928713482, 1161.7309830020945, 1142.7038796721083, 1152.2460133162629, 1175.2634649160082, 1151.7259594672712, 1141.8868794216526, 1149.2506021270538, 1154.5284766760765, 1148.3291517173166, 1147.5060612181517, 1145.273430964453, 1153.53230525832, 1142.8474942565515, 1147.5339920658948, 1145.4553169784408, 1133.756836598398, 1139.7537582577984, 1131.458673703974, 1141.5173477898004, 1131.9324059938654, 1141.5068089630697, 1137.7497636922405, 1138.8999133831946, 1149.937126788439, 1140.6624167415357, 1137.7465769154098, 1138.3105272438809, 1137.2133206982687, 1146.0542203674868, 1134.5000763704381, 1129.574779191523, 1133.474381291749, 1137.8532918008698, 1132.1442769511277, 1131.2802320701794, 1137.23144350115, 1131.608083874519, 1136.3096311640734, 1134.288105397275, 1125.8388459898079, 1148.0939452831267, 1123.273923382435, 1126.5858625982241, 1133.7753096527533, 1128.5317282943465, 1153.0524968188774, 1130.5284765781657, 1132.7883391348757, 1138.6092106566919, 1146.391383270331, 1119.540702555159, 1129.0508681213735, 1122.746146597364, 1127.712613512907, 1139.8632729031028, 1139.9184039880643, 1120.308247953468, 1117.6131999595432, 1122.145511214901, 1148.9627782478585, 1124.2212972967998, 1125.8766695263153, 1147.4147178415835, 1122.6357582938297, 1117.9535959020798, 1116.9245504620608, 1127.325623623374, 1115.8413105519664, 1122.559936964036, 1120.0443878938756, 1135.8673078568731, 1130.3655996557848, 1124.2316108062603, 1121.3620368279794, 1139.5699727494612, 1130.1916562407964, 1128.8980394925243, 1130.3500341366068, 1118.5327537633002, 1119.8637883790825, 1116.9868397335024, 1109.4958213376262, 1127.0980988558533, 1119.504930617457, 1116.3097822649436, 1119.6506724368123, 1121.1493729399565, 1120.0311575966655, 1117.0289279959927, 1120.6853024161649, 1118.8299606075404, 1115.8845231978144, 1138.5024020461012, 1127.9852879939012, 1128.265292054263, 1106.2378999118412, 1106.0474125303915, 1122.5046595268793, 1120.7387642440658, 1117.0319794111174, 1117.1998379624915, 1117.9837089861373, 1125.9915983706073, 1128.4158740704347, 1110.5811335791418, 1131.4880118973392, 1114.8907023985016, 1115.0190158474534, 1111.9269371116857, 1134.4676898461391, 1111.4890509269412, 1130.1698708634638, 1126.944435928923, 1109.758753031319, 1113.0551380611073, 1121.256213273652, 1112.8044446322156, 1107.8368091265806, 1114.040829961611, 1121.191479658636, 1116.7902255091753, 1118.2503041599566, 1108.9823582084905, 1115.6539952852024, 1108.8203578327089, 1106.2023905709532, 1114.0409635609224, 1112.3643394701223, 1113.1305832175374, 1113.1544575221749, 1104.6227780853264, 1110.0547204225127, 1115.4713534731318, 1113.269568529544, 1117.1837676151354, 1133.830158974858, 1110.4373162948275, 1111.9154376574406, 1110.8313363279683, 1121.3621336373008, 1107.34603102833, 1115.5168777933957, 1121.2577534597767, 1106.4081190685627, 1113.8872945390438, 1104.929088799825, 1107.5633695459758, 1113.8885149484363, 1107.5968450833495, 1111.955599793839, 1104.699446980117, 1103.177408797052, 1096.3053872441394, 1099.5431942386142, 1105.1204097450245, 1103.6605992964521, 1110.9276893605013, 1107.455086153687, 1120.5572157531458, 1104.757760974432, 1099.1921439590592]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('13e565af-a298-4090-ba3a-da94a8d95250');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.33% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1099   : Mean absolute error \n",
      "\n",
      "9.37% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a larger network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Larger_network\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Larger_network\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              45056     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,072,897\n",
      "Trainable params: 1,067,777\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/300\n",
      "19948/19948 [==============================] - 3s 140us/sample - loss: 11191.7813 - val_loss: 11104.0074\n",
      "Epoch 2/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 11177.7166 - val_loss: 11047.1123\n",
      "Epoch 3/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 11149.8176 - val_loss: 10989.5124\n",
      "Epoch 4/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 11107.0977 - val_loss: 10920.8445\n",
      "Epoch 5/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 11046.7887 - val_loss: 10824.1816\n",
      "Epoch 6/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 10964.4873 - val_loss: 10680.5738\n",
      "Epoch 7/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 10856.0065 - val_loss: 10554.1936\n",
      "Epoch 8/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 10713.6386 - val_loss: 10368.5409\n",
      "Epoch 9/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 10534.6067 - val_loss: 10162.0845\n",
      "Epoch 10/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 10311.2954 - val_loss: 9913.4269\n",
      "Epoch 11/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 10041.6595 - val_loss: 9527.6554\n",
      "Epoch 12/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 9717.1063 - val_loss: 9155.4200\n",
      "Epoch 13/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 9335.3371 - val_loss: 8756.4637\n",
      "Epoch 14/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 8891.3793 - val_loss: 8203.8264\n",
      "Epoch 15/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 8380.4355 - val_loss: 7906.1881\n",
      "Epoch 16/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 7797.6867 - val_loss: 7102.2482\n",
      "Epoch 17/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 7140.0890 - val_loss: 6202.3055\n",
      "Epoch 18/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 6403.0308 - val_loss: 5620.8976\n",
      "Epoch 19/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 5583.1706 - val_loss: 4502.0091\n",
      "Epoch 20/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 4695.7964 - val_loss: 3959.2684\n",
      "Epoch 21/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 3752.7917 - val_loss: 2594.8046\n",
      "Epoch 22/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 2815.2845 - val_loss: 1673.9324\n",
      "Epoch 23/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 2048.3031 - val_loss: 2137.1452\n",
      "Epoch 24/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1623.6189 - val_loss: 2925.2802\n",
      "Epoch 25/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1499.4758 - val_loss: 2669.5426\n",
      "Epoch 26/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1462.7882 - val_loss: 2731.6248\n",
      "Epoch 27/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1440.9266 - val_loss: 2506.5169\n",
      "Epoch 28/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1422.1987 - val_loss: 2311.0731\n",
      "Epoch 29/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1394.6923 - val_loss: 1789.9102\n",
      "Epoch 30/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1405.8011 - val_loss: 1547.9655\n",
      "Epoch 31/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1398.0873 - val_loss: 1517.1572\n",
      "Epoch 32/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1375.5425 - val_loss: 1439.5122\n",
      "Epoch 33/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1368.3589 - val_loss: 1347.1264\n",
      "Epoch 34/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1367.4620 - val_loss: 1331.1920\n",
      "Epoch 35/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1357.3058 - val_loss: 1279.2250\n",
      "Epoch 36/300\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1346.9741 - val_loss: 1278.2877\n",
      "Epoch 37/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1347.6920 - val_loss: 1231.3830\n",
      "Epoch 38/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1346.7002 - val_loss: 1275.1312\n",
      "Epoch 39/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1325.6698 - val_loss: 1241.8470\n",
      "Epoch 40/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1331.1701 - val_loss: 1267.6023\n",
      "Epoch 41/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1333.7427 - val_loss: 1273.7904\n",
      "Epoch 42/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1328.6310 - val_loss: 1245.2584\n",
      "Epoch 43/300\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1314.8409 - val_loss: 1214.1612\n",
      "Epoch 44/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1320.1993 - val_loss: 1222.8221\n",
      "Epoch 45/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1317.2439 - val_loss: 1211.2650\n",
      "Epoch 46/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1304.8098 - val_loss: 1221.3428\n",
      "Epoch 47/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1291.0134 - val_loss: 1217.9181\n",
      "Epoch 48/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1302.5417 - val_loss: 1200.6737\n",
      "Epoch 49/300\n",
      "19948/19948 [==============================] - 1s 47us/sample - loss: 1295.0495 - val_loss: 1202.0958\n",
      "Epoch 50/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1291.2089 - val_loss: 1210.8872\n",
      "Epoch 51/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1295.0455 - val_loss: 1219.0415\n",
      "Epoch 52/300\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1294.1246 - val_loss: 1191.0502\n",
      "Epoch 53/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1293.1332 - val_loss: 1195.1036\n",
      "Epoch 54/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1275.6439 - val_loss: 1193.8139\n",
      "Epoch 55/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1275.2226 - val_loss: 1194.8589\n",
      "Epoch 56/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1277.8351 - val_loss: 1190.4289\n",
      "Epoch 57/300\n",
      "19948/19948 [==============================] - 1s 43us/sample - loss: 1285.6157 - val_loss: 1187.9438\n",
      "Epoch 58/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1275.9405 - val_loss: 1187.1199\n",
      "Epoch 59/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1268.0747 - val_loss: 1193.8778\n",
      "Epoch 60/300\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1272.3419 - val_loss: 1181.8131\n",
      "Epoch 61/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1261.7149 - val_loss: 1190.9700\n",
      "Epoch 62/300\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1273.8277 - val_loss: 1185.8360\n",
      "Epoch 63/300\n",
      "19948/19948 [==============================] - 1s 42us/sample - loss: 1265.8715 - val_loss: 1173.1310\n",
      "Epoch 64/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1258.8339 - val_loss: 1178.5278\n",
      "Epoch 65/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1264.3854 - val_loss: 1174.2783\n",
      "Epoch 66/300\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1255.4507 - val_loss: 1179.3882\n",
      "Epoch 67/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1256.4088 - val_loss: 1171.5825\n",
      "Epoch 68/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1250.8377 - val_loss: 1174.7699\n",
      "Epoch 69/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1250.0482 - val_loss: 1171.3874\n",
      "Epoch 70/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1250.7393 - val_loss: 1176.9506\n",
      "Epoch 71/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1249.7765 - val_loss: 1171.7146\n",
      "Epoch 72/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1251.0174 - val_loss: 1196.8244\n",
      "Epoch 73/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1239.9494 - val_loss: 1172.2112\n",
      "Epoch 74/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1245.8018 - val_loss: 1170.1588\n",
      "Epoch 75/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1238.1144 - val_loss: 1161.0885\n",
      "Epoch 76/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1238.1763 - val_loss: 1174.1115\n",
      "Epoch 77/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1232.3501 - val_loss: 1154.4918\n",
      "Epoch 78/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1234.3021 - val_loss: 1164.6497\n",
      "Epoch 79/300\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1233.7283 - val_loss: 1169.4706\n",
      "Epoch 80/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1237.1966 - val_loss: 1180.3703\n",
      "Epoch 81/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1232.2437 - val_loss: 1168.2532\n",
      "Epoch 82/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1225.1902 - val_loss: 1167.7574\n",
      "Epoch 83/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1227.1344 - val_loss: 1159.3843\n",
      "Epoch 84/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1219.1465 - val_loss: 1169.9686\n",
      "Epoch 85/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1228.7741 - val_loss: 1159.7140\n",
      "Epoch 86/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1218.5667 - val_loss: 1187.6014\n",
      "Epoch 87/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1219.0398 - val_loss: 1156.4895\n",
      "Epoch 88/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1213.8813 - val_loss: 1164.4555\n",
      "Epoch 89/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1212.7447 - val_loss: 1152.2442\n",
      "Epoch 90/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1218.8645 - val_loss: 1167.9286\n",
      "Epoch 91/300\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1218.8837 - val_loss: 1145.6794\n",
      "Epoch 92/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1214.9631 - val_loss: 1171.1753\n",
      "Epoch 93/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1216.3268 - val_loss: 1172.6612\n",
      "Epoch 94/300\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 1215.9495 - val_loss: 1159.6559\n",
      "Epoch 95/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1210.0152 - val_loss: 1157.8341\n",
      "Epoch 96/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1204.8360 - val_loss: 1148.8068\n",
      "Epoch 97/300\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1199.7222 - val_loss: 1165.9610\n",
      "Epoch 98/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1205.2522 - val_loss: 1158.1272\n",
      "Epoch 99/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1209.1997 - val_loss: 1157.9160\n",
      "Epoch 100/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1208.2140 - val_loss: 1160.0789\n",
      "Epoch 101/300\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1201.0877 - val_loss: 1154.9846\n",
      "Epoch 102/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1204.4560 - val_loss: 1158.1324\n",
      "Epoch 103/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1199.5761 - val_loss: 1145.7031\n",
      "Epoch 104/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1199.2328 - val_loss: 1152.0831\n",
      "Epoch 105/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1196.1470 - val_loss: 1155.4410\n",
      "Epoch 106/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1184.8498 - val_loss: 1144.8933\n",
      "Epoch 107/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1196.2152 - val_loss: 1143.6876\n",
      "Epoch 108/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1195.0152 - val_loss: 1150.8686\n",
      "Epoch 109/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1185.5189 - val_loss: 1153.4429\n",
      "Epoch 110/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1191.0879 - val_loss: 1147.4041\n",
      "Epoch 111/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1195.2069 - val_loss: 1145.8725\n",
      "Epoch 112/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1193.6912 - val_loss: 1145.7029\n",
      "Epoch 113/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1188.2582 - val_loss: 1144.1666\n",
      "Epoch 114/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1182.8571 - val_loss: 1142.7606\n",
      "Epoch 115/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1189.0848 - val_loss: 1153.2428\n",
      "Epoch 116/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1175.5292 - val_loss: 1153.3770\n",
      "Epoch 117/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1191.1428 - val_loss: 1148.1752\n",
      "Epoch 118/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1180.5825 - val_loss: 1143.7849\n",
      "Epoch 119/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1183.4284 - val_loss: 1149.7472\n",
      "Epoch 120/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1179.9635 - val_loss: 1145.6183\n",
      "Epoch 121/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1181.7356 - val_loss: 1151.0911\n",
      "Epoch 122/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1181.9712 - val_loss: 1135.7247\n",
      "Epoch 123/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1176.2412 - val_loss: 1140.8139\n",
      "Epoch 124/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1172.3871 - val_loss: 1142.2736\n",
      "Epoch 125/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1167.9834 - val_loss: 1139.2912\n",
      "Epoch 126/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1168.5286 - val_loss: 1146.7736\n",
      "Epoch 127/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1172.2314 - val_loss: 1140.3955\n",
      "Epoch 128/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1164.5529 - val_loss: 1132.8011\n",
      "Epoch 129/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1171.9788 - val_loss: 1148.7085\n",
      "Epoch 130/300\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1168.4344 - val_loss: 1128.0749\n",
      "Epoch 131/300\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1178.7125 - val_loss: 1139.2022\n",
      "Epoch 132/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1165.3761 - val_loss: 1134.4470\n",
      "Epoch 133/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1154.9562 - val_loss: 1134.7513\n",
      "Epoch 134/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1164.7958 - val_loss: 1143.1155\n",
      "Epoch 135/300\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1164.6866 - val_loss: 1147.2683\n",
      "Epoch 136/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1163.2696 - val_loss: 1138.0122\n",
      "Epoch 137/300\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1162.5115 - val_loss: 1131.1390\n",
      "Epoch 138/300\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1158.1464 - val_loss: 1141.5447\n",
      "Epoch 139/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1169.3380 - val_loss: 1134.4067\n",
      "Epoch 140/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1155.4626 - val_loss: 1130.0409\n",
      "Epoch 141/300\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1161.0721 - val_loss: 1129.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1153.0398 - val_loss: 1147.5899\n",
      "Epoch 143/300\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1157.5704 - val_loss: 1138.6909\n",
      "Epoch 144/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1154.6896 - val_loss: 1133.3848\n",
      "Epoch 145/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1153.9761 - val_loss: 1133.5609\n",
      "Epoch 146/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1158.1799 - val_loss: 1145.1097\n",
      "Epoch 147/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1156.3617 - val_loss: 1138.7427\n",
      "Epoch 148/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1149.2112 - val_loss: 1138.0037\n",
      "Epoch 149/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1154.7119 - val_loss: 1130.6011\n",
      "Epoch 150/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1150.1125 - val_loss: 1145.6358\n",
      "Epoch 151/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1146.9263 - val_loss: 1131.1554\n",
      "Epoch 152/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1141.9800 - val_loss: 1131.1148\n",
      "Epoch 153/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1146.4955 - val_loss: 1134.4819\n",
      "Epoch 154/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1149.7898 - val_loss: 1137.4806\n",
      "Epoch 155/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1143.4051 - val_loss: 1132.8129\n",
      "Epoch 156/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1147.9333 - val_loss: 1128.5408\n",
      "Epoch 157/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1144.1039 - val_loss: 1126.1013\n",
      "Epoch 158/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1150.2513 - val_loss: 1146.0575\n",
      "Epoch 159/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1148.2234 - val_loss: 1129.2547\n",
      "Epoch 160/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1142.4374 - val_loss: 1131.1921\n",
      "Epoch 161/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1142.0032 - val_loss: 1128.2410\n",
      "Epoch 162/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1136.6992 - val_loss: 1124.3989\n",
      "Epoch 163/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1135.0808 - val_loss: 1130.5602\n",
      "Epoch 164/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1133.6498 - val_loss: 1128.3882\n",
      "Epoch 165/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1140.7000 - val_loss: 1125.7929\n",
      "Epoch 166/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1133.3740 - val_loss: 1120.3977\n",
      "Epoch 167/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1133.1187 - val_loss: 1125.3620\n",
      "Epoch 168/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1141.4993 - val_loss: 1124.2505\n",
      "Epoch 169/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1137.9978 - val_loss: 1121.8727\n",
      "Epoch 170/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1135.4238 - val_loss: 1124.4519\n",
      "Epoch 171/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1134.1308 - val_loss: 1125.9832\n",
      "Epoch 172/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1135.6990 - val_loss: 1121.1269\n",
      "Epoch 173/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1135.9948 - val_loss: 1124.8009\n",
      "Epoch 174/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1130.3160 - val_loss: 1120.8022\n",
      "Epoch 175/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1128.8484 - val_loss: 1130.7160\n",
      "Epoch 176/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1134.0266 - val_loss: 1115.3367\n",
      "Epoch 177/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1130.9597 - val_loss: 1125.4309\n",
      "Epoch 178/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1122.9701 - val_loss: 1121.3389\n",
      "Epoch 179/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1128.2003 - val_loss: 1127.8283\n",
      "Epoch 180/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1130.4546 - val_loss: 1113.1478\n",
      "Epoch 181/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1131.4402 - val_loss: 1121.8050\n",
      "Epoch 182/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1131.4530 - val_loss: 1124.7913\n",
      "Epoch 183/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.6064 - val_loss: 1115.3090\n",
      "Epoch 184/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1123.4432 - val_loss: 1111.7763\n",
      "Epoch 185/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1118.2769 - val_loss: 1124.7018\n",
      "Epoch 186/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.1421 - val_loss: 1118.9512\n",
      "Epoch 187/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1127.6719 - val_loss: 1113.0974\n",
      "Epoch 188/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1123.6311 - val_loss: 1126.2509\n",
      "Epoch 189/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1121.5623 - val_loss: 1126.1377\n",
      "Epoch 190/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1125.0570 - val_loss: 1132.9058\n",
      "Epoch 191/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1121.8299 - val_loss: 1118.4342\n",
      "Epoch 192/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1122.4214 - val_loss: 1112.1782\n",
      "Epoch 193/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1109.9774 - val_loss: 1113.2588\n",
      "Epoch 194/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1128.6239 - val_loss: 1113.3104\n",
      "Epoch 195/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1111.2517 - val_loss: 1112.6860\n",
      "Epoch 196/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1106.2124 - val_loss: 1105.0405\n",
      "Epoch 197/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1115.8136 - val_loss: 1111.9421\n",
      "Epoch 198/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1110.9518 - val_loss: 1111.2284\n",
      "Epoch 199/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1101.0581 - val_loss: 1115.5510\n",
      "Epoch 200/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1107.0452 - val_loss: 1114.0956\n",
      "Epoch 201/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1108.7578 - val_loss: 1138.2886\n",
      "Epoch 202/300\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1109.2452 - val_loss: 1117.5993\n",
      "Epoch 203/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1119.9778 - val_loss: 1117.4065\n",
      "Epoch 204/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1112.0274 - val_loss: 1113.5367\n",
      "Epoch 205/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1113.8212 - val_loss: 1118.4536\n",
      "Epoch 206/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1111.4009 - val_loss: 1103.8909\n",
      "Epoch 207/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1113.5056 - val_loss: 1110.8545\n",
      "Epoch 208/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1105.8712 - val_loss: 1107.8369\n",
      "Epoch 209/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1109.0319 - val_loss: 1107.6054\n",
      "Epoch 210/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1104.8217 - val_loss: 1110.1758\n",
      "Epoch 211/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1103.8614 - val_loss: 1113.4534\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1110.6295 - val_loss: 1110.5566\n",
      "Epoch 213/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1103.3080 - val_loss: 1107.1132\n",
      "Epoch 214/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1093.1509 - val_loss: 1112.7981\n",
      "Epoch 215/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1109.7088 - val_loss: 1111.0712\n",
      "Epoch 216/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1101.7280 - val_loss: 1109.9361\n",
      "Epoch 217/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1102.9613 - val_loss: 1108.9369\n",
      "Epoch 218/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1101.7768 - val_loss: 1115.7609\n",
      "Epoch 219/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1103.6902 - val_loss: 1105.0741\n",
      "Epoch 220/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1096.3813 - val_loss: 1105.8277\n",
      "Epoch 221/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1096.4588 - val_loss: 1111.3228\n",
      "Epoch 222/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1107.2529 - val_loss: 1105.1445\n",
      "Epoch 223/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.3007 - val_loss: 1114.3391\n",
      "Epoch 224/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1100.3947 - val_loss: 1116.8199\n",
      "Epoch 225/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1093.8738 - val_loss: 1106.3003\n",
      "Epoch 226/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1094.6491 - val_loss: 1108.2714\n",
      "Epoch 227/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1099.4504 - val_loss: 1110.4927\n",
      "Epoch 228/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1095.4535 - val_loss: 1105.2748\n",
      "Epoch 229/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1095.5608 - val_loss: 1107.6653\n",
      "Epoch 230/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1090.4383 - val_loss: 1102.3519\n",
      "Epoch 231/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1092.5234 - val_loss: 1113.9193\n",
      "Epoch 232/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1092.4532 - val_loss: 1103.5496\n",
      "Epoch 233/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1091.7317 - val_loss: 1109.3094\n",
      "Epoch 234/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1095.7833 - val_loss: 1103.3794\n",
      "Epoch 235/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1085.1810 - val_loss: 1100.1784\n",
      "Epoch 236/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1087.7113 - val_loss: 1109.8636\n",
      "Epoch 237/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1091.1672 - val_loss: 1108.8685\n",
      "Epoch 238/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1089.5733 - val_loss: 1108.7491\n",
      "Epoch 239/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1096.6142 - val_loss: 1102.8681\n",
      "Epoch 240/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1088.0545 - val_loss: 1104.8869\n",
      "Epoch 241/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1083.1111 - val_loss: 1102.0533\n",
      "Epoch 242/300\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1090.2598 - val_loss: 1109.2650\n",
      "Epoch 243/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1082.2830 - val_loss: 1107.5875\n",
      "Epoch 244/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1089.1883 - val_loss: 1108.9817\n",
      "Epoch 245/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1083.7337 - val_loss: 1102.9443\n",
      "Epoch 246/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1085.8599 - val_loss: 1102.9816\n",
      "Epoch 247/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1085.0270 - val_loss: 1103.3773\n",
      "Epoch 248/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1089.9213 - val_loss: 1108.4814\n",
      "Epoch 249/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1083.8959 - val_loss: 1108.3030\n",
      "Epoch 250/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1085.1527 - val_loss: 1112.2537\n",
      "Epoch 251/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1081.9456 - val_loss: 1105.9218\n",
      "Epoch 252/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1087.0914 - val_loss: 1106.2568\n",
      "Epoch 253/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1082.8659 - val_loss: 1125.9788\n",
      "Epoch 254/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1072.5969 - val_loss: 1105.3080\n",
      "Epoch 255/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1084.2482 - val_loss: 1095.5717\n",
      "Epoch 256/300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1076.2176 - val_loss: 1103.5039\n",
      "Epoch 257/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1082.9913 - val_loss: 1103.8988\n",
      "Epoch 258/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.0325 - val_loss: 1098.5662\n",
      "Epoch 259/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.2467 - val_loss: 1113.7661\n",
      "Epoch 260/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1084.1996 - val_loss: 1098.5031\n",
      "Epoch 261/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1073.7107 - val_loss: 1098.7751\n",
      "Epoch 262/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1076.2894 - val_loss: 1103.1014\n",
      "Epoch 263/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1071.7752 - val_loss: 1100.4969\n",
      "Epoch 264/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1074.5731 - val_loss: 1095.8174\n",
      "Epoch 265/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1082.6624 - val_loss: 1090.7217\n",
      "Epoch 266/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1074.6369 - val_loss: 1109.6747\n",
      "Epoch 267/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1079.1199 - val_loss: 1095.9834\n",
      "Epoch 268/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1078.7812 - val_loss: 1101.9054\n",
      "Epoch 269/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1066.7169 - val_loss: 1092.5647\n",
      "Epoch 270/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1073.6780 - val_loss: 1097.4320\n",
      "Epoch 271/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1072.9637 - val_loss: 1095.6860\n",
      "Epoch 272/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1070.7216 - val_loss: 1103.8743\n",
      "Epoch 273/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1071.9261 - val_loss: 1101.8036\n",
      "Epoch 274/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1068.4597 - val_loss: 1101.5210\n",
      "Epoch 275/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1067.4014 - val_loss: 1105.5170\n",
      "Epoch 276/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.7487 - val_loss: 1106.7484\n",
      "Epoch 277/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1080.2301 - val_loss: 1100.1836\n",
      "Epoch 278/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1062.2452 - val_loss: 1097.4667\n",
      "Epoch 279/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1066.8834 - val_loss: 1093.9870\n",
      "Epoch 280/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1065.6629 - val_loss: 1099.8835\n",
      "Epoch 281/300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1071.7649 - val_loss: 1107.2882\n",
      "Epoch 282/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1075.0486 - val_loss: 1098.3604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1068.2543 - val_loss: 1094.8821\n",
      "Epoch 284/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1064.3840 - val_loss: 1103.7309\n",
      "Epoch 285/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1060.6612 - val_loss: 1095.4342\n",
      "Epoch 286/300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1066.0253 - val_loss: 1098.9363\n",
      "Epoch 287/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1068.9224 - val_loss: 1101.9530\n",
      "Epoch 288/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1070.9516 - val_loss: 1098.0746\n",
      "Epoch 289/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1063.8086 - val_loss: 1097.1329\n",
      "Epoch 290/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1065.2417 - val_loss: 1091.4558\n",
      "Epoch 291/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1058.8776 - val_loss: 1095.3657\n",
      "Epoch 292/300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1059.7113 - val_loss: 1092.5060\n",
      "Epoch 293/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1059.3233 - val_loss: 1093.7635\n",
      "Epoch 294/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1064.6379 - val_loss: 1097.4691\n",
      "Epoch 295/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1067.4015 - val_loss: 1096.4290\n",
      "Epoch 296/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1059.7708 - val_loss: 1098.5815\n",
      "Epoch 297/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1056.9315 - val_loss: 1106.4000\n",
      "Epoch 298/300\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1061.1916 - val_loss: 1096.5384\n",
      "Epoch 299/300\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1060.9362 - val_loss: 1101.5605\n",
      "Epoch 300/300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1058.9252 - val_loss: 1097.4319\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=300, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          10041.659454033612,
          9717.106315874398,
          9335.33711145071,
          8891.379280073503,
          8380.435465511111,
          7797.68672807111,
          7140.089042447865,
          6403.030770236991,
          5583.170647002519,
          4695.796426372632,
          3752.791728562622,
          2815.284458194822,
          2048.3030578478656,
          1623.6189332254096,
          1499.4757944630649,
          1462.7882018883863,
          1440.9266239684116,
          1422.198657892356,
          1394.6922667881806,
          1405.8010963514905,
          1398.087326536299,
          1375.542514345893,
          1368.3589135463142,
          1367.4619897965179,
          1357.3058142087395,
          1346.9740932870777,
          1347.6920357431275,
          1346.7002153352623,
          1325.6698398502122,
          1331.1701351737172,
          1333.7427283348816,
          1328.6310085421314,
          1314.8409450508823,
          1320.199312597519,
          1317.243942967536,
          1304.8097774428356,
          1291.0133717783424,
          1302.5417182575086,
          1295.0495216762802,
          1291.2088687423238,
          1295.045452404768,
          1294.124610388377,
          1293.1331858829049,
          1275.6439442012122,
          1275.2225957166368,
          1277.8351191613822,
          1285.6156881426866,
          1275.9405308048974,
          1268.0746664227381,
          1272.341914514848,
          1261.7148729763792,
          1273.8276724561986,
          1265.8714661782744,
          1258.8338943215251,
          1264.3853928172232,
          1255.4507139705813,
          1256.4088246628735,
          1250.8376702277485,
          1250.048152785132,
          1250.7392837588668,
          1249.7765468293735,
          1251.0174072951002,
          1239.9493796664296,
          1245.801808334482,
          1238.1144220578974,
          1238.1763101152842,
          1232.3500909738366,
          1234.3021233380618,
          1233.7282961089459,
          1237.196594837985,
          1232.2436785593716,
          1225.1901608978264,
          1227.1344098905201,
          1219.1464663104541,
          1228.7740562229374,
          1218.566695943633,
          1219.0397659157952,
          1213.881277821359,
          1212.7446736270163,
          1218.8644724085357,
          1218.8836757062504,
          1214.9630633396832,
          1216.3267987245354,
          1215.9494994358379,
          1210.0152127220226,
          1204.8359868960079,
          1199.7221583000567,
          1205.2521892614157,
          1209.1997281310314,
          1208.214024163019,
          1201.0877196727115,
          1204.4559869087361,
          1199.576079667487,
          1199.2328306526656,
          1196.1469679565307,
          1184.849805294547,
          1196.2151508228033,
          1195.0151877792416,
          1185.5189417044865,
          1191.087906462075,
          1195.2069361051624,
          1193.691178998991,
          1188.2581574887677,
          1182.8570861388046,
          1189.0848066300507,
          1175.5292127451296,
          1191.1427993055381,
          1180.5825110374865,
          1183.428424964478,
          1179.9635023228363,
          1181.7356274262302,
          1181.9712481945246,
          1176.2412345829625,
          1172.3871075293812,
          1167.9834308704585,
          1168.5286276300803,
          1172.2314163798533,
          1164.5528507909628,
          1171.9787572444213,
          1168.4344490499518,
          1178.7124628771132,
          1165.3761226454405,
          1154.9561760724368,
          1164.7957566626353,
          1164.686638017635,
          1163.269589653803,
          1162.511534922439,
          1158.146405018282,
          1169.3380309067247,
          1155.4625699425928,
          1161.0721326131536,
          1153.0397704441705,
          1157.5704011092905,
          1154.6896303435808,
          1153.9761150965167,
          1158.1799391063248,
          1156.3617185933426,
          1149.2111776997147,
          1154.7118571273593,
          1150.1125495135007,
          1146.926348055726,
          1141.9799598585228,
          1146.4955371514766,
          1149.7898009678681,
          1143.4051022159567,
          1147.9333037871122,
          1144.1039071948394,
          1150.2513157745293,
          1148.2234033095424,
          1142.4373746496751,
          1142.003225353693,
          1136.6991824985196,
          1135.0808238382685,
          1133.6498041391994,
          1140.6999665193957,
          1133.3739679710216,
          1133.1186724154677,
          1141.499322922215,
          1137.9978001397383,
          1135.4237842364757,
          1134.130775538862,
          1135.6989606815612,
          1135.9948294520832,
          1130.3160423889221,
          1128.848373275399,
          1134.0265974247889,
          1130.959721594505,
          1122.9700897137245,
          1128.2002785734119,
          1130.454611139353,
          1131.4401692447238,
          1131.4529649892454,
          1123.6064146908916,
          1123.4431883737561,
          1118.2769323776977,
          1123.142141956783,
          1127.6718682196758,
          1123.6310599697886,
          1121.5622510372673,
          1125.0569592023244,
          1121.8299264366651,
          1122.4213625103002,
          1109.9774443582612,
          1128.6239041821236,
          1111.2517327522262,
          1106.2124483618345,
          1115.8136393147574,
          1110.951842086789,
          1101.0580875265925,
          1107.0451634209883,
          1108.7577578168082,
          1109.245161271846,
          1119.9777571588475,
          1112.0274188720753,
          1113.8211541169148,
          1111.4008666673976,
          1113.5056069853886,
          1105.871175028218,
          1109.0318914872028,
          1104.8217107888713,
          1103.8614174441439,
          1110.6294891620557,
          1103.30800853704,
          1093.1509073493426,
          1109.7088116847945,
          1101.7280035269434,
          1102.9612530910445,
          1101.7768144196395,
          1103.6901643736605,
          1096.3813124817887,
          1096.4587534523355,
          1107.2529225155326,
          1100.300711072421,
          1100.394663135872,
          1093.873769677137,
          1094.6490589644404,
          1099.450362238206,
          1095.453524402705,
          1095.5608053840376,
          1090.438282209526,
          1092.5234037452456,
          1092.4531585834106,
          1091.731686466845,
          1095.7832945492264,
          1085.1810020751218,
          1087.7113248351966,
          1091.1672296946672,
          1089.5733492529796,
          1096.6141722751615,
          1088.054500686159,
          1083.1111197218236,
          1090.2598287285223,
          1082.2829974388487,
          1089.1882674494702,
          1083.7337191688625,
          1085.8598710896376,
          1085.0269520088166,
          1089.9212558119862,
          1083.8959084146124,
          1085.1526695482473,
          1081.945576443088,
          1087.0913687599477,
          1082.8658967172069,
          1072.596869964936,
          1084.2481683823253,
          1076.2175698319536,
          1082.991313425649,
          1078.032544503404,
          1078.2467081893392,
          1084.1995974690444,
          1073.7107318628043,
          1076.2893539560278,
          1071.7752465981866,
          1074.5731261387027,
          1082.662382423794,
          1074.6369378009779,
          1079.1199251109526,
          1078.781193921579,
          1066.716948073781,
          1073.6780061264758,
          1072.9637459249518,
          1070.7216229848975,
          1071.9260622050926,
          1068.4596616486067,
          1067.4014155309665,
          1071.748658181193,
          1080.2300525800676,
          1062.2451786265383,
          1066.883437048631,
          1065.662865160844,
          1071.764896396647,
          1075.048597569501,
          1068.2542645179224,
          1064.3839721802076,
          1060.661158342746,
          1066.0252848568387,
          1068.922402543488,
          1070.951553457936,
          1063.8085836651858,
          1065.2416861966112,
          1058.8776398714783,
          1059.711344172583,
          1059.3232525905244,
          1064.6378907669707,
          1067.401483909434,
          1059.7708343695563,
          1056.9315358604247,
          1061.1915720007762,
          1060.9362138312542,
          1058.9251678142468
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          9527.65541932462,
          9155.419991783325,
          8756.463653937739,
          8203.826432317775,
          7906.1880819819025,
          7102.2482063717225,
          6202.305462268304,
          5620.897593312926,
          4502.009082109578,
          3959.26843504048,
          2594.8045889038062,
          1673.932429786194,
          2137.1451816226095,
          2925.280211704729,
          2669.542578722256,
          2731.624821949177,
          2506.516908170227,
          2311.0731230300344,
          1789.9102408204692,
          1547.9654858491454,
          1517.1572149845458,
          1439.5121871798317,
          1347.1263759896826,
          1331.1920493282535,
          1279.2249620742446,
          1278.2877071939777,
          1231.3829707973152,
          1275.131225120861,
          1241.846996233175,
          1267.6022730143295,
          1273.7903844918665,
          1245.258437562663,
          1214.1612046545629,
          1222.8220874517888,
          1211.2649805999506,
          1221.34277524885,
          1217.9181309392702,
          1200.673675775767,
          1202.0958456341957,
          1210.8872213017517,
          1219.041481386762,
          1191.0502349076348,
          1195.1036450386239,
          1193.8138641179958,
          1194.8589107803336,
          1190.4289426679288,
          1187.9437618129402,
          1187.1198894224594,
          1193.8778336859427,
          1181.8130845765397,
          1190.9700398526404,
          1185.8360053032416,
          1173.130992337891,
          1178.5278194741875,
          1174.2782926771715,
          1179.3882358291714,
          1171.58254768061,
          1174.7698836799898,
          1171.3873734062076,
          1176.9506179445464,
          1171.714642274014,
          1196.824438706653,
          1172.211204845489,
          1170.1587828059987,
          1161.0884947592256,
          1174.111544189698,
          1154.4917992591284,
          1164.6497227263542,
          1169.4705647525363,
          1180.3703162891486,
          1168.2531690060173,
          1167.7574030614362,
          1159.3843048811675,
          1169.9686140997844,
          1159.7140477595262,
          1187.6013670944847,
          1156.4894734365992,
          1164.4554904519327,
          1152.244160598807,
          1167.9286471241244,
          1145.6793689961087,
          1171.1752793591463,
          1172.6611635075415,
          1159.6559383009105,
          1157.8340508564454,
          1148.806762230236,
          1165.9610181540365,
          1158.1271580523423,
          1157.9159900702764,
          1160.0789037826473,
          1154.9845646042993,
          1158.1323669078668,
          1145.703107106798,
          1152.0830690067614,
          1155.44097779226,
          1144.8932529263586,
          1143.6876349700628,
          1150.8686284535104,
          1153.442877629297,
          1147.404071043698,
          1145.8724532661874,
          1145.702907197385,
          1144.1665775351853,
          1142.7605811242042,
          1153.2428120731088,
          1153.3770023986192,
          1148.1751633837823,
          1143.7849142878906,
          1149.7471721642285,
          1145.6183140373957,
          1151.0910542703998,
          1135.7247033693855,
          1140.8138818398538,
          1142.273580743527,
          1139.2912055475094,
          1146.7735810127817,
          1140.395503749201,
          1132.8010607364304,
          1148.7084687276763,
          1128.0748748503922,
          1139.20222233569,
          1134.4470031260967,
          1134.7512985422254,
          1143.1154732284408,
          1147.2682725819552,
          1138.0121649051205,
          1131.1390005371388,
          1141.544738610426,
          1134.406652046297,
          1130.0408503270612,
          1129.674047910309,
          1147.5898882259892,
          1138.69092575266,
          1133.384846401425,
          1133.5609363397568,
          1145.109705620355,
          1138.7426503978704,
          1138.0037130474782,
          1130.6010522622491,
          1145.6357721237327,
          1131.1553995466338,
          1131.1148399167914,
          1134.4818701964953,
          1137.480554740176,
          1132.8129455431779,
          1128.5407526610202,
          1126.1012740057745,
          1146.057489781048,
          1129.2546541417842,
          1131.1920593396346,
          1128.2409549251884,
          1124.3989020623544,
          1130.5602421520564,
          1128.3882000182898,
          1125.7928520647824,
          1120.3977194710153,
          1125.3619948976714,
          1124.2505184622596,
          1121.872733144261,
          1124.4519171966253,
          1125.9832485605152,
          1121.1268926405921,
          1124.8008751464745,
          1120.802213538403,
          1130.715983603267,
          1115.3366877171661,
          1125.430942667733,
          1121.3388598686663,
          1127.8283372216983,
          1113.1477637511828,
          1121.8049733506334,
          1124.7913382411457,
          1115.309033394051,
          1111.7763162983522,
          1124.7018092009928,
          1118.9511508731296,
          1113.0973839844926,
          1126.2509127980798,
          1126.1377365819098,
          1132.9057886931803,
          1118.4342190084847,
          1112.178223635358,
          1113.258809770638,
          1113.3103970714483,
          1112.6859501207045,
          1105.0404541309358,
          1111.9421275834748,
          1111.2284235398756,
          1115.5510338843885,
          1114.0955738827201,
          1138.2885954898752,
          1117.5992798316638,
          1117.406518838629,
          1113.5367433354065,
          1118.453587188017,
          1103.890934912217,
          1110.8545324778017,
          1107.8368751919052,
          1107.6054210919092,
          1110.1758162041622,
          1113.453435328338,
          1110.5565886343556,
          1107.1131772931888,
          1112.7980981508954,
          1111.0712204514941,
          1109.9361432987487,
          1108.936934907713,
          1115.7608857737148,
          1105.0741120761636,
          1105.827746648121,
          1111.322750748626,
          1105.1445338691144,
          1114.3391305676007,
          1116.819897211265,
          1106.3003495367252,
          1108.2713909436018,
          1110.4927251283023,
          1105.2748322640819,
          1107.665280877743,
          1102.3519103526592,
          1113.919254490386,
          1103.5495553820792,
          1109.309350184503,
          1103.3793699556347,
          1100.1783685188914,
          1109.8635535155074,
          1108.8684791395128,
          1108.7490802502523,
          1102.8680671552677,
          1104.8868744037231,
          1102.0532900188928,
          1109.2650294574487,
          1107.5875354290295,
          1108.981709745142,
          1102.944347295155,
          1102.9816189818137,
          1103.377339603477,
          1108.4814068580263,
          1108.3030100918638,
          1112.2537117012043,
          1105.9218324087942,
          1106.2568178239987,
          1125.9787753579228,
          1105.3080417288074,
          1095.5717395256966,
          1103.5038883812756,
          1103.898837637036,
          1098.5661928533718,
          1113.7661027313593,
          1098.5030872994396,
          1098.7751368156817,
          1103.1013587720652,
          1100.4969193095408,
          1095.8173901313337,
          1090.721654255165,
          1109.6747279988517,
          1095.9833896255263,
          1101.9054386130501,
          1092.564687156333,
          1097.4320262957126,
          1095.6859511242903,
          1103.874343703786,
          1101.8035661029223,
          1101.5210472521526,
          1105.5170398847551,
          1106.748404592174,
          1100.1835563725451,
          1097.466685525922,
          1093.9869740181896,
          1099.8834790523722,
          1107.288217089041,
          1098.3604108475004,
          1094.8821110913766,
          1103.7309356866917,
          1095.4341930865955,
          1098.9363065650377,
          1101.9530480910523,
          1098.0745896606568,
          1097.1329043648252,
          1091.455802665054,
          1095.3657483823174,
          1092.5059708219887,
          1093.7635320583518,
          1097.4691146443645,
          1096.4289819301669,
          1098.5815041226329,
          1106.3999963870908,
          1096.5383864258008,
          1101.5604898174706,
          1097.4319312732637
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299
         ],
         "y": [
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          299
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"33a99c4d-6c1b-4bc5-aa67-8cf21d3d9071\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"33a99c4d-6c1b-4bc5-aa67-8cf21d3d9071\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '33a99c4d-6c1b-4bc5-aa67-8cf21d3d9071',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [10041.659454033612, 9717.106315874398, 9335.33711145071, 8891.379280073503, 8380.435465511111, 7797.68672807111, 7140.089042447865, 6403.030770236991, 5583.170647002519, 4695.796426372632, 3752.791728562622, 2815.284458194822, 2048.3030578478656, 1623.6189332254096, 1499.4757944630649, 1462.7882018883863, 1440.9266239684116, 1422.198657892356, 1394.6922667881806, 1405.8010963514905, 1398.087326536299, 1375.542514345893, 1368.3589135463142, 1367.4619897965179, 1357.3058142087395, 1346.9740932870777, 1347.6920357431275, 1346.7002153352623, 1325.6698398502122, 1331.1701351737172, 1333.7427283348816, 1328.6310085421314, 1314.8409450508823, 1320.199312597519, 1317.243942967536, 1304.8097774428356, 1291.0133717783424, 1302.5417182575086, 1295.0495216762802, 1291.2088687423238, 1295.045452404768, 1294.124610388377, 1293.1331858829049, 1275.6439442012122, 1275.2225957166368, 1277.8351191613822, 1285.6156881426866, 1275.9405308048974, 1268.0746664227381, 1272.341914514848, 1261.7148729763792, 1273.8276724561986, 1265.8714661782744, 1258.8338943215251, 1264.3853928172232, 1255.4507139705813, 1256.4088246628735, 1250.8376702277485, 1250.048152785132, 1250.7392837588668, 1249.7765468293735, 1251.0174072951002, 1239.9493796664296, 1245.801808334482, 1238.1144220578974, 1238.1763101152842, 1232.3500909738366, 1234.3021233380618, 1233.7282961089459, 1237.196594837985, 1232.2436785593716, 1225.1901608978264, 1227.1344098905201, 1219.1464663104541, 1228.7740562229374, 1218.566695943633, 1219.0397659157952, 1213.881277821359, 1212.7446736270163, 1218.8644724085357, 1218.8836757062504, 1214.9630633396832, 1216.3267987245354, 1215.9494994358379, 1210.0152127220226, 1204.8359868960079, 1199.7221583000567, 1205.2521892614157, 1209.1997281310314, 1208.214024163019, 1201.0877196727115, 1204.4559869087361, 1199.576079667487, 1199.2328306526656, 1196.1469679565307, 1184.849805294547, 1196.2151508228033, 1195.0151877792416, 1185.5189417044865, 1191.087906462075, 1195.2069361051624, 1193.691178998991, 1188.2581574887677, 1182.8570861388046, 1189.0848066300507, 1175.5292127451296, 1191.1427993055381, 1180.5825110374865, 1183.428424964478, 1179.9635023228363, 1181.7356274262302, 1181.9712481945246, 1176.2412345829625, 1172.3871075293812, 1167.9834308704585, 1168.5286276300803, 1172.2314163798533, 1164.5528507909628, 1171.9787572444213, 1168.4344490499518, 1178.7124628771132, 1165.3761226454405, 1154.9561760724368, 1164.7957566626353, 1164.686638017635, 1163.269589653803, 1162.511534922439, 1158.146405018282, 1169.3380309067247, 1155.4625699425928, 1161.0721326131536, 1153.0397704441705, 1157.5704011092905, 1154.6896303435808, 1153.9761150965167, 1158.1799391063248, 1156.3617185933426, 1149.2111776997147, 1154.7118571273593, 1150.1125495135007, 1146.926348055726, 1141.9799598585228, 1146.4955371514766, 1149.7898009678681, 1143.4051022159567, 1147.9333037871122, 1144.1039071948394, 1150.2513157745293, 1148.2234033095424, 1142.4373746496751, 1142.003225353693, 1136.6991824985196, 1135.0808238382685, 1133.6498041391994, 1140.6999665193957, 1133.3739679710216, 1133.1186724154677, 1141.499322922215, 1137.9978001397383, 1135.4237842364757, 1134.130775538862, 1135.6989606815612, 1135.9948294520832, 1130.3160423889221, 1128.848373275399, 1134.0265974247889, 1130.959721594505, 1122.9700897137245, 1128.2002785734119, 1130.454611139353, 1131.4401692447238, 1131.4529649892454, 1123.6064146908916, 1123.4431883737561, 1118.2769323776977, 1123.142141956783, 1127.6718682196758, 1123.6310599697886, 1121.5622510372673, 1125.0569592023244, 1121.8299264366651, 1122.4213625103002, 1109.9774443582612, 1128.6239041821236, 1111.2517327522262, 1106.2124483618345, 1115.8136393147574, 1110.951842086789, 1101.0580875265925, 1107.0451634209883, 1108.7577578168082, 1109.245161271846, 1119.9777571588475, 1112.0274188720753, 1113.8211541169148, 1111.4008666673976, 1113.5056069853886, 1105.871175028218, 1109.0318914872028, 1104.8217107888713, 1103.8614174441439, 1110.6294891620557, 1103.30800853704, 1093.1509073493426, 1109.7088116847945, 1101.7280035269434, 1102.9612530910445, 1101.7768144196395, 1103.6901643736605, 1096.3813124817887, 1096.4587534523355, 1107.2529225155326, 1100.300711072421, 1100.394663135872, 1093.873769677137, 1094.6490589644404, 1099.450362238206, 1095.453524402705, 1095.5608053840376, 1090.438282209526, 1092.5234037452456, 1092.4531585834106, 1091.731686466845, 1095.7832945492264, 1085.1810020751218, 1087.7113248351966, 1091.1672296946672, 1089.5733492529796, 1096.6141722751615, 1088.054500686159, 1083.1111197218236, 1090.2598287285223, 1082.2829974388487, 1089.1882674494702, 1083.7337191688625, 1085.8598710896376, 1085.0269520088166, 1089.9212558119862, 1083.8959084146124, 1085.1526695482473, 1081.945576443088, 1087.0913687599477, 1082.8658967172069, 1072.596869964936, 1084.2481683823253, 1076.2175698319536, 1082.991313425649, 1078.032544503404, 1078.2467081893392, 1084.1995974690444, 1073.7107318628043, 1076.2893539560278, 1071.7752465981866, 1074.5731261387027, 1082.662382423794, 1074.6369378009779, 1079.1199251109526, 1078.781193921579, 1066.716948073781, 1073.6780061264758, 1072.9637459249518, 1070.7216229848975, 1071.9260622050926, 1068.4596616486067, 1067.4014155309665, 1071.748658181193, 1080.2300525800676, 1062.2451786265383, 1066.883437048631, 1065.662865160844, 1071.764896396647, 1075.048597569501, 1068.2542645179224, 1064.3839721802076, 1060.661158342746, 1066.0252848568387, 1068.922402543488, 1070.951553457936, 1063.8085836651858, 1065.2416861966112, 1058.8776398714783, 1059.711344172583, 1059.3232525905244, 1064.6378907669707, 1067.401483909434, 1059.7708343695563, 1056.9315358604247, 1061.1915720007762, 1060.9362138312542, 1058.9251678142468]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [9527.65541932462, 9155.419991783325, 8756.463653937739, 8203.826432317775, 7906.1880819819025, 7102.2482063717225, 6202.305462268304, 5620.897593312926, 4502.009082109578, 3959.26843504048, 2594.8045889038062, 1673.932429786194, 2137.1451816226095, 2925.280211704729, 2669.542578722256, 2731.624821949177, 2506.516908170227, 2311.0731230300344, 1789.9102408204692, 1547.9654858491454, 1517.1572149845458, 1439.5121871798317, 1347.1263759896826, 1331.1920493282535, 1279.2249620742446, 1278.2877071939777, 1231.3829707973152, 1275.131225120861, 1241.846996233175, 1267.6022730143295, 1273.7903844918665, 1245.258437562663, 1214.1612046545629, 1222.8220874517888, 1211.2649805999506, 1221.34277524885, 1217.9181309392702, 1200.673675775767, 1202.0958456341957, 1210.8872213017517, 1219.041481386762, 1191.0502349076348, 1195.1036450386239, 1193.8138641179958, 1194.8589107803336, 1190.4289426679288, 1187.9437618129402, 1187.1198894224594, 1193.8778336859427, 1181.8130845765397, 1190.9700398526404, 1185.8360053032416, 1173.130992337891, 1178.5278194741875, 1174.2782926771715, 1179.3882358291714, 1171.58254768061, 1174.7698836799898, 1171.3873734062076, 1176.9506179445464, 1171.714642274014, 1196.824438706653, 1172.211204845489, 1170.1587828059987, 1161.0884947592256, 1174.111544189698, 1154.4917992591284, 1164.6497227263542, 1169.4705647525363, 1180.3703162891486, 1168.2531690060173, 1167.7574030614362, 1159.3843048811675, 1169.9686140997844, 1159.7140477595262, 1187.6013670944847, 1156.4894734365992, 1164.4554904519327, 1152.244160598807, 1167.9286471241244, 1145.6793689961087, 1171.1752793591463, 1172.6611635075415, 1159.6559383009105, 1157.8340508564454, 1148.806762230236, 1165.9610181540365, 1158.1271580523423, 1157.9159900702764, 1160.0789037826473, 1154.9845646042993, 1158.1323669078668, 1145.703107106798, 1152.0830690067614, 1155.44097779226, 1144.8932529263586, 1143.6876349700628, 1150.8686284535104, 1153.442877629297, 1147.404071043698, 1145.8724532661874, 1145.702907197385, 1144.1665775351853, 1142.7605811242042, 1153.2428120731088, 1153.3770023986192, 1148.1751633837823, 1143.7849142878906, 1149.7471721642285, 1145.6183140373957, 1151.0910542703998, 1135.7247033693855, 1140.8138818398538, 1142.273580743527, 1139.2912055475094, 1146.7735810127817, 1140.395503749201, 1132.8010607364304, 1148.7084687276763, 1128.0748748503922, 1139.20222233569, 1134.4470031260967, 1134.7512985422254, 1143.1154732284408, 1147.2682725819552, 1138.0121649051205, 1131.1390005371388, 1141.544738610426, 1134.406652046297, 1130.0408503270612, 1129.674047910309, 1147.5898882259892, 1138.69092575266, 1133.384846401425, 1133.5609363397568, 1145.109705620355, 1138.7426503978704, 1138.0037130474782, 1130.6010522622491, 1145.6357721237327, 1131.1553995466338, 1131.1148399167914, 1134.4818701964953, 1137.480554740176, 1132.8129455431779, 1128.5407526610202, 1126.1012740057745, 1146.057489781048, 1129.2546541417842, 1131.1920593396346, 1128.2409549251884, 1124.3989020623544, 1130.5602421520564, 1128.3882000182898, 1125.7928520647824, 1120.3977194710153, 1125.3619948976714, 1124.2505184622596, 1121.872733144261, 1124.4519171966253, 1125.9832485605152, 1121.1268926405921, 1124.8008751464745, 1120.802213538403, 1130.715983603267, 1115.3366877171661, 1125.430942667733, 1121.3388598686663, 1127.8283372216983, 1113.1477637511828, 1121.8049733506334, 1124.7913382411457, 1115.309033394051, 1111.7763162983522, 1124.7018092009928, 1118.9511508731296, 1113.0973839844926, 1126.2509127980798, 1126.1377365819098, 1132.9057886931803, 1118.4342190084847, 1112.178223635358, 1113.258809770638, 1113.3103970714483, 1112.6859501207045, 1105.0404541309358, 1111.9421275834748, 1111.2284235398756, 1115.5510338843885, 1114.0955738827201, 1138.2885954898752, 1117.5992798316638, 1117.406518838629, 1113.5367433354065, 1118.453587188017, 1103.890934912217, 1110.8545324778017, 1107.8368751919052, 1107.6054210919092, 1110.1758162041622, 1113.453435328338, 1110.5565886343556, 1107.1131772931888, 1112.7980981508954, 1111.0712204514941, 1109.9361432987487, 1108.936934907713, 1115.7608857737148, 1105.0741120761636, 1105.827746648121, 1111.322750748626, 1105.1445338691144, 1114.3391305676007, 1116.819897211265, 1106.3003495367252, 1108.2713909436018, 1110.4927251283023, 1105.2748322640819, 1107.665280877743, 1102.3519103526592, 1113.919254490386, 1103.5495553820792, 1109.309350184503, 1103.3793699556347, 1100.1783685188914, 1109.8635535155074, 1108.8684791395128, 1108.7490802502523, 1102.8680671552677, 1104.8868744037231, 1102.0532900188928, 1109.2650294574487, 1107.5875354290295, 1108.981709745142, 1102.944347295155, 1102.9816189818137, 1103.377339603477, 1108.4814068580263, 1108.3030100918638, 1112.2537117012043, 1105.9218324087942, 1106.2568178239987, 1125.9787753579228, 1105.3080417288074, 1095.5717395256966, 1103.5038883812756, 1103.898837637036, 1098.5661928533718, 1113.7661027313593, 1098.5030872994396, 1098.7751368156817, 1103.1013587720652, 1100.4969193095408, 1095.8173901313337, 1090.721654255165, 1109.6747279988517, 1095.9833896255263, 1101.9054386130501, 1092.564687156333, 1097.4320262957126, 1095.6859511242903, 1103.874343703786, 1101.8035661029223, 1101.5210472521526, 1105.5170398847551, 1106.748404592174, 1100.1835563725451, 1097.466685525922, 1093.9869740181896, 1099.8834790523722, 1107.288217089041, 1098.3604108475004, 1094.8821110913766, 1103.7309356866917, 1095.4341930865955, 1098.9363065650377, 1101.9530480910523, 1098.0745896606568, 1097.1329043648252, 1091.455802665054, 1095.3657483823174, 1092.5059708219887, 1093.7635320583518, 1097.4691146443645, 1096.4289819301669, 1098.5815041226329, 1106.3999963870908, 1096.5383864258008, 1101.5604898174706, 1097.4319312732637]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299], \"y\": [1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 299], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('33a99c4d-6c1b-4bc5-aa67-8cf21d3d9071');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.53% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1097   : Mean absolute error \n",
      "\n",
      "9.50% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Learning_rate_decay\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "decay = 5e-4\n",
    "n_epochs=400\n",
    "n_steps_per_epoch = len(X_train) // 1024\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          0.005,
          0.004952947003467063,
          0.004906771344455349,
          0.004861448711716091,
          0.004816955684007707,
          0.0047732696897374695,
          0.004730368968779565,
          0.0046882325363338025,
          0.004646840148698885,
          0.00460617227084293,
          0.004566210045662101,
          0.004526935264825713,
          0.004488330341113105,
          0.004450378282153984,
          0.0044130626654898504,
          0.00437636761487965,
          0.004340277777777778,
          0.004304778303917348,
          0.004269854824935952,
          0.004235493434985176,
          0.004201680672268908,
          0.004168403501458941,
          0.0041356492969396195,
          0.004103405826836275,
          0.004071661237785016,
          0.00404040404040404,
          0.00400962309542903,
          0.003979307600477517,
          0.0039494470774091624,
          0.003920031360250882,
          0.003891050583657588,
          0.003862495171881035,
          0.003834355828220859,
          0.0038066235249333844,
          0.003779289493575208,
          0.00375234521575985,
          0.0037257824143070045,
          0.003699593044765076,
          0.0036737692872887582,
          0.0036483035388544327,
          0.003623188405797102,
          0.003598416696653473,
          0.0035739814152966403,
          0.0035498757543485976,
          0.003526093088857546,
          0.0035026269702276708,
          0.003479471120389701,
          0.0034566194262011757,
          0.0034340659340659344,
          0.0034118048447628795,
          0.003389830508474576,
          0.003368137420006736,
          0.0033467202141900937,
          0.003325573661456602,
          0.0033046926635822873,
          0.0032840722495894913,
          0.0032637075718015664,
          0.003243593902043464,
          0.003223726627981947,
          0.0032041012495994873,
          0.003184713375796179,
          0.003165558721114277,
          0.0031466331025802393,
          0.0031279324366593683,
          0.0031094527363184077,
          0.0030911901081916537,
          0.003073140749846343,
          0.0030553009471432934,
          0.003037667071688943,
          0.0030202355783751134,
          0.003003003003003003,
          0.002985965959988056,
          0.0029691211401425177,
          0.002952465308532625,
          0.0029359953024075164,
          0.0029197080291970805,
          0.0029036004645760743,
          0.0028876696505919725,
          0.002871912693854107,
          0.0028563267637817763,
          0.002840909090909091,
          0.0028256569652444193,
          0.002810567734682406,
          0.0027956388034665923,
          0.0027808676307007787,
          0.0027662517289073303,
          0.0027517886626307097,
          0.0027374760470845875,
          0.0027233115468409588,
          0.00270929287455974,
          0.0026954177897574125,
          0.0026816840976133013,
          0.0026680896478121665,
          0.002654632333421821,
          0.002641310089804543,
          0.002628120893561104,
          0.0026150627615062765,
          0.0026021337496747333,
          0.002589331952356292,
          0.0025766555011594947,
          0.002564102564102564,
          0.0025516713447307987,
          0.0025393600812595226,
          0.0025271670457417236,
          0.0025150905432595573,
          0.0025031289111389237,
          0.0024912805181863482,
          0.0024795437639474342,
          0.00246791707798618,
          0.0024563989191844758,
          0.0024449877750611247,
          0.002433682161109759,
          0.0024224806201550387,
          0.0024113817217265494,
          0.002400384061449832,
          0.0023894862604540022,
          0.0023786869647954324,
          0.0023679848448969923,
          0.0023573785950023575,
          0.002346866932644919,
          0.0023364485981308414,
          0.0023261223540358227,
          0.0023158869847151463,
          0.0023057412958266085,
          0.002295684113865932,
          0.002285714285714286,
          0.002275830678197542,
          0.0022660321776569224,
          0.002256317689530686,
          0.0022466861379465287,
          0.0022371364653243843,
          0.0022276676319893073,
          0.0022182786157941437,
          0.002208968411751712,
          0.002199736031676199,
          0.002190580503833516,
          0.0021815008726003495,
          0.0021724961981316533,
          0.002163565556036348,
          0.0021547080370609784,
          0.002145922746781116,
          0.0021372088053002777,
          0.0021285653469561515,
          0.0021199915200339195,
          0.002111486486486486,
          0.002103049421661409,
          0.0020946795140343527,
          0.002086375964948884,
          0.0020781379883624274,
          0.00206996481059822,
          0.002061855670103093,
          0.0020538098172109263,
          0.0020458265139116204,
          0.002037905033625433,
          0.0020300446609825416,
          0.0020222446916076846,
          0.00201450443190975,
          0.002006823198876179,
          0.0019992003198720507,
          0.0019916351324437364,
          0.001984126984126984,
          0.0019766752322593396,
          0.0019692792437967705,
          0.001961938395134393,
          0.0019546520719311965,
          0.0019474196689386564,
          0.0019402405898331393,
          0.0019331142470520007,
          0.001926040061633282,
          0.0019190174630589138,
          0.0019120458891013384,
          0.0019051247856734614,
          0.0018982536066818525,
          0.0018914318138831092,
          0.0018846588767433092,
          0.0018779342723004694,
          0.0018712574850299403,
          0.001864628006712661,
          0.001858045336306206,
          0.0018515089798185522,
          0.001845018450184502,
          0.0018385732671446957,
          0.0018321729571271527,
          0.0018258170531312763,
          0.001819505094614265,
          0.0018132366273798729,
          0.0018070112034694616,
          0.0018008283810552854,
          0.0017946877243359657,
          0.0017885888034340906,
          0.0017825311942959003,
          0.0017765144785930006,
          0.0017705382436260624,
          0.0017646020822304571,
          0.0017587055926837848,
          0.0017528483786152498,
          0.0017470300489168414,
          0.001741250217656277,
          0.0017355085039916696,
          0.001729804532087874,
          0.0017241379310344825,
          0.0017185083347654234,
          0.0017129153819801302,
          0.0017073587160662455,
          0.0017018379850238256,
          0.0016963528413910093,
          0.0016909029421711195,
          0.0016854879487611665,
          0.0016801075268817205,
          0.0016747613465081226,
          0.001669449081803005,
          0.0016641704110500917,
          0.0016589250165892503,
          0.0016537125847527702,
          0.0016485328058028356,
          0.0016433853738701727,
          0.00163826998689384,
          0.0016331863465621427,
          0.0016281341582546401,
          0.0016231131309852299,
          0.0016181229773462784,
          0.0016131634134537829,
          0.0016082341588935349,
          0.00160333493666827,
          0.00159846547314578,
          0.001593625498007968,
          0.001588814744200826,
          0.0015840329478853162,
          0.0015792798483891346,
          0.001574555188159345,
          0.0015698587127158557,
          0.0015651901706057285,
          0.001560549313358302,
          0.0015559358954411078,
          0.0015513496742165683,
          0.0015467904098994587,
          0.0015422578655151142,
          0.001537751806858373,
          0.0015332720024532351,
          0.0015288182235132241,
          0.001524390243902439,
          0.0015199878400972793,
          0.001515610791148833,
          0.001511258878645912,
          0.0015069318866787222,
          0.0015026296018031556,
          0.0014983518130056938,
          0.0014940983116689078,
          0.0014898688915375448,
          0.001485663348685188,
          0.0014814814814814814,
          0.0014773230905599055,
          0.0014731879787860931,
          0.0014690759512266784,
          0.001464986815118664,
          0.0014609203798392988,
          0.001456876456876457,
          0.001452854859799506,
          0.0014488554042306578,
          0.0014448779078167894,
          0.001440922190201729,
          0.001436988072998994,
          0.0014330753797649758,
          0.0014291839359725598,
          0.0014253135689851768,
          0.0014214641080312722,
          0.001417635384179189,
          0.0014138272303124558,
          0.0014100394811054709,
          0.001406271972999578,
          0.001402524544179523,
          0.0013987970345502867,
          0.0013950892857142857,
          0.0013914011409489355,
          0.0013877324451845683,
          0.001384083044982699,
          0.0013804527885146326,
          0.0013768415255404102,
          0.0013732491073880802,
          0.0013696753869332967,
          0.001366120218579235,
          0.0013625834582368169,
          0.001359064963305246,
          0.0013555645926528399,
          0.001352082206598161,
          0.001348617666891436,
          0.0013451708366962604,
          0.001341741580571582,
          0.0013383297644539614,
          0.0013349352556401015,
          0.0013315579227696406,
          0.0013281976358082083,
          0.0013248542660307366,
          0.001321527686005022,
          0.001318217769575534,
          0.0013149243918474688,
          0.001311647429171039,
          0.0013083867591259977,
          0.0013051422605063953,
          0.0013019138133055592,
          0.0012987012987012987,
          0.0012955045990413268,
          0.0012923235978288964,
          0.0012891581797086504,
          0.001286008230452675,
          0.0012828736369467609,
          0.001279754287176862,
          0.0012766500702157538,
          0.0012735608762098828,
          0.0012704865963664084,
          0.001267427122940431,
          0.001264382349222405,
          0.0012613521695257316,
          0.0012583364791745313,
          0.0012553351744915891,
          0.0012523481527864746,
          0.001249375312343828,
          0.0012464165524118161,
          0.0012434717731907485,
          0.0012405408758218584,
          0.0012376237623762376,
          0.0012347203358439314,
          0.001231830500123183,
          0.0012289541600098315,
          0.0012260912211868563,
          0.0012232415902140672,
          0.0012204051745179402,
          0.00121758188238159,
          0.0012147716229348885,
          0.0012119743061447096,
          0.0012091898428053206,
          0.0012064181445288935,
          0.001203659123736158,
          0.001200912693647172,
          0.0011981787682722263,
          0.001195457262402869,
          0.0011927480916030535,
          0.0011900511722004047,
          0.0011873664212776061,
          0.0011846937566639022,
          0.001182033096926714,
          0.0011793843613633684,
          0.0011767474699929394,
          0.0011741223435481978,
          0.0011715089034676663,
          0.001168907071887785,
          0.0011663167716351758,
          0.0011637379262190155,
          0.0011611704598235018,
          0.0011586142973004287,
          0.0011560693641618498,
          0.0011535355865728457,
          0.0011510128913443832,
          0.0011485012059262662,
          0.0011460004584001836,
          0.0011435105774728418,
          0.0011410314924691922,
          0.001138563133325743,
          0.0011361054305839583,
          0.0011336583153837435,
          0.0011312217194570137,
          0.0011287955751213454,
          0.0011263798152737104,
          0.0011239743733842868,
          0.0011215791834903544,
          0.001119194180190263,
          0.0011168192986374804,
          0.0011144544745347153,
          0.0011120996441281138,
          0.0011097547442015317,
          0.0011074197120708748,
          0.001105094485578517,
          0.0011027790030877812,
          0.0011004732034774953,
          0.0010981770261366134,
          0.001095890410958904,
          0.0010936132983377078,
          0.0010913456291607553,
          0.0010890873448050533,
          0.0010868383871318334,
          0.001084598698481562,
          0.0010823682216690117,
          0.0010801468999783973,
          0.001077934677158564,
          0.0010757314974182445,
          0.0010735373054213632,
          0.0010713520462824085,
          0.0010691756655618518,
          0.0010670081092616305,
          0.0010648493238206793,
          0.0010626992561105207,
          0.0010605578534309047,
          0.0010584250635055038,
          0.001056300834477659,
          0.0010541851149061775,
          0.0010520778537611783,
          0.0010499790004199914,
          0.001047888504663104,
          0.0010458063166701526,
          0.001043732387015969
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 1,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning rate decay"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Learning rate"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"72c436db-72ab-4c77-8439-d5b7bcd7dbfa\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"72c436db-72ab-4c77-8439-d5b7bcd7dbfa\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '72c436db-72ab-4c77-8439-d5b7bcd7dbfa',\n",
       "                        [{\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [0.005, 0.004952947003467063, 0.004906771344455349, 0.004861448711716091, 0.004816955684007707, 0.0047732696897374695, 0.004730368968779565, 0.0046882325363338025, 0.004646840148698885, 0.00460617227084293, 0.004566210045662101, 0.004526935264825713, 0.004488330341113105, 0.004450378282153984, 0.0044130626654898504, 0.00437636761487965, 0.004340277777777778, 0.004304778303917348, 0.004269854824935952, 0.004235493434985176, 0.004201680672268908, 0.004168403501458941, 0.0041356492969396195, 0.004103405826836275, 0.004071661237785016, 0.00404040404040404, 0.00400962309542903, 0.003979307600477517, 0.0039494470774091624, 0.003920031360250882, 0.003891050583657588, 0.003862495171881035, 0.003834355828220859, 0.0038066235249333844, 0.003779289493575208, 0.00375234521575985, 0.0037257824143070045, 0.003699593044765076, 0.0036737692872887582, 0.0036483035388544327, 0.003623188405797102, 0.003598416696653473, 0.0035739814152966403, 0.0035498757543485976, 0.003526093088857546, 0.0035026269702276708, 0.003479471120389701, 0.0034566194262011757, 0.0034340659340659344, 0.0034118048447628795, 0.003389830508474576, 0.003368137420006736, 0.0033467202141900937, 0.003325573661456602, 0.0033046926635822873, 0.0032840722495894913, 0.0032637075718015664, 0.003243593902043464, 0.003223726627981947, 0.0032041012495994873, 0.003184713375796179, 0.003165558721114277, 0.0031466331025802393, 0.0031279324366593683, 0.0031094527363184077, 0.0030911901081916537, 0.003073140749846343, 0.0030553009471432934, 0.003037667071688943, 0.0030202355783751134, 0.003003003003003003, 0.002985965959988056, 0.0029691211401425177, 0.002952465308532625, 0.0029359953024075164, 0.0029197080291970805, 0.0029036004645760743, 0.0028876696505919725, 0.002871912693854107, 0.0028563267637817763, 0.002840909090909091, 0.0028256569652444193, 0.002810567734682406, 0.0027956388034665923, 0.0027808676307007787, 0.0027662517289073303, 0.0027517886626307097, 0.0027374760470845875, 0.0027233115468409588, 0.00270929287455974, 0.0026954177897574125, 0.0026816840976133013, 0.0026680896478121665, 0.002654632333421821, 0.002641310089804543, 0.002628120893561104, 0.0026150627615062765, 0.0026021337496747333, 0.002589331952356292, 0.0025766555011594947, 0.002564102564102564, 0.0025516713447307987, 0.0025393600812595226, 0.0025271670457417236, 0.0025150905432595573, 0.0025031289111389237, 0.0024912805181863482, 0.0024795437639474342, 0.00246791707798618, 0.0024563989191844758, 0.0024449877750611247, 0.002433682161109759, 0.0024224806201550387, 0.0024113817217265494, 0.002400384061449832, 0.0023894862604540022, 0.0023786869647954324, 0.0023679848448969923, 0.0023573785950023575, 0.002346866932644919, 0.0023364485981308414, 0.0023261223540358227, 0.0023158869847151463, 0.0023057412958266085, 0.002295684113865932, 0.002285714285714286, 0.002275830678197542, 0.0022660321776569224, 0.002256317689530686, 0.0022466861379465287, 0.0022371364653243843, 0.0022276676319893073, 0.0022182786157941437, 0.002208968411751712, 0.002199736031676199, 0.002190580503833516, 0.0021815008726003495, 0.0021724961981316533, 0.002163565556036348, 0.0021547080370609784, 0.002145922746781116, 0.0021372088053002777, 0.0021285653469561515, 0.0021199915200339195, 0.002111486486486486, 0.002103049421661409, 0.0020946795140343527, 0.002086375964948884, 0.0020781379883624274, 0.00206996481059822, 0.002061855670103093, 0.0020538098172109263, 0.0020458265139116204, 0.002037905033625433, 0.0020300446609825416, 0.0020222446916076846, 0.00201450443190975, 0.002006823198876179, 0.0019992003198720507, 0.0019916351324437364, 0.001984126984126984, 0.0019766752322593396, 0.0019692792437967705, 0.001961938395134393, 0.0019546520719311965, 0.0019474196689386564, 0.0019402405898331393, 0.0019331142470520007, 0.001926040061633282, 0.0019190174630589138, 0.0019120458891013384, 0.0019051247856734614, 0.0018982536066818525, 0.0018914318138831092, 0.0018846588767433092, 0.0018779342723004694, 0.0018712574850299403, 0.001864628006712661, 0.001858045336306206, 0.0018515089798185522, 0.001845018450184502, 0.0018385732671446957, 0.0018321729571271527, 0.0018258170531312763, 0.001819505094614265, 0.0018132366273798729, 0.0018070112034694616, 0.0018008283810552854, 0.0017946877243359657, 0.0017885888034340906, 0.0017825311942959003, 0.0017765144785930006, 0.0017705382436260624, 0.0017646020822304571, 0.0017587055926837848, 0.0017528483786152498, 0.0017470300489168414, 0.001741250217656277, 0.0017355085039916696, 0.001729804532087874, 0.0017241379310344825, 0.0017185083347654234, 0.0017129153819801302, 0.0017073587160662455, 0.0017018379850238256, 0.0016963528413910093, 0.0016909029421711195, 0.0016854879487611665, 0.0016801075268817205, 0.0016747613465081226, 0.001669449081803005, 0.0016641704110500917, 0.0016589250165892503, 0.0016537125847527702, 0.0016485328058028356, 0.0016433853738701727, 0.00163826998689384, 0.0016331863465621427, 0.0016281341582546401, 0.0016231131309852299, 0.0016181229773462784, 0.0016131634134537829, 0.0016082341588935349, 0.00160333493666827, 0.00159846547314578, 0.001593625498007968, 0.001588814744200826, 0.0015840329478853162, 0.0015792798483891346, 0.001574555188159345, 0.0015698587127158557, 0.0015651901706057285, 0.001560549313358302, 0.0015559358954411078, 0.0015513496742165683, 0.0015467904098994587, 0.0015422578655151142, 0.001537751806858373, 0.0015332720024532351, 0.0015288182235132241, 0.001524390243902439, 0.0015199878400972793, 0.001515610791148833, 0.001511258878645912, 0.0015069318866787222, 0.0015026296018031556, 0.0014983518130056938, 0.0014940983116689078, 0.0014898688915375448, 0.001485663348685188, 0.0014814814814814814, 0.0014773230905599055, 0.0014731879787860931, 0.0014690759512266784, 0.001464986815118664, 0.0014609203798392988, 0.001456876456876457, 0.001452854859799506, 0.0014488554042306578, 0.0014448779078167894, 0.001440922190201729, 0.001436988072998994, 0.0014330753797649758, 0.0014291839359725598, 0.0014253135689851768, 0.0014214641080312722, 0.001417635384179189, 0.0014138272303124558, 0.0014100394811054709, 0.001406271972999578, 0.001402524544179523, 0.0013987970345502867, 0.0013950892857142857, 0.0013914011409489355, 0.0013877324451845683, 0.001384083044982699, 0.0013804527885146326, 0.0013768415255404102, 0.0013732491073880802, 0.0013696753869332967, 0.001366120218579235, 0.0013625834582368169, 0.001359064963305246, 0.0013555645926528399, 0.001352082206598161, 0.001348617666891436, 0.0013451708366962604, 0.001341741580571582, 0.0013383297644539614, 0.0013349352556401015, 0.0013315579227696406, 0.0013281976358082083, 0.0013248542660307366, 0.001321527686005022, 0.001318217769575534, 0.0013149243918474688, 0.001311647429171039, 0.0013083867591259977, 0.0013051422605063953, 0.0013019138133055592, 0.0012987012987012987, 0.0012955045990413268, 0.0012923235978288964, 0.0012891581797086504, 0.001286008230452675, 0.0012828736369467609, 0.001279754287176862, 0.0012766500702157538, 0.0012735608762098828, 0.0012704865963664084, 0.001267427122940431, 0.001264382349222405, 0.0012613521695257316, 0.0012583364791745313, 0.0012553351744915891, 0.0012523481527864746, 0.001249375312343828, 0.0012464165524118161, 0.0012434717731907485, 0.0012405408758218584, 0.0012376237623762376, 0.0012347203358439314, 0.001231830500123183, 0.0012289541600098315, 0.0012260912211868563, 0.0012232415902140672, 0.0012204051745179402, 0.00121758188238159, 0.0012147716229348885, 0.0012119743061447096, 0.0012091898428053206, 0.0012064181445288935, 0.001203659123736158, 0.001200912693647172, 0.0011981787682722263, 0.001195457262402869, 0.0011927480916030535, 0.0011900511722004047, 0.0011873664212776061, 0.0011846937566639022, 0.001182033096926714, 0.0011793843613633684, 0.0011767474699929394, 0.0011741223435481978, 0.0011715089034676663, 0.001168907071887785, 0.0011663167716351758, 0.0011637379262190155, 0.0011611704598235018, 0.0011586142973004287, 0.0011560693641618498, 0.0011535355865728457, 0.0011510128913443832, 0.0011485012059262662, 0.0011460004584001836, 0.0011435105774728418, 0.0011410314924691922, 0.001138563133325743, 0.0011361054305839583, 0.0011336583153837435, 0.0011312217194570137, 0.0011287955751213454, 0.0011263798152737104, 0.0011239743733842868, 0.0011215791834903544, 0.001119194180190263, 0.0011168192986374804, 0.0011144544745347153, 0.0011120996441281138, 0.0011097547442015317, 0.0011074197120708748, 0.001105094485578517, 0.0011027790030877812, 0.0011004732034774953, 0.0010981770261366134, 0.001095890410958904, 0.0010936132983377078, 0.0010913456291607553, 0.0010890873448050533, 0.0010868383871318334, 0.001084598698481562, 0.0010823682216690117, 0.0010801468999783973, 0.001077934677158564, 0.0010757314974182445, 0.0010735373054213632, 0.0010713520462824085, 0.0010691756655618518, 0.0010670081092616305, 0.0010648493238206793, 0.0010626992561105207, 0.0010605578534309047, 0.0010584250635055038, 0.001056300834477659, 0.0010541851149061775, 0.0010520778537611783, 0.0010499790004199914, 0.001047888504663104, 0.0010458063166701526, 0.001043732387015969]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 1, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning rate decay\"}, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Learning rate\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('72c436db-72ab-4c77-8439-d5b7bcd7dbfa');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trace0=go.Scatter(\n",
    "            y=lrs,\n",
    "            x=epochs,\n",
    "            mode='lines',\n",
    "            marker=dict(\n",
    "            color=\"red\",\n",
    "            size=5,\n",
    "            opacity=0.5\n",
    "            )\n",
    "    )\n",
    "        \n",
    "\n",
    "data=[trace0]\n",
    "figure=go.Figure(\n",
    "            data=data,\n",
    "            layout=go.Layout(\n",
    "                title=\"Learning rate decay\",\n",
    "                yaxis=dict(title=\"Learning rate\"),\n",
    "                xaxis=dict(title=\"Epoch\"),\n",
    "                legend=dict(\n",
    "                    x=1,\n",
    "                    y=1,\n",
    "                    traceorder=\"normal\",\n",
    "                    font=dict(\n",
    "                        family=\"sans-serif\",\n",
    "                        size=12,\n",
    "                        color=\"black\"\n",
    "                    ),\n",
    "                bgcolor=None\n",
    "\n",
    "\n",
    "            )))\n",
    "iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/400\n",
      "19948/19948 [==============================] - 3s 134us/sample - loss: 11171.3894 - val_loss: 10215.8105\n",
      "Epoch 2/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 10988.3311 - val_loss: 8093.7663\n",
      "Epoch 3/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 10369.8484 - val_loss: 6920.4378\n",
      "Epoch 4/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 8943.6729 - val_loss: 8042.0203\n",
      "Epoch 5/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 6391.3162 - val_loss: 15662.6513\n",
      "Epoch 6/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 4040.4297 - val_loss: 10329.9911\n",
      "Epoch 7/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1864.3942 - val_loss: 12797.6682\n",
      "Epoch 8/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1578.0143 - val_loss: 8774.8852\n",
      "Epoch 9/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1534.0043 - val_loss: 6359.2977\n",
      "Epoch 10/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1487.2056 - val_loss: 4215.6951\n",
      "Epoch 11/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1473.4251 - val_loss: 3226.8927\n",
      "Epoch 12/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1454.7861 - val_loss: 2526.0318\n",
      "Epoch 13/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1443.1135 - val_loss: 1940.3189\n",
      "Epoch 14/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1426.0031 - val_loss: 1966.5735\n",
      "Epoch 15/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1420.3248 - val_loss: 1618.3393\n",
      "Epoch 16/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1410.3662 - val_loss: 1547.3610\n",
      "Epoch 17/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1396.0182 - val_loss: 1549.8530\n",
      "Epoch 18/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1386.4667 - val_loss: 1449.1679\n",
      "Epoch 19/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1354.3592 - val_loss: 1399.7945\n",
      "Epoch 20/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1367.2353 - val_loss: 1349.0534\n",
      "Epoch 21/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1364.9632 - val_loss: 1380.0415\n",
      "Epoch 22/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1366.4289 - val_loss: 1295.0369\n",
      "Epoch 23/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1349.6691 - val_loss: 1327.7204\n",
      "Epoch 24/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1337.5837 - val_loss: 1372.4935\n",
      "Epoch 25/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1331.0284 - val_loss: 1282.7482\n",
      "Epoch 26/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1330.4568 - val_loss: 1252.9929\n",
      "Epoch 27/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1330.3317 - val_loss: 1214.3050\n",
      "Epoch 28/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1317.4884 - val_loss: 1299.2271\n",
      "Epoch 29/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1310.2244 - val_loss: 1209.1796\n",
      "Epoch 30/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1296.7559 - val_loss: 1195.9164\n",
      "Epoch 31/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1305.7583 - val_loss: 1214.2549\n",
      "Epoch 32/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1296.9664 - val_loss: 1209.4728\n",
      "Epoch 33/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1301.7304 - val_loss: 1206.8325\n",
      "Epoch 34/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1303.3816 - val_loss: 1209.9606\n",
      "Epoch 35/400\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1290.9000 - val_loss: 1214.8191\n",
      "Epoch 36/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1279.5641 - val_loss: 1214.5319\n",
      "Epoch 37/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1281.1342 - val_loss: 1219.9404\n",
      "Epoch 38/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1292.0880 - val_loss: 1196.5079\n",
      "Epoch 39/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1277.4292 - val_loss: 1202.7652\n",
      "Epoch 40/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1272.5326 - val_loss: 1183.7264\n",
      "Epoch 41/400\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1263.3257 - val_loss: 1201.4504\n",
      "Epoch 42/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1268.3206 - val_loss: 1177.3998\n",
      "Epoch 43/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1260.8831 - val_loss: 1218.7089\n",
      "Epoch 44/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1267.2570 - val_loss: 1180.9474\n",
      "Epoch 45/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1252.1846 - val_loss: 1182.3883\n",
      "Epoch 46/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1252.4591 - val_loss: 1190.1772\n",
      "Epoch 47/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1239.2961 - val_loss: 1185.0851\n",
      "Epoch 48/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1245.3355 - val_loss: 1177.6933\n",
      "Epoch 49/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1235.5068 - val_loss: 1179.7842\n",
      "Epoch 50/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1240.4451 - val_loss: 1173.6220\n",
      "Epoch 51/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1246.7970 - val_loss: 1189.7344\n",
      "Epoch 52/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1232.2679 - val_loss: 1174.2032\n",
      "Epoch 53/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1229.5749 - val_loss: 1183.7677\n",
      "Epoch 54/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1231.3809 - val_loss: 1167.3461\n",
      "Epoch 55/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1222.7193 - val_loss: 1174.4871\n",
      "Epoch 56/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1230.3167 - val_loss: 1176.2013\n",
      "Epoch 57/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1230.2943 - val_loss: 1170.5672\n",
      "Epoch 58/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1215.9205 - val_loss: 1163.1335\n",
      "Epoch 59/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1227.7155 - val_loss: 1170.4715\n",
      "Epoch 60/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1224.5357 - val_loss: 1170.6035\n",
      "Epoch 61/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1218.9341 - val_loss: 1164.9303\n",
      "Epoch 62/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1221.7411 - val_loss: 1164.6607\n",
      "Epoch 63/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1205.0487 - val_loss: 1157.4971\n",
      "Epoch 64/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1211.8226 - val_loss: 1162.5017\n",
      "Epoch 65/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1204.9379 - val_loss: 1169.0701\n",
      "Epoch 66/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1205.5271 - val_loss: 1162.4918\n",
      "Epoch 67/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1198.9790 - val_loss: 1156.5812\n",
      "Epoch 68/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1198.0181 - val_loss: 1161.2502\n",
      "Epoch 69/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1194.5693 - val_loss: 1156.2944\n",
      "Epoch 70/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1199.8736 - val_loss: 1178.3629\n",
      "Epoch 71/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1205.7360 - val_loss: 1181.6069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1208.1765 - val_loss: 1169.4947\n",
      "Epoch 73/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1187.2578 - val_loss: 1150.5507\n",
      "Epoch 74/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1189.0278 - val_loss: 1141.2771\n",
      "Epoch 75/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1192.7991 - val_loss: 1161.2935\n",
      "Epoch 76/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1187.8521 - val_loss: 1164.6987\n",
      "Epoch 77/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1183.7498 - val_loss: 1147.6664\n",
      "Epoch 78/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1189.7391 - val_loss: 1147.7745\n",
      "Epoch 79/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1191.2774 - val_loss: 1166.7738\n",
      "Epoch 80/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1188.8559 - val_loss: 1141.0733\n",
      "Epoch 81/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1186.0490 - val_loss: 1143.2499\n",
      "Epoch 82/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1183.5244 - val_loss: 1151.0796\n",
      "Epoch 83/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1176.2774 - val_loss: 1142.9105\n",
      "Epoch 84/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1180.4823 - val_loss: 1155.0727\n",
      "Epoch 85/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1173.6365 - val_loss: 1146.9233\n",
      "Epoch 86/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1167.3931 - val_loss: 1149.2414\n",
      "Epoch 87/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1173.0466 - val_loss: 1139.3388\n",
      "Epoch 88/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1172.1893 - val_loss: 1143.3741\n",
      "Epoch 89/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1169.3179 - val_loss: 1148.2775\n",
      "Epoch 90/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1173.6997 - val_loss: 1149.4863\n",
      "Epoch 91/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1168.2572 - val_loss: 1157.9501\n",
      "Epoch 92/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1166.9889 - val_loss: 1151.0083\n",
      "Epoch 93/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1168.2611 - val_loss: 1147.2400\n",
      "Epoch 94/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1159.0628 - val_loss: 1141.3206\n",
      "Epoch 95/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1164.5434 - val_loss: 1130.2048\n",
      "Epoch 96/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1158.8619 - val_loss: 1135.3018\n",
      "Epoch 97/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1161.7723 - val_loss: 1146.2134\n",
      "Epoch 98/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1162.2330 - val_loss: 1139.7175\n",
      "Epoch 99/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1160.5965 - val_loss: 1140.3083\n",
      "Epoch 100/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1160.0056 - val_loss: 1132.4563\n",
      "Epoch 101/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1158.0341 - val_loss: 1130.3932\n",
      "Epoch 102/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1157.9289 - val_loss: 1137.0869\n",
      "Epoch 103/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1158.0398 - val_loss: 1137.4955\n",
      "Epoch 104/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1157.1908 - val_loss: 1132.4248\n",
      "Epoch 105/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1158.7234 - val_loss: 1138.6609\n",
      "Epoch 106/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1161.5447 - val_loss: 1124.8799\n",
      "Epoch 107/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1152.8448 - val_loss: 1126.7175\n",
      "Epoch 108/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1150.8664 - val_loss: 1128.3115\n",
      "Epoch 109/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1152.3141 - val_loss: 1130.9518\n",
      "Epoch 110/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1145.0119 - val_loss: 1132.9764\n",
      "Epoch 111/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1147.2014 - val_loss: 1125.6450\n",
      "Epoch 112/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1152.1257 - val_loss: 1130.8750\n",
      "Epoch 113/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1145.7432 - val_loss: 1132.0854\n",
      "Epoch 114/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1141.7835 - val_loss: 1124.9454\n",
      "Epoch 115/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1143.6399 - val_loss: 1124.8707\n",
      "Epoch 116/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1143.8753 - val_loss: 1128.9974\n",
      "Epoch 117/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1139.7146 - val_loss: 1121.5336\n",
      "Epoch 118/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1140.2338 - val_loss: 1126.5507\n",
      "Epoch 119/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1134.0117 - val_loss: 1127.3203\n",
      "Epoch 120/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1151.3375 - val_loss: 1126.6831\n",
      "Epoch 121/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1138.8755 - val_loss: 1127.8825\n",
      "Epoch 122/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1143.2790 - val_loss: 1121.4721\n",
      "Epoch 123/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1136.6700 - val_loss: 1123.5621\n",
      "Epoch 124/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1125.7735 - val_loss: 1115.6948\n",
      "Epoch 125/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1131.6607 - val_loss: 1117.3865\n",
      "Epoch 126/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1136.0080 - val_loss: 1112.0009\n",
      "Epoch 127/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1131.5825 - val_loss: 1134.7412\n",
      "Epoch 128/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1127.3502 - val_loss: 1110.9558\n",
      "Epoch 129/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1130.9955 - val_loss: 1117.9257\n",
      "Epoch 130/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1132.3462 - val_loss: 1113.4084\n",
      "Epoch 131/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1130.1678 - val_loss: 1116.0674\n",
      "Epoch 132/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1128.4110 - val_loss: 1123.3674\n",
      "Epoch 133/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1131.7786 - val_loss: 1124.5122\n",
      "Epoch 134/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1127.0058 - val_loss: 1115.8930\n",
      "Epoch 135/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1129.4099 - val_loss: 1118.0817\n",
      "Epoch 136/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1124.7884 - val_loss: 1108.0512\n",
      "Epoch 137/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1125.4598 - val_loss: 1114.5710\n",
      "Epoch 138/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1126.9511 - val_loss: 1116.8120\n",
      "Epoch 139/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1124.3521 - val_loss: 1113.0289\n",
      "Epoch 140/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1115.4710 - val_loss: 1111.8593\n",
      "Epoch 141/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1127.4526 - val_loss: 1113.8950\n",
      "Epoch 142/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1118.8265 - val_loss: 1117.4663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1119.0780 - val_loss: 1110.7133\n",
      "Epoch 144/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1125.3924 - val_loss: 1113.9108\n",
      "Epoch 145/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1118.2526 - val_loss: 1117.8953\n",
      "Epoch 146/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1118.0542 - val_loss: 1113.4458\n",
      "Epoch 147/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1118.5877 - val_loss: 1116.5380\n",
      "Epoch 148/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1103.0027 - val_loss: 1111.1589\n",
      "Epoch 149/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1122.8149 - val_loss: 1105.3445\n",
      "Epoch 150/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1121.8092 - val_loss: 1108.5650\n",
      "Epoch 151/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1118.9211 - val_loss: 1119.6996\n",
      "Epoch 152/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1111.0760 - val_loss: 1109.9768\n",
      "Epoch 153/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1112.8780 - val_loss: 1110.0341\n",
      "Epoch 154/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1117.6440 - val_loss: 1110.2612\n",
      "Epoch 155/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1105.5622 - val_loss: 1114.4490\n",
      "Epoch 156/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1117.4732 - val_loss: 1116.0404\n",
      "Epoch 157/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1115.5068 - val_loss: 1105.2304\n",
      "Epoch 158/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1108.9097 - val_loss: 1123.5991\n",
      "Epoch 159/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1106.3474 - val_loss: 1110.3042\n",
      "Epoch 160/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1102.4649 - val_loss: 1111.4039\n",
      "Epoch 161/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1107.9383 - val_loss: 1112.3502\n",
      "Epoch 162/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1101.0049 - val_loss: 1105.2178\n",
      "Epoch 163/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1106.9954 - val_loss: 1109.5128\n",
      "Epoch 164/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1104.0462 - val_loss: 1108.4073\n",
      "Epoch 165/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1102.2670 - val_loss: 1109.2704\n",
      "Epoch 166/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1102.1481 - val_loss: 1109.7278\n",
      "Epoch 167/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1104.4138 - val_loss: 1104.7669\n",
      "Epoch 168/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1096.0964 - val_loss: 1103.7405\n",
      "Epoch 169/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1107.7231 - val_loss: 1113.9285\n",
      "Epoch 170/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1104.2334 - val_loss: 1107.1907\n",
      "Epoch 171/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1097.4208 - val_loss: 1107.2083\n",
      "Epoch 172/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1109.6081 - val_loss: 1106.5503\n",
      "Epoch 173/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1102.4426 - val_loss: 1111.3200\n",
      "Epoch 174/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1096.9949 - val_loss: 1102.0148\n",
      "Epoch 175/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1093.7660 - val_loss: 1110.1415\n",
      "Epoch 176/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1094.3305 - val_loss: 1101.1411\n",
      "Epoch 177/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1094.2741 - val_loss: 1106.8373\n",
      "Epoch 178/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1092.0546 - val_loss: 1101.9893\n",
      "Epoch 179/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1105.8126 - val_loss: 1100.4030\n",
      "Epoch 180/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1092.3681 - val_loss: 1104.6266\n",
      "Epoch 181/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1091.9998 - val_loss: 1105.1728\n",
      "Epoch 182/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1095.0647 - val_loss: 1094.8266\n",
      "Epoch 183/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1093.1634 - val_loss: 1108.2417\n",
      "Epoch 184/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1096.0462 - val_loss: 1098.6142\n",
      "Epoch 185/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1092.6927 - val_loss: 1099.8509\n",
      "Epoch 186/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1097.1056 - val_loss: 1097.0221\n",
      "Epoch 187/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.5983 - val_loss: 1100.9536\n",
      "Epoch 188/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1098.5823 - val_loss: 1104.0817\n",
      "Epoch 189/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1098.1943 - val_loss: 1097.7556\n",
      "Epoch 190/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1096.1290 - val_loss: 1101.0615\n",
      "Epoch 191/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.0104 - val_loss: 1107.2506\n",
      "Epoch 192/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1093.2293 - val_loss: 1098.4193\n",
      "Epoch 193/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1083.0980 - val_loss: 1105.4703\n",
      "Epoch 194/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1092.4070 - val_loss: 1103.8102\n",
      "Epoch 195/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1085.1679 - val_loss: 1097.6723\n",
      "Epoch 196/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1081.9991 - val_loss: 1102.5976\n",
      "Epoch 197/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1077.7717 - val_loss: 1107.4567\n",
      "Epoch 198/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.0467 - val_loss: 1094.3663\n",
      "Epoch 199/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1079.8471 - val_loss: 1102.4975\n",
      "Epoch 200/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1082.4404 - val_loss: 1105.3534\n",
      "Epoch 201/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1080.8809 - val_loss: 1094.0692\n",
      "Epoch 202/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.0881 - val_loss: 1098.4008\n",
      "Epoch 203/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1086.3433 - val_loss: 1098.3733\n",
      "Epoch 204/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1089.0918 - val_loss: 1099.4315\n",
      "Epoch 205/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1077.6132 - val_loss: 1099.0188\n",
      "Epoch 206/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1079.7424 - val_loss: 1107.1029\n",
      "Epoch 207/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1084.0132 - val_loss: 1096.7116\n",
      "Epoch 208/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1077.5654 - val_loss: 1103.8259\n",
      "Epoch 209/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1078.0872 - val_loss: 1093.2292\n",
      "Epoch 210/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1076.8475 - val_loss: 1100.6557\n",
      "Epoch 211/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1073.5970 - val_loss: 1094.9564\n",
      "Epoch 212/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1071.7419 - val_loss: 1090.0648\n",
      "Epoch 213/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1074.3685 - val_loss: 1092.9811\n",
      "Epoch 214/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1068.7884 - val_loss: 1091.0727\n",
      "Epoch 215/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1079.5693 - val_loss: 1092.2428\n",
      "Epoch 216/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1069.8279 - val_loss: 1093.1142\n",
      "Epoch 217/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1073.9470 - val_loss: 1094.4866\n",
      "Epoch 218/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1072.6508 - val_loss: 1092.9297\n",
      "Epoch 219/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1082.4189 - val_loss: 1094.5677\n",
      "Epoch 220/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1070.8349 - val_loss: 1096.0116\n",
      "Epoch 221/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1076.8125 - val_loss: 1088.8111\n",
      "Epoch 222/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1063.1521 - val_loss: 1095.5668\n",
      "Epoch 223/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1080.2199 - val_loss: 1092.8793\n",
      "Epoch 224/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1079.1497 - val_loss: 1096.5500\n",
      "Epoch 225/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1083.8327 - val_loss: 1096.1651\n",
      "Epoch 226/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.4601 - val_loss: 1092.5835\n",
      "Epoch 227/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1071.7425 - val_loss: 1091.0827\n",
      "Epoch 228/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1064.4584 - val_loss: 1093.7988\n",
      "Epoch 229/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1071.6472 - val_loss: 1095.2144\n",
      "Epoch 230/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1071.4504 - val_loss: 1088.7292\n",
      "Epoch 231/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1073.1053 - val_loss: 1102.8900\n",
      "Epoch 232/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1069.9349 - val_loss: 1099.2639\n",
      "Epoch 233/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1071.3758 - val_loss: 1106.3820\n",
      "Epoch 234/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1067.6307 - val_loss: 1091.8459\n",
      "Epoch 235/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1068.9737 - val_loss: 1091.3444\n",
      "Epoch 236/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1061.9855 - val_loss: 1090.6475\n",
      "Epoch 237/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1067.2497 - val_loss: 1090.3902\n",
      "Epoch 238/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1064.7642 - val_loss: 1093.6833\n",
      "Epoch 239/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1077.5645 - val_loss: 1097.5360\n",
      "Epoch 240/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1067.8964 - val_loss: 1093.2617\n",
      "Epoch 241/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1064.1050 - val_loss: 1092.4963\n",
      "Epoch 242/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1068.9872 - val_loss: 1091.7602\n",
      "Epoch 243/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1062.4591 - val_loss: 1093.8471\n",
      "Epoch 244/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1053.8395 - val_loss: 1100.5657\n",
      "Epoch 245/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1065.0909 - val_loss: 1091.4333\n",
      "Epoch 246/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1064.1914 - val_loss: 1089.7073\n",
      "Epoch 247/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1059.7735 - val_loss: 1097.4222\n",
      "Epoch 248/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1061.6720 - val_loss: 1086.0211\n",
      "Epoch 249/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1068.3209 - val_loss: 1091.1292\n",
      "Epoch 250/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1070.4414 - val_loss: 1088.7253\n",
      "Epoch 251/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1064.1377 - val_loss: 1091.4168\n",
      "Epoch 252/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1063.0807 - val_loss: 1090.9272\n",
      "Epoch 253/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1054.0485 - val_loss: 1085.1316\n",
      "Epoch 254/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1056.9105 - val_loss: 1092.6798\n",
      "Epoch 255/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1055.6114 - val_loss: 1092.8207\n",
      "Epoch 256/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1060.7361 - val_loss: 1093.0977\n",
      "Epoch 257/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1065.2181 - val_loss: 1084.6095\n",
      "Epoch 258/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1057.4356 - val_loss: 1085.9295\n",
      "Epoch 259/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1058.5971 - val_loss: 1092.9169\n",
      "Epoch 260/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1060.9540 - val_loss: 1084.7503\n",
      "Epoch 261/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1051.3292 - val_loss: 1089.0278\n",
      "Epoch 262/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1051.7418 - val_loss: 1080.2573\n",
      "Epoch 263/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1062.3241 - val_loss: 1092.9033\n",
      "Epoch 264/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1056.8544 - val_loss: 1089.2308\n",
      "Epoch 265/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1055.3364 - val_loss: 1085.5795\n",
      "Epoch 266/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1053.1097 - val_loss: 1083.9698\n",
      "Epoch 267/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1058.9663 - val_loss: 1083.4878\n",
      "Epoch 268/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1058.8746 - val_loss: 1087.2116\n",
      "Epoch 269/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1055.0787 - val_loss: 1087.5410\n",
      "Epoch 270/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1063.1762 - val_loss: 1084.5580\n",
      "Epoch 271/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1063.4134 - val_loss: 1085.6230\n",
      "Epoch 272/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1053.2922 - val_loss: 1089.5216\n",
      "Epoch 273/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1050.7684 - val_loss: 1091.0540\n",
      "Epoch 274/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1052.0709 - val_loss: 1086.0331\n",
      "Epoch 275/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1059.4850 - val_loss: 1093.5305\n",
      "Epoch 276/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1053.0434 - val_loss: 1084.6754\n",
      "Epoch 277/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1050.9328 - val_loss: 1087.3130\n",
      "Epoch 278/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1048.9964 - val_loss: 1085.3139\n",
      "Epoch 279/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1055.0419 - val_loss: 1086.9164\n",
      "Epoch 280/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1052.7702 - val_loss: 1085.7146\n",
      "Epoch 281/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1052.9572 - val_loss: 1084.8060\n",
      "Epoch 282/400\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1059.3004 - val_loss: 1083.4348\n",
      "Epoch 283/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1046.7246 - val_loss: 1084.2606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1048.2490 - val_loss: 1087.2652\n",
      "Epoch 285/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1051.5229 - val_loss: 1080.3830\n",
      "Epoch 286/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1046.6626 - val_loss: 1088.4076\n",
      "Epoch 287/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1057.5062 - val_loss: 1080.8944\n",
      "Epoch 288/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1053.9465 - val_loss: 1081.9508\n",
      "Epoch 289/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1054.5748 - val_loss: 1082.4989\n",
      "Epoch 290/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1047.0767 - val_loss: 1085.7088\n",
      "Epoch 291/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1051.9027 - val_loss: 1083.0210\n",
      "Epoch 292/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.8860 - val_loss: 1083.9180\n",
      "Epoch 293/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1045.2720 - val_loss: 1079.6777\n",
      "Epoch 294/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1044.7039 - val_loss: 1084.7972\n",
      "Epoch 295/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1049.2626 - val_loss: 1083.2440\n",
      "Epoch 296/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1050.8284 - val_loss: 1083.6280\n",
      "Epoch 297/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1046.3535 - val_loss: 1092.6774\n",
      "Epoch 298/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1044.8292 - val_loss: 1083.8131s - loss: 1045.29\n",
      "Epoch 299/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1052.9391 - val_loss: 1092.0623\n",
      "Epoch 300/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1048.9463 - val_loss: 1099.2218\n",
      "Epoch 301/400\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1046.9562 - val_loss: 1080.9855\n",
      "Epoch 302/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1038.6780 - val_loss: 1088.6342\n",
      "Epoch 303/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1051.3420 - val_loss: 1084.9742\n",
      "Epoch 304/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1051.1243 - val_loss: 1082.1712\n",
      "Epoch 305/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1045.8101 - val_loss: 1083.4832\n",
      "Epoch 306/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1038.0090 - val_loss: 1080.7386\n",
      "Epoch 307/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1040.8055 - val_loss: 1081.6313\n",
      "Epoch 308/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1037.6905 - val_loss: 1079.3762\n",
      "Epoch 309/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1034.5658 - val_loss: 1082.9002\n",
      "Epoch 310/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1039.9340 - val_loss: 1084.9743\n",
      "Epoch 311/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1035.4762 - val_loss: 1083.8763\n",
      "Epoch 312/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1043.8551 - val_loss: 1081.7562\n",
      "Epoch 313/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1042.3436 - val_loss: 1081.7511\n",
      "Epoch 314/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1037.5989 - val_loss: 1081.1496\n",
      "Epoch 315/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1045.1323 - val_loss: 1081.2316\n",
      "Epoch 316/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1043.1994 - val_loss: 1083.7778\n",
      "Epoch 317/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1046.9270 - val_loss: 1085.8382\n",
      "Epoch 318/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1047.1148 - val_loss: 1080.8595\n",
      "Epoch 319/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1037.5933 - val_loss: 1079.8341\n",
      "Epoch 320/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1030.6767 - val_loss: 1078.6031\n",
      "Epoch 321/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1035.7669 - val_loss: 1085.4403\n",
      "Epoch 322/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1036.9111 - val_loss: 1083.2884\n",
      "Epoch 323/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1042.5257 - val_loss: 1083.6630\n",
      "Epoch 324/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1031.3771 - val_loss: 1081.6033\n",
      "Epoch 325/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1039.3898 - val_loss: 1075.4548\n",
      "Epoch 326/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1033.7662 - val_loss: 1078.0691\n",
      "Epoch 327/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1032.0470 - val_loss: 1074.4950\n",
      "Epoch 328/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1037.9692 - val_loss: 1081.9302\n",
      "Epoch 329/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1035.2313 - val_loss: 1080.7324\n",
      "Epoch 330/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1037.4666 - val_loss: 1084.0886\n",
      "Epoch 331/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1029.7589 - val_loss: 1077.6496\n",
      "Epoch 332/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1034.0507 - val_loss: 1082.2118\n",
      "Epoch 333/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1034.6380 - val_loss: 1078.0509\n",
      "Epoch 334/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1029.0920 - val_loss: 1078.8900\n",
      "Epoch 335/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1028.2431 - val_loss: 1078.8158\n",
      "Epoch 336/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1038.0568 - val_loss: 1081.2494\n",
      "Epoch 337/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1039.8624 - val_loss: 1077.1482\n",
      "Epoch 338/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1037.2919 - val_loss: 1075.5101\n",
      "Epoch 339/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1033.1658 - val_loss: 1079.1830\n",
      "Epoch 340/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1029.6357 - val_loss: 1083.7377\n",
      "Epoch 341/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1032.7162 - val_loss: 1076.7422\n",
      "Epoch 342/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1040.8158 - val_loss: 1088.0533\n",
      "Epoch 343/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1030.6289 - val_loss: 1082.8729\n",
      "Epoch 344/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1029.8614 - val_loss: 1073.9046\n",
      "Epoch 345/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1034.1493 - val_loss: 1077.4698\n",
      "Epoch 346/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1036.5424 - val_loss: 1075.4979\n",
      "Epoch 347/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1032.6961 - val_loss: 1073.8998\n",
      "Epoch 348/400\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1031.4082 - val_loss: 1076.3698\n",
      "Epoch 349/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1024.5190 - val_loss: 1078.5990\n",
      "Epoch 350/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1025.2116 - val_loss: 1077.7280\n",
      "Epoch 351/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1031.6460 - val_loss: 1075.3227\n",
      "Epoch 352/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1030.2927 - val_loss: 1078.3869\n",
      "Epoch 353/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1030.4588 - val_loss: 1074.6894\n",
      "Epoch 354/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1035.8678 - val_loss: 1076.9038\n",
      "Epoch 355/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1029.5436 - val_loss: 1077.1456\n",
      "Epoch 356/400\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1038.0853 - val_loss: 1074.7141\n",
      "Epoch 357/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1026.0475 - val_loss: 1074.8276\n",
      "Epoch 358/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1025.2332 - val_loss: 1074.7612\n",
      "Epoch 359/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1033.8520 - val_loss: 1072.3684\n",
      "Epoch 360/400\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1016.8170 - val_loss: 1070.7449\n",
      "Epoch 361/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1028.4912 - val_loss: 1077.1841\n",
      "Epoch 362/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1031.1790 - val_loss: 1072.4849\n",
      "Epoch 363/400\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1029.9579 - val_loss: 1073.1304\n",
      "Epoch 364/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1017.9421 - val_loss: 1070.6338\n",
      "Epoch 365/400\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1021.1910 - val_loss: 1070.6908\n",
      "Epoch 366/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1023.4910 - val_loss: 1074.2801\n",
      "Epoch 367/400\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1033.4698 - val_loss: 1073.6332\n",
      "Epoch 368/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1032.6018 - val_loss: 1073.6640\n",
      "Epoch 369/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1025.5494 - val_loss: 1073.8821\n",
      "Epoch 370/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1024.9298 - val_loss: 1078.7650\n",
      "Epoch 371/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1030.4727 - val_loss: 1078.0936\n",
      "Epoch 372/400\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1023.8996 - val_loss: 1077.9738\n",
      "Epoch 373/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1028.3797 - val_loss: 1076.9395\n",
      "Epoch 374/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1028.5118 - val_loss: 1076.2584\n",
      "Epoch 375/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1019.4264 - val_loss: 1072.4135\n",
      "Epoch 376/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1030.5003 - val_loss: 1072.9386\n",
      "Epoch 377/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1025.4151 - val_loss: 1075.5048\n",
      "Epoch 378/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1022.5836 - val_loss: 1076.4393\n",
      "Epoch 379/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1020.0276 - val_loss: 1074.4834\n",
      "Epoch 380/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1020.7362 - val_loss: 1073.6726\n",
      "Epoch 381/400\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1019.7491 - val_loss: 1075.1238\n",
      "Epoch 382/400\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1023.3829 - val_loss: 1076.0743\n",
      "Epoch 383/400\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1028.1228 - val_loss: 1074.0811\n",
      "Epoch 384/400\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1017.2741 - val_loss: 1073.4062\n",
      "Epoch 385/400\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1020.4641 - val_loss: 1080.1895\n",
      "Epoch 386/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1022.5610 - val_loss: 1084.7678\n",
      "Epoch 387/400\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1024.7578 - val_loss: 1082.3260\n",
      "Epoch 388/400\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1021.3033 - val_loss: 1073.2566\n",
      "Epoch 389/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1022.7047 - val_loss: 1070.6462\n",
      "Epoch 390/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1019.4467 - val_loss: 1076.2570\n",
      "Epoch 391/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1020.4580 - val_loss: 1072.6503\n",
      "Epoch 392/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1024.0425 - val_loss: 1069.3059\n",
      "Epoch 393/400\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1018.4504 - val_loss: 1074.6402\n",
      "Epoch 394/400\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1021.8291 - val_loss: 1071.3166\n",
      "Epoch 395/400\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1023.8839 - val_loss: 1073.3071\n",
      "Epoch 396/400\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1020.9337 - val_loss: 1071.1827\n",
      "Epoch 397/400\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1017.8166 - val_loss: 1074.6492\n",
      "Epoch 398/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1016.9551 - val_loss: 1073.9434\n",
      "Epoch 399/400\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1018.8707 - val_loss: 1071.5960\n",
      "Epoch 400/400\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1014.6263 - val_loss: 1074.8323\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.005, decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=400, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1473.425104245648,
          1454.786106229713,
          1443.1135181207467,
          1426.0031353246957,
          1420.3247609311554,
          1410.3661814663437,
          1396.018175846067,
          1386.4666997229908,
          1354.359159008735,
          1367.235257274382,
          1364.9632447684291,
          1366.428855086702,
          1349.6690600639788,
          1337.5836962724177,
          1331.0284475231304,
          1330.4567943792533,
          1330.3316519679684,
          1317.4884391067164,
          1310.2244272119228,
          1296.7558826777747,
          1305.7582581655663,
          1296.9664385214057,
          1301.7303601913804,
          1303.3815733406857,
          1290.9000204976298,
          1279.5640827038974,
          1281.1342449352692,
          1292.0880044952814,
          1277.4291549385825,
          1272.5326191359252,
          1263.325710176537,
          1268.3205858425265,
          1260.8830860383482,
          1267.2569585707997,
          1252.1845543775144,
          1252.4591011581283,
          1239.296101235282,
          1245.3354661573214,
          1235.506771242927,
          1240.4451181715037,
          1246.7969836320526,
          1232.2679028462283,
          1229.5748750706916,
          1231.3809041447216,
          1222.7192856945637,
          1230.3166681369607,
          1230.2942988195089,
          1215.9204735535047,
          1227.7154674419114,
          1224.5357485370166,
          1218.9340623266978,
          1221.7410532276497,
          1205.0486717369458,
          1211.8225887796552,
          1204.9378738969367,
          1205.5270637005824,
          1198.9789877999203,
          1198.018064986543,
          1194.5692724668122,
          1199.8736434700925,
          1205.7360480511047,
          1208.1765404749615,
          1187.2577827351113,
          1189.0277683647405,
          1192.7990679330712,
          1187.8521215952492,
          1183.7497880230787,
          1189.7391243355771,
          1191.2773770886336,
          1188.8558585084893,
          1186.0490261741074,
          1183.5244314416702,
          1176.2773646539597,
          1180.482328100914,
          1173.6365148017658,
          1167.3931333038165,
          1173.0465811206793,
          1172.1893115704347,
          1169.3179100133002,
          1173.699698415098,
          1168.2572439808346,
          1166.9889226392918,
          1168.2611440869307,
          1159.0628451845894,
          1164.5433797796145,
          1158.8618692046587,
          1161.772260856743,
          1162.2330300970023,
          1160.5965497598443,
          1160.0056174618462,
          1158.0341496533565,
          1157.928902157328,
          1158.0398072831158,
          1157.190761311441,
          1158.7234014737144,
          1161.544658519377,
          1152.8448413384565,
          1150.8663623076247,
          1152.3140841236043,
          1145.0118834849516,
          1147.2013756274125,
          1152.1256965620378,
          1145.743219161813,
          1141.7834891469774,
          1143.639859092625,
          1143.875262449948,
          1139.7145631375952,
          1140.2338375675192,
          1134.011715274166,
          1151.3375000342687,
          1138.8755045589235,
          1143.2790332256445,
          1136.6700194431303,
          1125.7735301481116,
          1131.6607217584078,
          1136.0079929985932,
          1131.5825086631492,
          1127.3501891783872,
          1130.9955442010555,
          1132.346220069642,
          1130.1677578256201,
          1128.411016004894,
          1131.7786379596716,
          1127.0058272602516,
          1129.4099022938155,
          1124.788449529323,
          1125.4598016610767,
          1126.9511139362735,
          1124.352080521661,
          1115.4710383471636,
          1127.452554791873,
          1118.8265219551301,
          1119.0779633492393,
          1125.3924181876787,
          1118.252614218844,
          1118.0541655374286,
          1118.587731397532,
          1103.0026668703865,
          1122.8149198903477,
          1121.8092158019053,
          1118.9210834889464,
          1111.0759718431987,
          1112.8780175624593,
          1117.6439944539395,
          1105.5622172335572,
          1117.4731677116363,
          1115.5067793940025,
          1108.9097031187337,
          1106.3473845474798,
          1102.464858093935,
          1107.9382865665575,
          1101.0049144376942,
          1106.995430502118,
          1104.0462019120807,
          1102.2670009896826,
          1102.148129252267,
          1104.4138107957642,
          1096.0964439916831,
          1107.7230707064932,
          1104.2334107497854,
          1097.4208267912588,
          1109.6080809127168,
          1102.442580714741,
          1096.9948550312845,
          1093.766032431392,
          1094.330482570112,
          1094.2741190082495,
          1092.054597936079,
          1105.8126088278743,
          1092.3681017422643,
          1091.9997955132562,
          1095.0646532547123,
          1093.1633548006066,
          1096.0462165742258,
          1092.6927237379687,
          1097.1055579467554,
          1100.5982998422853,
          1098.5823134289778,
          1098.1943306992712,
          1096.1289940515262,
          1092.0104197916537,
          1093.2293328376436,
          1083.0979608505552,
          1092.4070309122076,
          1085.167947380964,
          1081.9990782920358,
          1077.7716765249806,
          1078.046729602435,
          1079.847145596128,
          1082.4404206797046,
          1080.8808920772133,
          1078.088113959576,
          1086.343318458069,
          1089.0917782719446,
          1077.6132219649996,
          1079.7424236976099,
          1084.013163375166,
          1077.5653840855364,
          1078.0871832438168,
          1076.8475293575798,
          1073.596954890332,
          1071.7418590826383,
          1074.3684889550723,
          1068.7883953846406,
          1079.5692914859885,
          1069.827919387282,
          1073.9470056962555,
          1072.650819584533,
          1082.4189269052663,
          1070.8348892667811,
          1076.8125349786399,
          1063.1520875418666,
          1080.2198880419169,
          1079.149688531001,
          1083.8326858327746,
          1078.4600541485988,
          1071.742513983623,
          1064.4584081453577,
          1071.6472081317677,
          1071.4504488280859,
          1073.1052525629136,
          1069.9348622825596,
          1071.3758178980192,
          1067.6306721083215,
          1068.9737221561215,
          1061.9854810006016,
          1067.2496872728468,
          1064.764188917553,
          1077.5645228007859,
          1067.89640567918,
          1064.105023133389,
          1068.9872330657365,
          1062.45913547587,
          1053.839544020508,
          1065.0908503711212,
          1064.1913685176185,
          1059.773455221858,
          1061.6719522271578,
          1068.3209279184473,
          1070.441436504443,
          1064.1376815070746,
          1063.0807444815505,
          1054.0485340009022,
          1056.9105315544048,
          1055.6113550259972,
          1060.7361164662889,
          1065.2181388651509,
          1057.4355679747814,
          1058.5971001899861,
          1060.954016233222,
          1051.3292247098316,
          1051.7417715748445,
          1062.3240757267724,
          1056.8544239436599,
          1055.3363717589561,
          1053.1097151421823,
          1058.9662663941874,
          1058.874581896329,
          1055.0786539876726,
          1063.176172012075,
          1063.413361972574,
          1053.2922426556115,
          1050.7683690975443,
          1052.0709068920992,
          1059.4850071367196,
          1053.0434192866296,
          1050.9327997422204,
          1048.9964364378634,
          1055.0419254857552,
          1052.7702320222031,
          1052.9572495666466,
          1059.3003697063536,
          1046.7246168896552,
          1048.2490127652209,
          1051.5229160759382,
          1046.6626276414381,
          1057.5061729588729,
          1053.9464608470305,
          1054.5747889826048,
          1047.076721032301,
          1051.9027042331547,
          1045.886018296007,
          1045.272024181818,
          1044.7038717902876,
          1049.2626173387723,
          1050.8284005944754,
          1046.3535444107806,
          1044.8291611412328,
          1052.9391356002168,
          1048.946271414075,
          1046.9561930844413,
          1038.6779848308727,
          1051.3419833951086,
          1051.1242514595565,
          1045.810108307968,
          1038.0089526469994,
          1040.8054926451352,
          1037.6904791324632,
          1034.5658338388796,
          1039.9339615275105,
          1035.4762225414006,
          1043.8551056232532,
          1042.3435544476185,
          1037.5989020182944,
          1045.1322857397984,
          1043.1993777082132,
          1046.9270104468885,
          1047.1148186946216,
          1037.5933303542336,
          1030.676692212252,
          1035.7669096511281,
          1036.9111308053282,
          1042.5256773935475,
          1031.3770731759018,
          1039.3897938840203,
          1033.7661702408684,
          1032.04702503609,
          1037.9691882732996,
          1035.2313401929978,
          1037.4666302552653,
          1029.7589254277918,
          1034.0507208145475,
          1034.637982117764,
          1029.09197218432,
          1028.2431209450235,
          1038.0568249665928,
          1039.8624302336466,
          1037.2919101137568,
          1033.1657973817864,
          1029.6357113945478,
          1032.7162040739518,
          1040.815836874115,
          1030.6289058583568,
          1029.8614237593915,
          1034.149307437619,
          1036.542440202926,
          1032.6960580296857,
          1031.4081537412312,
          1024.5190322230378,
          1025.2115935024815,
          1031.646013570831,
          1030.2927186123375,
          1030.4588382553427,
          1035.8678106901186,
          1029.5435861266637,
          1038.0852915333774,
          1026.047511567184,
          1025.2331658258738,
          1033.8519800529227,
          1016.8169507442984,
          1028.4912260280048,
          1031.1789507945266,
          1029.957928945336,
          1017.9420661444365,
          1021.1909775964382,
          1023.4909793294597,
          1033.4698162488094,
          1032.6017810614237,
          1025.549356745507,
          1024.9298099130003,
          1030.472734113578,
          1023.8996171540145,
          1028.3796716800596,
          1028.5118463033184,
          1019.4264375217361,
          1030.5003254433207,
          1025.415101199153,
          1022.5835624772847,
          1020.0275875234595,
          1020.7362316706053,
          1019.7491411018257,
          1023.3829387804777,
          1028.1228037379608,
          1017.2740590378735,
          1020.464060879576,
          1022.5609666309161,
          1024.7577904945435,
          1021.3032810967695,
          1022.7047384723719,
          1019.4467002634584,
          1020.4579845097253,
          1024.0424656964553,
          1018.4503558984806,
          1021.8290817845148,
          1023.8838666568233,
          1020.9337424520551,
          1017.8165669950856,
          1016.9550653965937,
          1018.8707114572106,
          1014.6263499821215
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          3226.8927015905415,
          2526.03175267414,
          1940.3189272087898,
          1966.5735182715293,
          1618.3392558200148,
          1547.3609586585278,
          1549.8530336100423,
          1449.167929316418,
          1399.7944581693653,
          1349.0533800234125,
          1380.041523047815,
          1295.0368678290117,
          1327.720412501214,
          1372.4935154644263,
          1282.7482144248875,
          1252.9929497112414,
          1214.3050479341991,
          1299.227059323969,
          1209.1796200884016,
          1195.916373342174,
          1214.2548609049545,
          1209.472845217879,
          1206.8324902559154,
          1209.960567592929,
          1214.8191276322345,
          1214.5318673766637,
          1219.9403677833852,
          1196.5079019415323,
          1202.7651903738783,
          1183.7263627864088,
          1201.4504102267458,
          1177.399847415781,
          1218.7088709453171,
          1180.9474445814978,
          1182.388335624773,
          1190.1772401946232,
          1185.0851051356574,
          1177.6933440030016,
          1179.7842081061149,
          1173.6219956310235,
          1189.7343615127847,
          1174.203180417523,
          1183.767715053279,
          1167.34605469827,
          1174.4871395364275,
          1176.2012571553225,
          1170.5672442686923,
          1163.1334622606669,
          1170.4714943667989,
          1170.6035350602974,
          1164.9302977046964,
          1164.6607278288786,
          1157.4971339300541,
          1162.5017115300564,
          1169.070142012788,
          1162.4917851110151,
          1156.5812071591215,
          1161.2502149632012,
          1156.294429946572,
          1178.3628891906064,
          1181.6069356253995,
          1169.4947423359329,
          1150.5506902174168,
          1141.2770908463567,
          1161.293484617623,
          1164.6987339690618,
          1147.6663805386193,
          1147.7744664692655,
          1166.7738113479813,
          1141.0733320206757,
          1143.2498683344272,
          1151.07962183611,
          1142.9104944339658,
          1155.0726739865447,
          1146.9233271153437,
          1149.2413808127694,
          1139.3387594856001,
          1143.3741422767555,
          1148.2775051070282,
          1149.4862949087549,
          1157.950090180759,
          1151.0083139992105,
          1147.2399717047526,
          1141.3205942139014,
          1130.2048347676616,
          1135.301778447205,
          1146.2134141541446,
          1139.7174952973435,
          1140.3082619791928,
          1132.456343402025,
          1130.3931767272643,
          1137.0869060093353,
          1137.4955199436504,
          1132.4247901477436,
          1138.6609459252259,
          1124.8798801933856,
          1126.7174819569943,
          1128.3114782271798,
          1130.9518143168332,
          1132.976371059873,
          1125.6449988475897,
          1130.8749920937014,
          1132.085396371386,
          1124.9453726661977,
          1124.8706778982778,
          1128.9973911172956,
          1121.5336116089331,
          1126.55070137925,
          1127.3203168570315,
          1126.683067112187,
          1127.8825406750911,
          1121.4721429280582,
          1123.5620908062133,
          1115.694806643758,
          1117.3864886106219,
          1112.000877158542,
          1134.7411559605757,
          1110.9558127743462,
          1117.9257144013889,
          1113.4084094230939,
          1116.0674465034872,
          1123.367421531333,
          1124.5121536698541,
          1115.892988714016,
          1118.0816783059784,
          1108.0512399132276,
          1114.5710435511235,
          1116.8119926750958,
          1113.028927139273,
          1111.8593362028382,
          1113.8949772337767,
          1117.4663184337871,
          1110.7133269890387,
          1113.9107826834145,
          1117.8953332864667,
          1113.4457776498193,
          1116.5379553979017,
          1111.1588531549216,
          1105.3444737812454,
          1108.5649585651208,
          1119.6995906642426,
          1109.9768404932904,
          1110.0340674817023,
          1110.2611580147445,
          1114.4489984996146,
          1116.0403750434723,
          1105.2304044470702,
          1123.59911299163,
          1110.3041932951455,
          1111.4039448366534,
          1112.3501855312093,
          1105.2177927993644,
          1109.5128373810971,
          1108.4072736820813,
          1109.2703739194562,
          1109.727836637954,
          1104.7669285601548,
          1103.7405076196158,
          1113.9285069403104,
          1107.190671674087,
          1107.208264632576,
          1106.550308707914,
          1111.3200167114185,
          1102.0148034792805,
          1110.1414612562976,
          1101.141149385864,
          1106.837294225729,
          1101.9892957039867,
          1100.4029823488336,
          1104.6265897045207,
          1105.1728080900969,
          1094.8265516710635,
          1108.241719682111,
          1098.6141710145598,
          1099.8509039028818,
          1097.0221304150557,
          1100.9536453715207,
          1104.0816856737674,
          1097.7555808187224,
          1101.061457151876,
          1107.2505806111515,
          1098.4192882940958,
          1105.4703004422827,
          1103.8101667362487,
          1097.6722509285862,
          1102.5975852646568,
          1107.456651674236,
          1094.3663291158575,
          1102.497522391225,
          1105.353409171463,
          1094.0692274278358,
          1098.400758867587,
          1098.3732930472745,
          1099.431533192355,
          1099.0187556004987,
          1107.1028588637098,
          1096.71155235546,
          1103.8258817897236,
          1093.2292148551076,
          1100.6557198129199,
          1094.9563932141536,
          1090.06477836026,
          1092.9810828956067,
          1091.0726858337537,
          1092.242774940431,
          1093.1141770850304,
          1094.4866471429232,
          1092.9297332733074,
          1094.5676972530923,
          1096.0115774646895,
          1088.81112462226,
          1095.5668357113261,
          1092.8792998025335,
          1096.5500143390393,
          1096.1650532840463,
          1092.5834869635662,
          1091.082695648334,
          1093.798792167252,
          1095.214384205575,
          1088.7291682740358,
          1102.890044927358,
          1099.2639465148447,
          1106.3820356755689,
          1091.8458695244353,
          1091.3443590052887,
          1090.6474902373122,
          1090.390226429537,
          1093.6833187919449,
          1097.535978309229,
          1093.261651485268,
          1092.4963048702016,
          1091.7602209103043,
          1093.8470603647609,
          1100.5657406522585,
          1091.4333253030145,
          1089.707263372072,
          1097.4221622703403,
          1086.0210768946527,
          1091.1292069341225,
          1088.7253365831239,
          1091.4168279421026,
          1090.9272058524039,
          1085.1316309612023,
          1092.6798137070446,
          1092.820685784994,
          1093.0977392049406,
          1084.6094913180518,
          1085.9295067076744,
          1092.9169390464035,
          1084.7503434221946,
          1089.0277662841356,
          1080.257271518252,
          1092.9032746371033,
          1089.2307839249236,
          1085.5794720815136,
          1083.9698208261402,
          1083.4878354375674,
          1087.211641772515,
          1087.5409724463293,
          1084.5580323783242,
          1085.6229637976708,
          1089.5215526188401,
          1091.0540032257697,
          1086.033102301335,
          1093.5304834806825,
          1084.6753646149912,
          1087.3130361351623,
          1085.3139121677298,
          1086.9164339489705,
          1085.714603011776,
          1084.8059984524216,
          1083.4348284054165,
          1084.2605858973566,
          1087.2652279961103,
          1080.383006118643,
          1088.407634826134,
          1080.89436820401,
          1081.9507536489405,
          1082.4988905969974,
          1085.7087905115802,
          1083.0210320270203,
          1083.9179813804956,
          1079.677670243414,
          1084.7972164395005,
          1083.2440404132776,
          1083.6279973438752,
          1092.6773658142029,
          1083.8130839890748,
          1092.0622691507685,
          1099.221801390647,
          1080.9854806089584,
          1088.6342001165922,
          1084.9742112696135,
          1082.1711612790912,
          1083.483177648605,
          1080.738558190749,
          1081.6313252346727,
          1079.3761547846666,
          1082.9002175921066,
          1084.9743131947753,
          1083.876265766579,
          1081.7561890407246,
          1081.751125191122,
          1081.1495839867248,
          1081.2316086767,
          1083.7777674345878,
          1085.83823409615,
          1080.8594755544102,
          1079.8341456341175,
          1078.6030803135027,
          1085.4402994171564,
          1083.2883660848283,
          1083.6630054058521,
          1081.6033071973459,
          1075.4548087723392,
          1078.0690873908882,
          1074.4949733565081,
          1081.930248357644,
          1080.7324099054024,
          1084.088564055607,
          1077.6496185002882,
          1082.2118244006683,
          1078.0508762234936,
          1078.8900299959582,
          1078.8157613603964,
          1081.2493537641226,
          1077.1481682942056,
          1075.5100917659352,
          1079.1829660682226,
          1083.737690578512,
          1076.7422079878386,
          1088.0533157449604,
          1082.8729240948342,
          1073.9045916599957,
          1077.4698358799285,
          1075.4978967042828,
          1073.8997963797672,
          1076.369814398295,
          1078.599018287391,
          1077.72802435747,
          1075.3227230643422,
          1078.386933394991,
          1074.6894073027372,
          1076.9037971137066,
          1077.1455845501036,
          1074.7140727757403,
          1074.8276163777775,
          1074.7611966160846,
          1072.3684495949233,
          1070.7448717006012,
          1077.184133899506,
          1072.4848526089709,
          1073.1304295220307,
          1070.6338019622503,
          1070.69083777979,
          1074.2800505680002,
          1073.6332063217883,
          1073.6640070090439,
          1073.8820590028215,
          1078.7650179774053,
          1078.0936236950447,
          1077.9737765945365,
          1076.9394835752644,
          1076.2584319083132,
          1072.413493858446,
          1072.938598412513,
          1075.504817285685,
          1076.4392891243797,
          1074.483428251344,
          1073.6725671805289,
          1075.123810359082,
          1076.074324444728,
          1074.0811068984437,
          1073.406184057064,
          1080.1894754731443,
          1084.7677889269914,
          1082.3260194033785,
          1073.256594856588,
          1070.646179431757,
          1076.2569507868898,
          1072.6502592042043,
          1069.305873004186,
          1074.6401592137604,
          1071.31658187753,
          1073.3071483415474,
          1071.1827068003763,
          1074.6491908992678,
          1073.9433865697297,
          1071.5960035398678,
          1074.8323216045312
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399
         ],
         "y": [
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013,
          1092.2156825275013
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          399
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"919c2a12-0c50-4420-a460-3af19d2e5da0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"919c2a12-0c50-4420-a460-3af19d2e5da0\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '919c2a12-0c50-4420-a460-3af19d2e5da0',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1473.425104245648, 1454.786106229713, 1443.1135181207467, 1426.0031353246957, 1420.3247609311554, 1410.3661814663437, 1396.018175846067, 1386.4666997229908, 1354.359159008735, 1367.235257274382, 1364.9632447684291, 1366.428855086702, 1349.6690600639788, 1337.5836962724177, 1331.0284475231304, 1330.4567943792533, 1330.3316519679684, 1317.4884391067164, 1310.2244272119228, 1296.7558826777747, 1305.7582581655663, 1296.9664385214057, 1301.7303601913804, 1303.3815733406857, 1290.9000204976298, 1279.5640827038974, 1281.1342449352692, 1292.0880044952814, 1277.4291549385825, 1272.5326191359252, 1263.325710176537, 1268.3205858425265, 1260.8830860383482, 1267.2569585707997, 1252.1845543775144, 1252.4591011581283, 1239.296101235282, 1245.3354661573214, 1235.506771242927, 1240.4451181715037, 1246.7969836320526, 1232.2679028462283, 1229.5748750706916, 1231.3809041447216, 1222.7192856945637, 1230.3166681369607, 1230.2942988195089, 1215.9204735535047, 1227.7154674419114, 1224.5357485370166, 1218.9340623266978, 1221.7410532276497, 1205.0486717369458, 1211.8225887796552, 1204.9378738969367, 1205.5270637005824, 1198.9789877999203, 1198.018064986543, 1194.5692724668122, 1199.8736434700925, 1205.7360480511047, 1208.1765404749615, 1187.2577827351113, 1189.0277683647405, 1192.7990679330712, 1187.8521215952492, 1183.7497880230787, 1189.7391243355771, 1191.2773770886336, 1188.8558585084893, 1186.0490261741074, 1183.5244314416702, 1176.2773646539597, 1180.482328100914, 1173.6365148017658, 1167.3931333038165, 1173.0465811206793, 1172.1893115704347, 1169.3179100133002, 1173.699698415098, 1168.2572439808346, 1166.9889226392918, 1168.2611440869307, 1159.0628451845894, 1164.5433797796145, 1158.8618692046587, 1161.772260856743, 1162.2330300970023, 1160.5965497598443, 1160.0056174618462, 1158.0341496533565, 1157.928902157328, 1158.0398072831158, 1157.190761311441, 1158.7234014737144, 1161.544658519377, 1152.8448413384565, 1150.8663623076247, 1152.3140841236043, 1145.0118834849516, 1147.2013756274125, 1152.1256965620378, 1145.743219161813, 1141.7834891469774, 1143.639859092625, 1143.875262449948, 1139.7145631375952, 1140.2338375675192, 1134.011715274166, 1151.3375000342687, 1138.8755045589235, 1143.2790332256445, 1136.6700194431303, 1125.7735301481116, 1131.6607217584078, 1136.0079929985932, 1131.5825086631492, 1127.3501891783872, 1130.9955442010555, 1132.346220069642, 1130.1677578256201, 1128.411016004894, 1131.7786379596716, 1127.0058272602516, 1129.4099022938155, 1124.788449529323, 1125.4598016610767, 1126.9511139362735, 1124.352080521661, 1115.4710383471636, 1127.452554791873, 1118.8265219551301, 1119.0779633492393, 1125.3924181876787, 1118.252614218844, 1118.0541655374286, 1118.587731397532, 1103.0026668703865, 1122.8149198903477, 1121.8092158019053, 1118.9210834889464, 1111.0759718431987, 1112.8780175624593, 1117.6439944539395, 1105.5622172335572, 1117.4731677116363, 1115.5067793940025, 1108.9097031187337, 1106.3473845474798, 1102.464858093935, 1107.9382865665575, 1101.0049144376942, 1106.995430502118, 1104.0462019120807, 1102.2670009896826, 1102.148129252267, 1104.4138107957642, 1096.0964439916831, 1107.7230707064932, 1104.2334107497854, 1097.4208267912588, 1109.6080809127168, 1102.442580714741, 1096.9948550312845, 1093.766032431392, 1094.330482570112, 1094.2741190082495, 1092.054597936079, 1105.8126088278743, 1092.3681017422643, 1091.9997955132562, 1095.0646532547123, 1093.1633548006066, 1096.0462165742258, 1092.6927237379687, 1097.1055579467554, 1100.5982998422853, 1098.5823134289778, 1098.1943306992712, 1096.1289940515262, 1092.0104197916537, 1093.2293328376436, 1083.0979608505552, 1092.4070309122076, 1085.167947380964, 1081.9990782920358, 1077.7716765249806, 1078.046729602435, 1079.847145596128, 1082.4404206797046, 1080.8808920772133, 1078.088113959576, 1086.343318458069, 1089.0917782719446, 1077.6132219649996, 1079.7424236976099, 1084.013163375166, 1077.5653840855364, 1078.0871832438168, 1076.8475293575798, 1073.596954890332, 1071.7418590826383, 1074.3684889550723, 1068.7883953846406, 1079.5692914859885, 1069.827919387282, 1073.9470056962555, 1072.650819584533, 1082.4189269052663, 1070.8348892667811, 1076.8125349786399, 1063.1520875418666, 1080.2198880419169, 1079.149688531001, 1083.8326858327746, 1078.4600541485988, 1071.742513983623, 1064.4584081453577, 1071.6472081317677, 1071.4504488280859, 1073.1052525629136, 1069.9348622825596, 1071.3758178980192, 1067.6306721083215, 1068.9737221561215, 1061.9854810006016, 1067.2496872728468, 1064.764188917553, 1077.5645228007859, 1067.89640567918, 1064.105023133389, 1068.9872330657365, 1062.45913547587, 1053.839544020508, 1065.0908503711212, 1064.1913685176185, 1059.773455221858, 1061.6719522271578, 1068.3209279184473, 1070.441436504443, 1064.1376815070746, 1063.0807444815505, 1054.0485340009022, 1056.9105315544048, 1055.6113550259972, 1060.7361164662889, 1065.2181388651509, 1057.4355679747814, 1058.5971001899861, 1060.954016233222, 1051.3292247098316, 1051.7417715748445, 1062.3240757267724, 1056.8544239436599, 1055.3363717589561, 1053.1097151421823, 1058.9662663941874, 1058.874581896329, 1055.0786539876726, 1063.176172012075, 1063.413361972574, 1053.2922426556115, 1050.7683690975443, 1052.0709068920992, 1059.4850071367196, 1053.0434192866296, 1050.9327997422204, 1048.9964364378634, 1055.0419254857552, 1052.7702320222031, 1052.9572495666466, 1059.3003697063536, 1046.7246168896552, 1048.2490127652209, 1051.5229160759382, 1046.6626276414381, 1057.5061729588729, 1053.9464608470305, 1054.5747889826048, 1047.076721032301, 1051.9027042331547, 1045.886018296007, 1045.272024181818, 1044.7038717902876, 1049.2626173387723, 1050.8284005944754, 1046.3535444107806, 1044.8291611412328, 1052.9391356002168, 1048.946271414075, 1046.9561930844413, 1038.6779848308727, 1051.3419833951086, 1051.1242514595565, 1045.810108307968, 1038.0089526469994, 1040.8054926451352, 1037.6904791324632, 1034.5658338388796, 1039.9339615275105, 1035.4762225414006, 1043.8551056232532, 1042.3435544476185, 1037.5989020182944, 1045.1322857397984, 1043.1993777082132, 1046.9270104468885, 1047.1148186946216, 1037.5933303542336, 1030.676692212252, 1035.7669096511281, 1036.9111308053282, 1042.5256773935475, 1031.3770731759018, 1039.3897938840203, 1033.7661702408684, 1032.04702503609, 1037.9691882732996, 1035.2313401929978, 1037.4666302552653, 1029.7589254277918, 1034.0507208145475, 1034.637982117764, 1029.09197218432, 1028.2431209450235, 1038.0568249665928, 1039.8624302336466, 1037.2919101137568, 1033.1657973817864, 1029.6357113945478, 1032.7162040739518, 1040.815836874115, 1030.6289058583568, 1029.8614237593915, 1034.149307437619, 1036.542440202926, 1032.6960580296857, 1031.4081537412312, 1024.5190322230378, 1025.2115935024815, 1031.646013570831, 1030.2927186123375, 1030.4588382553427, 1035.8678106901186, 1029.5435861266637, 1038.0852915333774, 1026.047511567184, 1025.2331658258738, 1033.8519800529227, 1016.8169507442984, 1028.4912260280048, 1031.1789507945266, 1029.957928945336, 1017.9420661444365, 1021.1909775964382, 1023.4909793294597, 1033.4698162488094, 1032.6017810614237, 1025.549356745507, 1024.9298099130003, 1030.472734113578, 1023.8996171540145, 1028.3796716800596, 1028.5118463033184, 1019.4264375217361, 1030.5003254433207, 1025.415101199153, 1022.5835624772847, 1020.0275875234595, 1020.7362316706053, 1019.7491411018257, 1023.3829387804777, 1028.1228037379608, 1017.2740590378735, 1020.464060879576, 1022.5609666309161, 1024.7577904945435, 1021.3032810967695, 1022.7047384723719, 1019.4467002634584, 1020.4579845097253, 1024.0424656964553, 1018.4503558984806, 1021.8290817845148, 1023.8838666568233, 1020.9337424520551, 1017.8165669950856, 1016.9550653965937, 1018.8707114572106, 1014.6263499821215]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [3226.8927015905415, 2526.03175267414, 1940.3189272087898, 1966.5735182715293, 1618.3392558200148, 1547.3609586585278, 1549.8530336100423, 1449.167929316418, 1399.7944581693653, 1349.0533800234125, 1380.041523047815, 1295.0368678290117, 1327.720412501214, 1372.4935154644263, 1282.7482144248875, 1252.9929497112414, 1214.3050479341991, 1299.227059323969, 1209.1796200884016, 1195.916373342174, 1214.2548609049545, 1209.472845217879, 1206.8324902559154, 1209.960567592929, 1214.8191276322345, 1214.5318673766637, 1219.9403677833852, 1196.5079019415323, 1202.7651903738783, 1183.7263627864088, 1201.4504102267458, 1177.399847415781, 1218.7088709453171, 1180.9474445814978, 1182.388335624773, 1190.1772401946232, 1185.0851051356574, 1177.6933440030016, 1179.7842081061149, 1173.6219956310235, 1189.7343615127847, 1174.203180417523, 1183.767715053279, 1167.34605469827, 1174.4871395364275, 1176.2012571553225, 1170.5672442686923, 1163.1334622606669, 1170.4714943667989, 1170.6035350602974, 1164.9302977046964, 1164.6607278288786, 1157.4971339300541, 1162.5017115300564, 1169.070142012788, 1162.4917851110151, 1156.5812071591215, 1161.2502149632012, 1156.294429946572, 1178.3628891906064, 1181.6069356253995, 1169.4947423359329, 1150.5506902174168, 1141.2770908463567, 1161.293484617623, 1164.6987339690618, 1147.6663805386193, 1147.7744664692655, 1166.7738113479813, 1141.0733320206757, 1143.2498683344272, 1151.07962183611, 1142.9104944339658, 1155.0726739865447, 1146.9233271153437, 1149.2413808127694, 1139.3387594856001, 1143.3741422767555, 1148.2775051070282, 1149.4862949087549, 1157.950090180759, 1151.0083139992105, 1147.2399717047526, 1141.3205942139014, 1130.2048347676616, 1135.301778447205, 1146.2134141541446, 1139.7174952973435, 1140.3082619791928, 1132.456343402025, 1130.3931767272643, 1137.0869060093353, 1137.4955199436504, 1132.4247901477436, 1138.6609459252259, 1124.8798801933856, 1126.7174819569943, 1128.3114782271798, 1130.9518143168332, 1132.976371059873, 1125.6449988475897, 1130.8749920937014, 1132.085396371386, 1124.9453726661977, 1124.8706778982778, 1128.9973911172956, 1121.5336116089331, 1126.55070137925, 1127.3203168570315, 1126.683067112187, 1127.8825406750911, 1121.4721429280582, 1123.5620908062133, 1115.694806643758, 1117.3864886106219, 1112.000877158542, 1134.7411559605757, 1110.9558127743462, 1117.9257144013889, 1113.4084094230939, 1116.0674465034872, 1123.367421531333, 1124.5121536698541, 1115.892988714016, 1118.0816783059784, 1108.0512399132276, 1114.5710435511235, 1116.8119926750958, 1113.028927139273, 1111.8593362028382, 1113.8949772337767, 1117.4663184337871, 1110.7133269890387, 1113.9107826834145, 1117.8953332864667, 1113.4457776498193, 1116.5379553979017, 1111.1588531549216, 1105.3444737812454, 1108.5649585651208, 1119.6995906642426, 1109.9768404932904, 1110.0340674817023, 1110.2611580147445, 1114.4489984996146, 1116.0403750434723, 1105.2304044470702, 1123.59911299163, 1110.3041932951455, 1111.4039448366534, 1112.3501855312093, 1105.2177927993644, 1109.5128373810971, 1108.4072736820813, 1109.2703739194562, 1109.727836637954, 1104.7669285601548, 1103.7405076196158, 1113.9285069403104, 1107.190671674087, 1107.208264632576, 1106.550308707914, 1111.3200167114185, 1102.0148034792805, 1110.1414612562976, 1101.141149385864, 1106.837294225729, 1101.9892957039867, 1100.4029823488336, 1104.6265897045207, 1105.1728080900969, 1094.8265516710635, 1108.241719682111, 1098.6141710145598, 1099.8509039028818, 1097.0221304150557, 1100.9536453715207, 1104.0816856737674, 1097.7555808187224, 1101.061457151876, 1107.2505806111515, 1098.4192882940958, 1105.4703004422827, 1103.8101667362487, 1097.6722509285862, 1102.5975852646568, 1107.456651674236, 1094.3663291158575, 1102.497522391225, 1105.353409171463, 1094.0692274278358, 1098.400758867587, 1098.3732930472745, 1099.431533192355, 1099.0187556004987, 1107.1028588637098, 1096.71155235546, 1103.8258817897236, 1093.2292148551076, 1100.6557198129199, 1094.9563932141536, 1090.06477836026, 1092.9810828956067, 1091.0726858337537, 1092.242774940431, 1093.1141770850304, 1094.4866471429232, 1092.9297332733074, 1094.5676972530923, 1096.0115774646895, 1088.81112462226, 1095.5668357113261, 1092.8792998025335, 1096.5500143390393, 1096.1650532840463, 1092.5834869635662, 1091.082695648334, 1093.798792167252, 1095.214384205575, 1088.7291682740358, 1102.890044927358, 1099.2639465148447, 1106.3820356755689, 1091.8458695244353, 1091.3443590052887, 1090.6474902373122, 1090.390226429537, 1093.6833187919449, 1097.535978309229, 1093.261651485268, 1092.4963048702016, 1091.7602209103043, 1093.8470603647609, 1100.5657406522585, 1091.4333253030145, 1089.707263372072, 1097.4221622703403, 1086.0210768946527, 1091.1292069341225, 1088.7253365831239, 1091.4168279421026, 1090.9272058524039, 1085.1316309612023, 1092.6798137070446, 1092.820685784994, 1093.0977392049406, 1084.6094913180518, 1085.9295067076744, 1092.9169390464035, 1084.7503434221946, 1089.0277662841356, 1080.257271518252, 1092.9032746371033, 1089.2307839249236, 1085.5794720815136, 1083.9698208261402, 1083.4878354375674, 1087.211641772515, 1087.5409724463293, 1084.5580323783242, 1085.6229637976708, 1089.5215526188401, 1091.0540032257697, 1086.033102301335, 1093.5304834806825, 1084.6753646149912, 1087.3130361351623, 1085.3139121677298, 1086.9164339489705, 1085.714603011776, 1084.8059984524216, 1083.4348284054165, 1084.2605858973566, 1087.2652279961103, 1080.383006118643, 1088.407634826134, 1080.89436820401, 1081.9507536489405, 1082.4988905969974, 1085.7087905115802, 1083.0210320270203, 1083.9179813804956, 1079.677670243414, 1084.7972164395005, 1083.2440404132776, 1083.6279973438752, 1092.6773658142029, 1083.8130839890748, 1092.0622691507685, 1099.221801390647, 1080.9854806089584, 1088.6342001165922, 1084.9742112696135, 1082.1711612790912, 1083.483177648605, 1080.738558190749, 1081.6313252346727, 1079.3761547846666, 1082.9002175921066, 1084.9743131947753, 1083.876265766579, 1081.7561890407246, 1081.751125191122, 1081.1495839867248, 1081.2316086767, 1083.7777674345878, 1085.83823409615, 1080.8594755544102, 1079.8341456341175, 1078.6030803135027, 1085.4402994171564, 1083.2883660848283, 1083.6630054058521, 1081.6033071973459, 1075.4548087723392, 1078.0690873908882, 1074.4949733565081, 1081.930248357644, 1080.7324099054024, 1084.088564055607, 1077.6496185002882, 1082.2118244006683, 1078.0508762234936, 1078.8900299959582, 1078.8157613603964, 1081.2493537641226, 1077.1481682942056, 1075.5100917659352, 1079.1829660682226, 1083.737690578512, 1076.7422079878386, 1088.0533157449604, 1082.8729240948342, 1073.9045916599957, 1077.4698358799285, 1075.4978967042828, 1073.8997963797672, 1076.369814398295, 1078.599018287391, 1077.72802435747, 1075.3227230643422, 1078.386933394991, 1074.6894073027372, 1076.9037971137066, 1077.1455845501036, 1074.7140727757403, 1074.8276163777775, 1074.7611966160846, 1072.3684495949233, 1070.7448717006012, 1077.184133899506, 1072.4848526089709, 1073.1304295220307, 1070.6338019622503, 1070.69083777979, 1074.2800505680002, 1073.6332063217883, 1073.6640070090439, 1073.8820590028215, 1078.7650179774053, 1078.0936236950447, 1077.9737765945365, 1076.9394835752644, 1076.2584319083132, 1072.413493858446, 1072.938598412513, 1075.504817285685, 1076.4392891243797, 1074.483428251344, 1073.6725671805289, 1075.123810359082, 1076.074324444728, 1074.0811068984437, 1073.406184057064, 1080.1894754731443, 1084.7677889269914, 1082.3260194033785, 1073.256594856588, 1070.646179431757, 1076.2569507868898, 1072.6502592042043, 1069.305873004186, 1074.6401592137604, 1071.31658187753, 1073.3071483415474, 1071.1827068003763, 1074.6491908992678, 1073.9433865697297, 1071.5960035398678, 1074.8323216045312]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399], \"y\": [1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013, 1092.2156825275013]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 399], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('919c2a12-0c50-4420-a460-3af19d2e5da0');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.31% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1075   : Mean absolute error \n",
      "\n",
      "9.25% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(history.history[\"val_loss\"][-1])\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(60)\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    \n",
    "    keras.layers.Dense(1024, input_dim = X_train.shape[1]), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    \n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(512),  \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    \n",
    "    keras.layers.Dense(units=256), \n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.01),\n",
    "\n",
    "    \n",
    "    keras.layers.Dense(units=128),\n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dropout(0.05),\n",
    "    keras.layers.Dense(units=1, activation=\"linear\"),\n",
    "\n",
    "\n",
    "\n",
    "],name=\"Callbacks_stop\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19948 samples, validate on 4987 samples\n",
      "Epoch 1/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 11178.8039\n",
      "Epoch 00001: val_loss improved from inf to 10212.57571, saving model to Weights\\Weights-001--10212.57571.hdf5\n",
      "19948/19948 [==============================] - 3s 154us/sample - loss: 11171.3879 - val_loss: 10212.5757\n",
      "Epoch 2/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 11019.9533\n",
      "Epoch 00002: val_loss improved from 10212.57571 to 7787.54832, saving model to Weights\\Weights-002--7787.54832.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 10988.2913 - val_loss: 7787.5483\n",
      "Epoch 3/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 10378.0419\n",
      "Epoch 00003: val_loss improved from 7787.54832 to 6571.05155, saving model to Weights\\Weights-003--6571.05155.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 10369.8656 - val_loss: 6571.0515\n",
      "Epoch 4/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 8967.4902\n",
      "Epoch 00004: val_loss did not improve from 6571.05155\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 8944.5364 - val_loss: 7547.1617\n",
      "Epoch 5/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 6463.6545\n",
      "Epoch 00005: val_loss did not improve from 6571.05155\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 6344.1931 - val_loss: 11420.9925\n",
      "Epoch 6/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 3944.4921\n",
      "Epoch 00006: val_loss did not improve from 6571.05155\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 3729.9947 - val_loss: 12385.4399\n",
      "Epoch 7/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1824.0914\n",
      "Epoch 00007: val_loss did not improve from 6571.05155\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1822.4908 - val_loss: 9926.7253\n",
      "Epoch 8/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1589.9488\n",
      "Epoch 00008: val_loss did not improve from 6571.05155\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1585.9134 - val_loss: 6605.1790\n",
      "Epoch 9/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1526.5669\n",
      "Epoch 00009: val_loss improved from 6571.05155 to 4885.86146, saving model to Weights\\Weights-009--4885.86146.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1530.4039 - val_loss: 4885.8615\n",
      "Epoch 10/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1511.7391\n",
      "Epoch 00010: val_loss improved from 4885.86146 to 4488.94329, saving model to Weights\\Weights-010--4488.94329.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1510.2467 - val_loss: 4488.9433\n",
      "Epoch 11/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1472.2321\n",
      "Epoch 00011: val_loss improved from 4488.94329 to 3084.50901, saving model to Weights\\Weights-011--3084.50901.hdf5\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1478.9786 - val_loss: 3084.5090\n",
      "Epoch 12/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1463.0490\n",
      "Epoch 00012: val_loss improved from 3084.50901 to 2715.49508, saving model to Weights\\Weights-012--2715.49508.hdf5\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 1463.9528 - val_loss: 2715.4951\n",
      "Epoch 13/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1457.6768\n",
      "Epoch 00013: val_loss improved from 2715.49508 to 2335.57792, saving model to Weights\\Weights-013--2335.57792.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1459.4709 - val_loss: 2335.5779\n",
      "Epoch 14/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1427.7257\n",
      "Epoch 00014: val_loss improved from 2335.57792 to 1942.25475, saving model to Weights\\Weights-014--1942.25475.hdf5\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1433.2028 - val_loss: 1942.2547\n",
      "Epoch 15/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1403.3413\n",
      "Epoch 00015: val_loss improved from 1942.25475 to 1560.23057, saving model to Weights\\Weights-015--1560.23057.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1403.0045 - val_loss: 1560.2306\n",
      "Epoch 16/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1408.0311\n",
      "Epoch 00016: val_loss improved from 1560.23057 to 1533.34403, saving model to Weights\\Weights-016--1533.34403.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1406.8128 - val_loss: 1533.3440\n",
      "Epoch 17/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1391.9938\n",
      "Epoch 00017: val_loss improved from 1533.34403 to 1391.92707, saving model to Weights\\Weights-017--1391.92707.hdf5\n",
      "19948/19948 [==============================] - 1s 48us/sample - loss: 1389.3272 - val_loss: 1391.9271\n",
      "Epoch 18/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1368.9393\n",
      "Epoch 00018: val_loss improved from 1391.92707 to 1391.90611, saving model to Weights\\Weights-018--1391.90611.hdf5\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1371.4300 - val_loss: 1391.9061\n",
      "Epoch 19/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1356.9847\n",
      "Epoch 00019: val_loss improved from 1391.90611 to 1283.81649, saving model to Weights\\Weights-019--1283.81649.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1357.5007 - val_loss: 1283.8165\n",
      "Epoch 20/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1352.1751\n",
      "Epoch 00020: val_loss improved from 1283.81649 to 1257.55545, saving model to Weights\\Weights-020--1257.55545.hdf5\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1359.5355 - val_loss: 1257.5554\n",
      "Epoch 21/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1345.5756\n",
      "Epoch 00021: val_loss did not improve from 1257.55545\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1346.5418 - val_loss: 1343.4926\n",
      "Epoch 22/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1353.5737\n",
      "Epoch 00022: val_loss improved from 1257.55545 to 1249.15660, saving model to Weights\\Weights-022--1249.15660.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1359.6629 - val_loss: 1249.1566\n",
      "Epoch 23/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1333.3883\n",
      "Epoch 00023: val_loss did not improve from 1249.15660\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1331.1571 - val_loss: 1273.9318\n",
      "Epoch 24/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1321.0058\n",
      "Epoch 00024: val_loss improved from 1249.15660 to 1238.52807, saving model to Weights\\Weights-024--1238.52807.hdf5\n",
      "19948/19948 [==============================] - 1s 42us/sample - loss: 1325.8146 - val_loss: 1238.5281\n",
      "Epoch 25/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1307.1766\n",
      "Epoch 00025: val_loss did not improve from 1238.52807\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1308.7267 - val_loss: 1249.1370\n",
      "Epoch 26/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1306.2096\n",
      "Epoch 00026: val_loss improved from 1238.52807 to 1222.93025, saving model to Weights\\Weights-026--1222.93025.hdf5\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 1308.0340 - val_loss: 1222.9302\n",
      "Epoch 27/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1304.8438\n",
      "Epoch 00027: val_loss did not improve from 1222.93025\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1305.9603 - val_loss: 1250.9406\n",
      "Epoch 28/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1307.4344\n",
      "Epoch 00028: val_loss did not improve from 1222.93025\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1304.8266 - val_loss: 1259.9327\n",
      "Epoch 29/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1295.5251\n",
      "Epoch 00029: val_loss improved from 1222.93025 to 1222.08511, saving model to Weights\\Weights-029--1222.08511.hdf5\n",
      "19948/19948 [==============================] - 1s 54us/sample - loss: 1298.1237 - val_loss: 1222.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1302.3485\n",
      "Epoch 00030: val_loss improved from 1222.08511 to 1220.06825, saving model to Weights\\Weights-030--1220.06825.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1301.9109 - val_loss: 1220.0683\n",
      "Epoch 31/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1291.5311\n",
      "Epoch 00031: val_loss improved from 1220.06825 to 1211.83694, saving model to Weights\\Weights-031--1211.83694.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1289.2417 - val_loss: 1211.8369\n",
      "Epoch 32/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1284.3923\n",
      "Epoch 00032: val_loss improved from 1211.83694 to 1205.20489, saving model to Weights\\Weights-032--1205.20489.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1283.5300 - val_loss: 1205.2049\n",
      "Epoch 33/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1281.3039\n",
      "Epoch 00033: val_loss did not improve from 1205.20489\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1281.3064 - val_loss: 1206.0696\n",
      "Epoch 34/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1280.4592\n",
      "Epoch 00034: val_loss improved from 1205.20489 to 1198.60681, saving model to Weights\\Weights-034--1198.60681.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1278.2896 - val_loss: 1198.6068\n",
      "Epoch 35/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1276.2366\n",
      "Epoch 00035: val_loss did not improve from 1198.60681\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1278.9223 - val_loss: 1213.5471\n",
      "Epoch 36/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1263.6879\n",
      "Epoch 00036: val_loss did not improve from 1198.60681\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1263.2573 - val_loss: 1226.6462\n",
      "Epoch 37/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1278.5010\n",
      "Epoch 00037: val_loss did not improve from 1198.60681\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1274.4841 - val_loss: 1200.5661\n",
      "Epoch 38/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1272.2124\n",
      "Epoch 00038: val_loss did not improve from 1198.60681\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1276.8268 - val_loss: 1206.3319\n",
      "Epoch 39/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1267.1988\n",
      "Epoch 00039: val_loss improved from 1198.60681 to 1193.68167, saving model to Weights\\Weights-039--1193.68167.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1267.5156 - val_loss: 1193.6817\n",
      "Epoch 40/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1263.0851\n",
      "Epoch 00040: val_loss improved from 1193.68167 to 1191.05509, saving model to Weights\\Weights-040--1191.05509.hdf5\n",
      "19948/19948 [==============================] - 1s 41us/sample - loss: 1261.0284 - val_loss: 1191.0551\n",
      "Epoch 41/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1257.5541\n",
      "Epoch 00041: val_loss improved from 1191.05509 to 1185.61460, saving model to Weights\\Weights-041--1185.61460.hdf5\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1258.2431 - val_loss: 1185.6146\n",
      "Epoch 42/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1258.2737\n",
      "Epoch 00042: val_loss improved from 1185.61460 to 1185.10492, saving model to Weights\\Weights-042--1185.10492.hdf5\n",
      "19948/19948 [==============================] - 1s 57us/sample - loss: 1258.5703 - val_loss: 1185.1049\n",
      "Epoch 43/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1247.1500\n",
      "Epoch 00043: val_loss did not improve from 1185.10492\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1249.5797 - val_loss: 1195.6123\n",
      "Epoch 44/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1257.8581\n",
      "Epoch 00044: val_loss improved from 1185.10492 to 1178.89239, saving model to Weights\\Weights-044--1178.89239.hdf5\n",
      "19948/19948 [==============================] - 1s 51us/sample - loss: 1258.8403 - val_loss: 1178.8924\n",
      "Epoch 45/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1242.0842\n",
      "Epoch 00045: val_loss did not improve from 1178.89239\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1243.0978 - val_loss: 1190.6616\n",
      "Epoch 46/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1244.1739 ETA: 0s - loss:\n",
      "Epoch 00046: val_loss did not improve from 1178.89239\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1244.6020 - val_loss: 1188.8722\n",
      "Epoch 47/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1225.9845\n",
      "Epoch 00047: val_loss did not improve from 1178.89239\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1227.2027 - val_loss: 1185.9437\n",
      "Epoch 48/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1241.7392\n",
      "Epoch 00048: val_loss did not improve from 1178.89239\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1241.6998 - val_loss: 1180.6641\n",
      "Epoch 49/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1233.8193\n",
      "Epoch 00049: val_loss improved from 1178.89239 to 1176.85858, saving model to Weights\\Weights-049--1176.85858.hdf5\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 1231.2809 - val_loss: 1176.8586\n",
      "Epoch 50/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1231.4715\n",
      "Epoch 00050: val_loss improved from 1176.85858 to 1170.07663, saving model to Weights\\Weights-050--1170.07663.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1233.2898 - val_loss: 1170.0766\n",
      "Epoch 51/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1231.4351\n",
      "Epoch 00051: val_loss did not improve from 1170.07663\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1232.9248 - val_loss: 1182.5096\n",
      "Epoch 52/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1231.1768\n",
      "Epoch 00052: val_loss did not improve from 1170.07663\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1231.3112 - val_loss: 1187.1985\n",
      "Epoch 53/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1217.5402\n",
      "Epoch 00053: val_loss did not improve from 1170.07663\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1225.9195 - val_loss: 1193.4852\n",
      "Epoch 54/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1214.1085\n",
      "Epoch 00054: val_loss did not improve from 1170.07663\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1219.5397 - val_loss: 1182.7333\n",
      "Epoch 55/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1223.2598\n",
      "Epoch 00055: val_loss improved from 1170.07663 to 1169.19540, saving model to Weights\\Weights-055--1169.19540.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1224.6360 - val_loss: 1169.1954\n",
      "Epoch 56/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1227.0150\n",
      "Epoch 00056: val_loss did not improve from 1169.19540\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1228.0749 - val_loss: 1201.3073\n",
      "Epoch 57/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1225.1640\n",
      "Epoch 00057: val_loss improved from 1169.19540 to 1163.29106, saving model to Weights\\Weights-057--1163.29106.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1226.4206 - val_loss: 1163.2911\n",
      "Epoch 58/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1211.8795\n",
      "Epoch 00058: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1212.5228 - val_loss: 1171.4304\n",
      "Epoch 59/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1210.5429\n",
      "Epoch 00059: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1209.8515 - val_loss: 1167.9389\n",
      "Epoch 60/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1223.8503\n",
      "Epoch 00060: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1223.2185 - val_loss: 1167.4191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1206.7843\n",
      "Epoch 00061: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1210.7726 - val_loss: 1172.5246\n",
      "Epoch 62/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1208.3193\n",
      "Epoch 00062: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1212.2413 - val_loss: 1163.2916\n",
      "Epoch 63/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1215.4892\n",
      "Epoch 00063: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1212.0075 - val_loss: 1169.0116\n",
      "Epoch 64/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1208.5356\n",
      "Epoch 00064: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1208.2433 - val_loss: 1167.0751\n",
      "Epoch 65/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1214.5865\n",
      "Epoch 00065: val_loss did not improve from 1163.29106\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1207.5731 - val_loss: 1169.4974\n",
      "Epoch 66/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1203.0513\n",
      "Epoch 00066: val_loss improved from 1163.29106 to 1160.09587, saving model to Weights\\Weights-066--1160.09587.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1205.8652 - val_loss: 1160.0959\n",
      "Epoch 67/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1191.8417\n",
      "Epoch 00067: val_loss improved from 1160.09587 to 1154.33589, saving model to Weights\\Weights-067--1154.33589.hdf5\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 1194.4083 - val_loss: 1154.3359\n",
      "Epoch 68/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1205.7636\n",
      "Epoch 00068: val_loss did not improve from 1154.33589\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1199.4111 - val_loss: 1177.3461\n",
      "Epoch 69/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1197.3631\n",
      "Epoch 00069: val_loss did not improve from 1154.33589\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1196.0367 - val_loss: 1163.2173\n",
      "Epoch 70/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1192.9347\n",
      "Epoch 00070: val_loss did not improve from 1154.33589\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 1190.8784 - val_loss: 1171.3177\n",
      "Epoch 71/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1193.3201\n",
      "Epoch 00071: val_loss improved from 1154.33589 to 1149.06668, saving model to Weights\\Weights-071--1149.06668.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1192.6252 - val_loss: 1149.0667\n",
      "Epoch 72/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1188.1681\n",
      "Epoch 00072: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1188.3912 - val_loss: 1165.8869\n",
      "Epoch 73/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1196.9444\n",
      "Epoch 00073: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 1s 42us/sample - loss: 1196.6469 - val_loss: 1152.0677\n",
      "Epoch 74/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1181.9253\n",
      "Epoch 00074: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1185.5963 - val_loss: 1151.3972\n",
      "Epoch 75/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1183.6562\n",
      "Epoch 00075: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1186.5589 - val_loss: 1161.8524\n",
      "Epoch 76/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1186.3137\n",
      "Epoch 00076: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1186.9709 - val_loss: 1162.7438\n",
      "Epoch 77/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1179.0249\n",
      "Epoch 00077: val_loss did not improve from 1149.06668\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1180.1611 - val_loss: 1162.2467\n",
      "Epoch 78/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1188.6361\n",
      "Epoch 00078: val_loss improved from 1149.06668 to 1148.93623, saving model to Weights\\Weights-078--1148.93623.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1187.3879 - val_loss: 1148.9362\n",
      "Epoch 79/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1177.3972\n",
      "Epoch 00079: val_loss did not improve from 1148.93623\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1174.9011 - val_loss: 1156.4446\n",
      "Epoch 80/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1179.6796\n",
      "Epoch 00080: val_loss did not improve from 1148.93623\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1181.7504 - val_loss: 1167.4868\n",
      "Epoch 81/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1178.8761\n",
      "Epoch 00081: val_loss improved from 1148.93623 to 1145.28765, saving model to Weights\\Weights-081--1145.28765.hdf5\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1176.7389 - val_loss: 1145.2876\n",
      "Epoch 82/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1169.4513\n",
      "Epoch 00082: val_loss improved from 1145.28765 to 1144.12960, saving model to Weights\\Weights-082--1144.12960.hdf5\n",
      "19948/19948 [==============================] - 1s 44us/sample - loss: 1173.9629 - val_loss: 1144.1296\n",
      "Epoch 83/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1174.6803\n",
      "Epoch 00083: val_loss improved from 1144.12960 to 1136.37474, saving model to Weights\\Weights-083--1136.37474.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1176.3904 - val_loss: 1136.3747\n",
      "Epoch 84/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1169.2142\n",
      "Epoch 00084: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1168.2845 - val_loss: 1158.4674\n",
      "Epoch 85/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1168.2000\n",
      "Epoch 00085: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1169.0819 - val_loss: 1147.3642\n",
      "Epoch 86/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1165.4056\n",
      "Epoch 00086: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1169.2248 - val_loss: 1141.2628\n",
      "Epoch 87/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1175.7276\n",
      "Epoch 00087: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1173.8353 - val_loss: 1140.8812\n",
      "Epoch 88/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1172.9104\n",
      "Epoch 00088: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1173.5108 - val_loss: 1144.4910\n",
      "Epoch 89/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1164.2113\n",
      "Epoch 00089: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1163.1926 - val_loss: 1139.0003\n",
      "Epoch 90/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1159.1796\n",
      "Epoch 00090: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1162.2173 - val_loss: 1161.8138\n",
      "Epoch 91/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1175.67 - ETA: 0s - loss: 1175.6072\n",
      "Epoch 00091: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1170.6501 - val_loss: 1168.6338\n",
      "Epoch 92/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1176.2674\n",
      "Epoch 00092: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1171.0108 - val_loss: 1164.2797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1170.5467\n",
      "Epoch 00093: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1168.3901 - val_loss: 1149.7063\n",
      "Epoch 94/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1157.9013\n",
      "Epoch 00094: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1160.2907 - val_loss: 1142.1767\n",
      "Epoch 95/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1161.8297\n",
      "Epoch 00095: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1162.1122 - val_loss: 1146.4462\n",
      "Epoch 96/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1158.6322\n",
      "Epoch 00096: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1157.2077 - val_loss: 1142.1643\n",
      "Epoch 97/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1160.3958\n",
      "Epoch 00097: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1164.1265 - val_loss: 1152.8611\n",
      "Epoch 98/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1159.3756\n",
      "Epoch 00098: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1160.2583 - val_loss: 1150.0568\n",
      "Epoch 99/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1160.6306\n",
      "Epoch 00099: val_loss did not improve from 1136.37474\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1159.3282 - val_loss: 1137.7878\n",
      "Epoch 100/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1154.4270\n",
      "Epoch 00100: val_loss improved from 1136.37474 to 1131.84891, saving model to Weights\\Weights-100--1131.84891.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1155.4014 - val_loss: 1131.8489\n",
      "Epoch 101/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1159.0637\n",
      "Epoch 00101: val_loss improved from 1131.84891 to 1130.49617, saving model to Weights\\Weights-101--1130.49617.hdf5\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1159.6700 - val_loss: 1130.4962\n",
      "Epoch 102/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1152.2367\n",
      "Epoch 00102: val_loss did not improve from 1130.49617\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1149.1549 - val_loss: 1139.8263\n",
      "Epoch 103/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1151.7121\n",
      "Epoch 00103: val_loss did not improve from 1130.49617\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1151.1958 - val_loss: 1140.8905\n",
      "Epoch 104/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1154.4090\n",
      "Epoch 00104: val_loss improved from 1130.49617 to 1127.82280, saving model to Weights\\Weights-104--1127.82280.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1151.0324 - val_loss: 1127.8228\n",
      "Epoch 105/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1147.2896\n",
      "Epoch 00105: val_loss improved from 1127.82280 to 1127.21309, saving model to Weights\\Weights-105--1127.21309.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1147.4041 - val_loss: 1127.2131\n",
      "Epoch 106/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1150.0092\n",
      "Epoch 00106: val_loss did not improve from 1127.21309\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1145.5732 - val_loss: 1131.6641\n",
      "Epoch 107/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1140.7716\n",
      "Epoch 00107: val_loss did not improve from 1127.21309\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1147.6679 - val_loss: 1140.8941\n",
      "Epoch 108/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1153.8513\n",
      "Epoch 00108: val_loss did not improve from 1127.21309\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1148.9008 - val_loss: 1132.3219\n",
      "Epoch 109/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1145.2742\n",
      "Epoch 00109: val_loss did not improve from 1127.21309\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1144.8804 - val_loss: 1141.0794\n",
      "Epoch 110/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1144.1978\n",
      "Epoch 00110: val_loss did not improve from 1127.21309\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1143.5486 - val_loss: 1127.8423\n",
      "Epoch 111/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1141.1609\n",
      "Epoch 00111: val_loss improved from 1127.21309 to 1120.76212, saving model to Weights\\Weights-111--1120.76212.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1144.3306 - val_loss: 1120.7621\n",
      "Epoch 112/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1148.1725\n",
      "Epoch 00112: val_loss did not improve from 1120.76212\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1146.6593 - val_loss: 1136.3703\n",
      "Epoch 113/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1145.8783\n",
      "Epoch 00113: val_loss did not improve from 1120.76212\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1147.7349 - val_loss: 1128.6569\n",
      "Epoch 114/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1137.9361\n",
      "Epoch 00114: val_loss did not improve from 1120.76212\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1137.8736 - val_loss: 1131.4713\n",
      "Epoch 115/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1140.5191\n",
      "Epoch 00115: val_loss did not improve from 1120.76212\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1141.7457 - val_loss: 1133.4345\n",
      "Epoch 116/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1135.0109\n",
      "Epoch 00116: val_loss did not improve from 1120.76212\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1143.8004 - val_loss: 1122.5901\n",
      "Epoch 117/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1147.0626\n",
      "Epoch 00117: val_loss improved from 1120.76212 to 1120.27456, saving model to Weights\\Weights-117--1120.27456.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1145.6419 - val_loss: 1120.2746\n",
      "Epoch 118/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1131.9701\n",
      "Epoch 00118: val_loss did not improve from 1120.27456\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1135.2810 - val_loss: 1126.3801\n",
      "Epoch 119/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1131.2542\n",
      "Epoch 00119: val_loss did not improve from 1120.27456\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1133.2909 - val_loss: 1127.8652\n",
      "Epoch 120/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1143.8833\n",
      "Epoch 00120: val_loss did not improve from 1120.27456\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1142.5516 - val_loss: 1125.2782\n",
      "Epoch 121/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1138.8755\n",
      "Epoch 00121: val_loss did not improve from 1120.27456\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1139.5683 - val_loss: 1128.2894\n",
      "Epoch 122/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1133.7742\n",
      "Epoch 00122: val_loss did not improve from 1120.27456\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1133.7296 - val_loss: 1127.7512\n",
      "Epoch 123/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1135.4132\n",
      "Epoch 00123: val_loss improved from 1120.27456 to 1118.39590, saving model to Weights\\Weights-123--1118.39590.hdf5\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1137.5462 - val_loss: 1118.3959\n",
      "Epoch 124/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1130.6687\n",
      "Epoch 00124: val_loss did not improve from 1118.39590\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1128.1186 - val_loss: 1124.1053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1136.2434\n",
      "Epoch 00125: val_loss improved from 1118.39590 to 1117.43091, saving model to Weights\\Weights-125--1117.43091.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1132.7100 - val_loss: 1117.4309\n",
      "Epoch 126/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1126.6963\n",
      "Epoch 00126: val_loss did not improve from 1117.43091\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1127.1259 - val_loss: 1122.8788\n",
      "Epoch 127/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1130.7035\n",
      "Epoch 00127: val_loss did not improve from 1117.43091\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1127.8761 - val_loss: 1126.8128\n",
      "Epoch 128/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1131.0003\n",
      "Epoch 00128: val_loss did not improve from 1117.43091\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1132.4583 - val_loss: 1119.8262\n",
      "Epoch 129/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1136.0961\n",
      "Epoch 00129: val_loss improved from 1117.43091 to 1113.39387, saving model to Weights\\Weights-129--1113.39387.hdf5\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 1131.7528 - val_loss: 1113.3939\n",
      "Epoch 130/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1127.1982\n",
      "Epoch 00130: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1128.4278 - val_loss: 1122.3735\n",
      "Epoch 131/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1131.3792\n",
      "Epoch 00131: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1132.5829 - val_loss: 1115.8288\n",
      "Epoch 132/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1130.6656\n",
      "Epoch 00132: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1131.7940 - val_loss: 1132.7294\n",
      "Epoch 133/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.8276\n",
      "Epoch 00133: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1120.8534 - val_loss: 1119.4132\n",
      "Epoch 134/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.5749\n",
      "Epoch 00134: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1121.8188 - val_loss: 1120.5984\n",
      "Epoch 135/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1124.2325\n",
      "Epoch 00135: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1123.0610 - val_loss: 1117.8890\n",
      "Epoch 136/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1113.1574 ETA: 0s - loss: 110\n",
      "Epoch 00136: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1113.8385 - val_loss: 1117.2657\n",
      "Epoch 137/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1131.3997\n",
      "Epoch 00137: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1128.2254 - val_loss: 1126.2950\n",
      "Epoch 138/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1118.8739\n",
      "Epoch 00138: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1119.3997 - val_loss: 1123.6536\n",
      "Epoch 139/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1128.3671\n",
      "Epoch 00139: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1128.5206 - val_loss: 1116.7634\n",
      "Epoch 140/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1111.4887\n",
      "Epoch 00140: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1110.6954 - val_loss: 1115.2602\n",
      "Epoch 141/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1128.9524\n",
      "Epoch 00141: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1128.8129 - val_loss: 1117.3295\n",
      "Epoch 142/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1117.6008\n",
      "Epoch 00142: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1119.4224 - val_loss: 1128.9985\n",
      "Epoch 143/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.8750\n",
      "Epoch 00143: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1121.5123 - val_loss: 1115.6550\n",
      "Epoch 144/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1117.4997\n",
      "Epoch 00144: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1117.2360 - val_loss: 1126.2292\n",
      "Epoch 145/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1118.4251\n",
      "Epoch 00145: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1119.9043 - val_loss: 1121.2937\n",
      "Epoch 146/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1118.8770\n",
      "Epoch 00146: val_loss did not improve from 1113.39387\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1118.2355 - val_loss: 1115.7381\n",
      "Epoch 147/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1116.0305\n",
      "Epoch 00147: val_loss improved from 1113.39387 to 1106.46744, saving model to Weights\\Weights-147--1106.46744.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1115.1927 - val_loss: 1106.4674\n",
      "Epoch 148/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1109.5774\n",
      "Epoch 00148: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1110.9112 - val_loss: 1118.5557\n",
      "Epoch 149/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1107.3673\n",
      "Epoch 00149: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1109.5113 - val_loss: 1109.4610\n",
      "Epoch 150/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1116.5930\n",
      "Epoch 00150: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1114.4021 - val_loss: 1109.4842\n",
      "Epoch 151/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1121.0493\n",
      "Epoch 00151: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1123.0278 - val_loss: 1119.9273\n",
      "Epoch 152/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1110.9655\n",
      "Epoch 00152: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1112.0035 - val_loss: 1108.1630\n",
      "Epoch 153/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1103.8090\n",
      "Epoch 00153: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1103.0703 - val_loss: 1113.8115\n",
      "Epoch 154/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1115.0840\n",
      "Epoch 00154: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1115.4570 - val_loss: 1112.4910\n",
      "Epoch 155/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1104.8882\n",
      "Epoch 00155: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1105.5106 - val_loss: 1110.2631\n",
      "Epoch 156/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1098.8663\n",
      "Epoch 00156: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1100.8920 - val_loss: 1119.2769\n",
      "Epoch 157/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1113.6020\n",
      "Epoch 00157: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1116.0417 - val_loss: 1112.8054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1114.5607\n",
      "Epoch 00158: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1110.7528 - val_loss: 1110.8754\n",
      "Epoch 159/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1105.9364\n",
      "Epoch 00159: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1103.6790 - val_loss: 1113.6598\n",
      "Epoch 160/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1102.4534\n",
      "Epoch 00160: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1103.3532 - val_loss: 1115.3613\n",
      "Epoch 161/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1102.3304\n",
      "Epoch 00161: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1103.4634 - val_loss: 1116.2924\n",
      "Epoch 162/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1089.2923\n",
      "Epoch 00162: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1093.0952 - val_loss: 1111.6544\n",
      "Epoch 163/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1102.9727\n",
      "Epoch 00163: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1103.9083 - val_loss: 1110.1152\n",
      "Epoch 164/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1105.3924\n",
      "Epoch 00164: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1103.2162 - val_loss: 1107.5438\n",
      "Epoch 165/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1094.9763\n",
      "Epoch 00165: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1096.2654 - val_loss: 1115.9570\n",
      "Epoch 166/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1100.1319\n",
      "Epoch 00166: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1102.3904 - val_loss: 1107.7202\n",
      "Epoch 167/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1103.5631\n",
      "Epoch 00167: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1102.4729 - val_loss: 1108.5712\n",
      "Epoch 168/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1101.7061\n",
      "Epoch 00168: val_loss did not improve from 1106.46744\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1100.9741 - val_loss: 1112.1014\n",
      "Epoch 169/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1098.9180\n",
      "Epoch 00169: val_loss improved from 1106.46744 to 1103.02476, saving model to Weights\\Weights-169--1103.02476.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1098.7849 - val_loss: 1103.0248\n",
      "Epoch 170/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1099.8699\n",
      "Epoch 00170: val_loss improved from 1103.02476 to 1102.66872, saving model to Weights\\Weights-170--1102.66872.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1096.6937 - val_loss: 1102.6687\n",
      "Epoch 171/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1095.0774\n",
      "Epoch 00171: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1096.0703 - val_loss: 1105.7976\n",
      "Epoch 172/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1099.3828\n",
      "Epoch 00172: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1097.3989 - val_loss: 1105.7135\n",
      "Epoch 173/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1096.8185\n",
      "Epoch 00173: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1096.0180 - val_loss: 1113.5484\n",
      "Epoch 174/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1091.8883\n",
      "Epoch 00174: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1093.3501 - val_loss: 1106.5695\n",
      "Epoch 175/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1091.3349\n",
      "Epoch 00175: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1093.0324 - val_loss: 1105.1958\n",
      "Epoch 176/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1097.1069\n",
      "Epoch 00176: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1098.7600 - val_loss: 1103.8883\n",
      "Epoch 177/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1086.9794\n",
      "Epoch 00177: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1088.4068 - val_loss: 1105.5378\n",
      "Epoch 178/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1090.0577\n",
      "Epoch 00178: val_loss did not improve from 1102.66872\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1089.6563 - val_loss: 1107.6492\n",
      "Epoch 179/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1098.2495\n",
      "Epoch 00179: val_loss improved from 1102.66872 to 1098.86167, saving model to Weights\\Weights-179--1098.86167.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1094.7464 - val_loss: 1098.8617\n",
      "Epoch 180/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1097.4984\n",
      "Epoch 00180: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1097.5023 - val_loss: 1103.1243\n",
      "Epoch 181/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1091.6142\n",
      "Epoch 00181: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1094.3584 - val_loss: 1104.7098\n",
      "Epoch 182/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1084.8540\n",
      "Epoch 00182: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1086.7142 - val_loss: 1105.4615\n",
      "Epoch 183/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1089.9018\n",
      "Epoch 00183: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 1091.8834 - val_loss: 1109.4101\n",
      "Epoch 184/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1089.3431\n",
      "Epoch 00184: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1087.2360 - val_loss: 1104.3016\n",
      "Epoch 185/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1090.4566\n",
      "Epoch 00185: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1090.4767 - val_loss: 1101.4917\n",
      "Epoch 186/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1099.7477\n",
      "Epoch 00186: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1098.1791 - val_loss: 1102.2332\n",
      "Epoch 187/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1091.3760\n",
      "Epoch 00187: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1091.4230 - val_loss: 1099.3023\n",
      "Epoch 188/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1092.6254\n",
      "Epoch 00188: val_loss did not improve from 1098.86167\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1094.1364 - val_loss: 1100.9727\n",
      "Epoch 189/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1089.7806\n",
      "Epoch 00189: val_loss improved from 1098.86167 to 1096.92928, saving model to Weights\\Weights-189--1096.92928.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1089.9328 - val_loss: 1096.9293\n",
      "Epoch 190/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1092.6432\n",
      "Epoch 00190: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1092.3095 - val_loss: 1107.5921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1097.6760\n",
      "Epoch 00191: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1099.0948 - val_loss: 1097.5671\n",
      "Epoch 192/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1089.2370\n",
      "Epoch 00192: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1083.1948 - val_loss: 1099.4797\n",
      "Epoch 193/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1088.2514\n",
      "Epoch 00193: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1089.7444 - val_loss: 1104.1624\n",
      "Epoch 194/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1092.2431\n",
      "Epoch 00194: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1093.6129 - val_loss: 1112.2470\n",
      "Epoch 195/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1076.5094\n",
      "Epoch 00195: val_loss did not improve from 1096.92928\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 1079.3111 - val_loss: 1101.0932\n",
      "Epoch 196/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1083.3541\n",
      "Epoch 00196: val_loss improved from 1096.92928 to 1091.89420, saving model to Weights\\Weights-196--1091.89420.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1083.9753 - val_loss: 1091.8942\n",
      "Epoch 197/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1076.3298\n",
      "Epoch 00197: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1075.8209 - val_loss: 1103.0320\n",
      "Epoch 198/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1075.2257\n",
      "Epoch 00198: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1073.7565 - val_loss: 1094.0532\n",
      "Epoch 199/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1084.7302\n",
      "Epoch 00199: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.4506 - val_loss: 1097.5224\n",
      "Epoch 200/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1085.7914\n",
      "Epoch 00200: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1084.6157 - val_loss: 1102.4405\n",
      "Epoch 201/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1084.3301\n",
      "Epoch 00201: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1083.6753 - val_loss: 1094.7854\n",
      "Epoch 202/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1076.3650\n",
      "Epoch 00202: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.0691 - val_loss: 1097.3182\n",
      "Epoch 203/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1073.4166\n",
      "Epoch 00203: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1073.2925 - val_loss: 1098.1010\n",
      "Epoch 204/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1081.5803\n",
      "Epoch 00204: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1081.7202 - val_loss: 1094.6318\n",
      "Epoch 205/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1083.5026\n",
      "Epoch 00205: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1083.0741 - val_loss: 1104.6267\n",
      "Epoch 206/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1082.8049\n",
      "Epoch 00206: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1081.8708 - val_loss: 1104.7041\n",
      "Epoch 207/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1081.0358\n",
      "Epoch 00207: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1081.7262 - val_loss: 1093.1216\n",
      "Epoch 208/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1076.4127\n",
      "Epoch 00208: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1070.9376 - val_loss: 1097.7742\n",
      "Epoch 209/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1077.6811\n",
      "Epoch 00209: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1078.0298 - val_loss: 1092.4146\n",
      "Epoch 210/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1068.5773\n",
      "Epoch 00210: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1073.4319 - val_loss: 1095.8059\n",
      "Epoch 211/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1076.0830\n",
      "Epoch 00211: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1076.9844 - val_loss: 1092.0616\n",
      "Epoch 212/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1076.3546\n",
      "Epoch 00212: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1077.5060 - val_loss: 1094.9603\n",
      "Epoch 213/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1072.3819\n",
      "Epoch 00213: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1069.3525 - val_loss: 1094.8990\n",
      "Epoch 214/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1067.1251\n",
      "Epoch 00214: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1070.7035 - val_loss: 1098.5851\n",
      "Epoch 215/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1070.3234\n",
      "Epoch 00215: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1069.5223 - val_loss: 1092.4516\n",
      "Epoch 216/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1061.8612\n",
      "Epoch 00216: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1064.3249 - val_loss: 1093.5776\n",
      "Epoch 217/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1063.1113\n",
      "Epoch 00217: val_loss did not improve from 1091.89420\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1067.9475 - val_loss: 1092.9432\n",
      "Epoch 218/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1076.3061\n",
      "Epoch 00218: val_loss improved from 1091.89420 to 1091.38049, saving model to Weights\\Weights-218--1091.38049.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1069.9650 - val_loss: 1091.3805\n",
      "Epoch 219/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.7593\n",
      "Epoch 00219: val_loss improved from 1091.38049 to 1088.07137, saving model to Weights\\Weights-219--1088.07137.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1073.4824 - val_loss: 1088.0714\n",
      "Epoch 220/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1065.2273\n",
      "Epoch 00220: val_loss did not improve from 1088.07137\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1063.6471 - val_loss: 1090.1025\n",
      "Epoch 221/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.0835\n",
      "Epoch 00221: val_loss improved from 1088.07137 to 1087.99572, saving model to Weights\\Weights-221--1087.99572.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1064.8588 - val_loss: 1087.9957\n",
      "Epoch 222/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.5516\n",
      "Epoch 00222: val_loss improved from 1087.99572 to 1087.40728, saving model to Weights\\Weights-222--1087.40728.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1071.9989 - val_loss: 1087.4073\n",
      "Epoch 223/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1085.0994\n",
      "Epoch 00223: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1082.9008 - val_loss: 1094.5605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1071.8529\n",
      "Epoch 00224: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1070.8415 - val_loss: 1090.1291\n",
      "Epoch 225/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1068.4257\n",
      "Epoch 00225: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1069.5791 - val_loss: 1092.1251\n",
      "Epoch 226/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1070.7111\n",
      "Epoch 00226: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1064.5717 - val_loss: 1095.7357\n",
      "Epoch 227/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1071.5659\n",
      "Epoch 00227: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1070.4328 - val_loss: 1098.7899\n",
      "Epoch 228/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1057.7370\n",
      "Epoch 00228: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1062.4970 - val_loss: 1095.7518\n",
      "Epoch 229/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1061.0148\n",
      "Epoch 00229: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1062.6370 - val_loss: 1095.5319\n",
      "Epoch 230/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1070.4738\n",
      "Epoch 00230: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1071.0938 - val_loss: 1093.8512\n",
      "Epoch 231/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1057.4592\n",
      "Epoch 00231: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1057.8703 - val_loss: 1093.8005\n",
      "Epoch 232/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1060.4668\n",
      "Epoch 00232: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1064.2551 - val_loss: 1089.3400\n",
      "Epoch 233/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1065.5455\n",
      "Epoch 00233: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1064.6796 - val_loss: 1088.6692\n",
      "Epoch 234/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.4375\n",
      "Epoch 00234: val_loss did not improve from 1087.40728\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1057.8399 - val_loss: 1094.1113\n",
      "Epoch 235/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1056.5666\n",
      "Epoch 00235: val_loss improved from 1087.40728 to 1086.82477, saving model to Weights\\Weights-235--1086.82477.hdf5\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 1058.2580 - val_loss: 1086.8248\n",
      "Epoch 236/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1057.4967\n",
      "Epoch 00236: val_loss improved from 1086.82477 to 1085.27874, saving model to Weights\\Weights-236--1085.27874.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1055.8519 - val_loss: 1085.2787\n",
      "Epoch 237/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1054.6987\n",
      "Epoch 00237: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1055.5972 - val_loss: 1087.9099\n",
      "Epoch 238/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1066.9903\n",
      "Epoch 00238: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1060.4705 - val_loss: 1086.7036\n",
      "Epoch 239/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1072.3605\n",
      "Epoch 00239: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1066.2836 - val_loss: 1086.0744\n",
      "Epoch 240/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.8462\n",
      "Epoch 00240: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1058.3395 - val_loss: 1092.0782\n",
      "Epoch 241/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1058.0378\n",
      "Epoch 00241: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1054.6480 - val_loss: 1092.4544\n",
      "Epoch 242/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1061.6855\n",
      "Epoch 00242: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1058.4866 - val_loss: 1091.6721\n",
      "Epoch 243/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1060.8969\n",
      "Epoch 00243: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1062.6903 - val_loss: 1089.6633\n",
      "Epoch 244/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.3645\n",
      "Epoch 00244: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1054.3192 - val_loss: 1086.8883\n",
      "Epoch 245/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1054.2465\n",
      "Epoch 00245: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1056.0001 - val_loss: 1090.5558\n",
      "Epoch 246/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.8419\n",
      "Epoch 00246: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1052.0060 - val_loss: 1086.5544\n",
      "Epoch 247/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.2402\n",
      "Epoch 00247: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1056.6926 - val_loss: 1096.6056\n",
      "Epoch 248/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1052.8059\n",
      "Epoch 00248: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 1054.0964 - val_loss: 1086.5940\n",
      "Epoch 249/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.9046 ETA: 0s - loss: 105\n",
      "Epoch 00249: val_loss did not improve from 1085.27874\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1062.0603 - val_loss: 1087.9999\n",
      "Epoch 250/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1058.9000\n",
      "Epoch 00250: val_loss improved from 1085.27874 to 1084.99093, saving model to Weights\\Weights-250--1084.99093.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1058.2807 - val_loss: 1084.9909\n",
      "Epoch 251/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1056.8458\n",
      "Epoch 00251: val_loss did not improve from 1084.99093\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1057.1874 - val_loss: 1095.1411\n",
      "Epoch 252/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1064.1076\n",
      "Epoch 00252: val_loss did not improve from 1084.99093\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1065.3733 - val_loss: 1091.8623\n",
      "Epoch 253/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1051.5522\n",
      "Epoch 00253: val_loss did not improve from 1084.99093\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1049.4479 - val_loss: 1090.0911\n",
      "Epoch 254/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1054.2216\n",
      "Epoch 00254: val_loss did not improve from 1084.99093\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1055.9431 - val_loss: 1091.8977\n",
      "Epoch 255/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1059.4082\n",
      "Epoch 00255: val_loss did not improve from 1084.99093\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1057.7273 - val_loss: 1089.2881\n",
      "Epoch 256/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1052.4520\n",
      "Epoch 00256: val_loss improved from 1084.99093 to 1084.95888, saving model to Weights\\Weights-256--1084.95888.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1052.1343 - val_loss: 1084.9589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1052.5721\n",
      "Epoch 00257: val_loss did not improve from 1084.95888\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1050.7508 - val_loss: 1092.6250\n",
      "Epoch 258/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1051.0646\n",
      "Epoch 00258: val_loss improved from 1084.95888 to 1084.29711, saving model to Weights\\Weights-258--1084.29711.hdf5\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1050.9252 - val_loss: 1084.2971\n",
      "Epoch 259/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.1498\n",
      "Epoch 00259: val_loss did not improve from 1084.29711\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1046.2719 - val_loss: 1085.2392\n",
      "Epoch 260/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1053.0024\n",
      "Epoch 00260: val_loss improved from 1084.29711 to 1084.12239, saving model to Weights\\Weights-260--1084.12239.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1057.6945 - val_loss: 1084.1224\n",
      "Epoch 261/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1047.9143\n",
      "Epoch 00261: val_loss did not improve from 1084.12239\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1048.2629 - val_loss: 1085.6389\n",
      "Epoch 262/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1044.2606\n",
      "Epoch 00262: val_loss improved from 1084.12239 to 1083.18424, saving model to Weights\\Weights-262--1083.18424.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1044.5319 - val_loss: 1083.1842\n",
      "Epoch 263/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1053.1675\n",
      "Epoch 00263: val_loss did not improve from 1083.18424\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1053.3186 - val_loss: 1086.7782\n",
      "Epoch 264/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1045.9916\n",
      "Epoch 00264: val_loss improved from 1083.18424 to 1083.02769, saving model to Weights\\Weights-264--1083.02769.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1048.0543 - val_loss: 1083.0277\n",
      "Epoch 265/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1051.5871\n",
      "Epoch 00265: val_loss did not improve from 1083.02769\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1051.8077 - val_loss: 1088.3631\n",
      "Epoch 266/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.2463\n",
      "Epoch 00266: val_loss did not improve from 1083.02769\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1044.6377 - val_loss: 1083.9044\n",
      "Epoch 267/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1052.5606\n",
      "Epoch 00267: val_loss improved from 1083.02769 to 1081.13132, saving model to Weights\\Weights-267--1081.13132.hdf5\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1048.4900 - val_loss: 1081.1313\n",
      "Epoch 268/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1046.2185\n",
      "Epoch 00268: val_loss improved from 1081.13132 to 1080.71880, saving model to Weights\\Weights-268--1080.71880.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1046.3893 - val_loss: 1080.7188\n",
      "Epoch 269/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1055.8758\n",
      "Epoch 00269: val_loss improved from 1080.71880 to 1080.21963, saving model to Weights\\Weights-269--1080.21963.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1055.8172 - val_loss: 1080.2196\n",
      "Epoch 270/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1050.5400\n",
      "Epoch 00270: val_loss did not improve from 1080.21963\n",
      "19948/19948 [==============================] - 1s 34us/sample - loss: 1050.5680 - val_loss: 1080.2486\n",
      "Epoch 271/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1049.8786\n",
      "Epoch 00271: val_loss did not improve from 1080.21963\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1049.3117 - val_loss: 1084.1386\n",
      "Epoch 272/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.4152\n",
      "Epoch 00272: val_loss did not improve from 1080.21963\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1046.5898 - val_loss: 1082.4467\n",
      "Epoch 273/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1047.8156\n",
      "Epoch 00273: val_loss improved from 1080.21963 to 1077.14210, saving model to Weights\\Weights-273--1077.14210.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1042.8863 - val_loss: 1077.1421\n",
      "Epoch 274/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1045.2291\n",
      "Epoch 00274: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1044.8595 - val_loss: 1081.4599\n",
      "Epoch 275/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1050.3736\n",
      "Epoch 00275: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1051.6621 - val_loss: 1094.7565\n",
      "Epoch 276/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1043.2702\n",
      "Epoch 00276: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1045.9732 - val_loss: 1087.8108\n",
      "Epoch 277/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1050.7006\n",
      "Epoch 00277: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1048.8114 - val_loss: 1080.2593\n",
      "Epoch 278/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1038.7989\n",
      "Epoch 00278: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1041.2634 - val_loss: 1079.9757\n",
      "Epoch 279/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.3915\n",
      "Epoch 00279: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1048.1477 - val_loss: 1080.7571\n",
      "Epoch 280/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1046.4091\n",
      "Epoch 00280: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 39us/sample - loss: 1042.1820 - val_loss: 1082.6524\n",
      "Epoch 281/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1047.3357\n",
      "Epoch 00281: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1047.9142 - val_loss: 1080.3789\n",
      "Epoch 282/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1040.6087\n",
      "Epoch 00282: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1041.3769 - val_loss: 1085.4915\n",
      "Epoch 283/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1042.7636\n",
      "Epoch 00283: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1041.5801 - val_loss: 1082.8610\n",
      "Epoch 284/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1045.1263\n",
      "Epoch 00284: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1040.9151 - val_loss: 1094.0774\n",
      "Epoch 285/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1041.7969\n",
      "Epoch 00285: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1041.3456 - val_loss: 1084.2106\n",
      "Epoch 286/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1039.8429\n",
      "Epoch 00286: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1042.2443 - val_loss: 1086.9138\n",
      "Epoch 287/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1043.9512\n",
      "Epoch 00287: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1042.6334 - val_loss: 1080.2743\n",
      "Epoch 288/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.1196\n",
      "Epoch 00288: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1043.7184 - val_loss: 1081.0294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.0565\n",
      "Epoch 00289: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1042.3182 - val_loss: 1083.4668\n",
      "Epoch 290/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1035.7902\n",
      "Epoch 00290: val_loss did not improve from 1077.14210\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1037.6078 - val_loss: 1082.9427\n",
      "Epoch 291/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1038.3480\n",
      "Epoch 00291: val_loss improved from 1077.14210 to 1077.04707, saving model to Weights\\Weights-291--1077.04707.hdf5\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1042.4188 - val_loss: 1077.0471\n",
      "Epoch 292/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1029.6748\n",
      "Epoch 00292: val_loss did not improve from 1077.04707\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1032.4465 - val_loss: 1084.0303\n",
      "Epoch 293/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1034.1235\n",
      "Epoch 00293: val_loss improved from 1077.04707 to 1076.26349, saving model to Weights\\Weights-293--1076.26349.hdf5\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1036.3250 - val_loss: 1076.2635\n",
      "Epoch 294/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1037.0808\n",
      "Epoch 00294: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1036.1736 - val_loss: 1081.8667\n",
      "Epoch 295/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1047.1207\n",
      "Epoch 00295: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1040.8959 - val_loss: 1078.7213\n",
      "Epoch 296/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1041.4396\n",
      "Epoch 00296: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1042.0888 - val_loss: 1080.1345\n",
      "Epoch 297/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.3415\n",
      "Epoch 00297: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1040.9520 - val_loss: 1086.7442\n",
      "Epoch 298/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1045.7950\n",
      "Epoch 00298: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1044.5505 - val_loss: 1082.0119\n",
      "Epoch 299/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1044.7124\n",
      "Epoch 00299: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1043.2810 - val_loss: 1079.5431\n",
      "Epoch 300/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1039.3921\n",
      "Epoch 00300: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1042.5852 - val_loss: 1084.0114\n",
      "Epoch 301/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1031.9211\n",
      "Epoch 00301: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1032.9033 - val_loss: 1080.4678\n",
      "Epoch 302/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1031.2339\n",
      "Epoch 00302: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1031.0989 - val_loss: 1078.5866\n",
      "Epoch 303/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1029.6649\n",
      "Epoch 00303: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1033.5056 - val_loss: 1082.2471\n",
      "Epoch 304/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1040.2987\n",
      "Epoch 00304: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1041.1206 - val_loss: 1082.6229\n",
      "Epoch 305/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1029.5266\n",
      "Epoch 00305: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1035.9257 - val_loss: 1079.7533\n",
      "Epoch 306/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1037.1061\n",
      "Epoch 00306: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1035.3861 - val_loss: 1085.7583\n",
      "Epoch 307/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1025.2919\n",
      "Epoch 00307: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1028.3909 - val_loss: 1085.8656\n",
      "Epoch 308/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1036.3714\n",
      "Epoch 00308: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1038.0314 - val_loss: 1082.5278\n",
      "Epoch 309/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1030.0139\n",
      "Epoch 00309: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1030.1670 - val_loss: 1077.4579\n",
      "Epoch 310/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1027.0306\n",
      "Epoch 00310: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1027.3348 - val_loss: 1082.5376\n",
      "Epoch 311/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1037.7960\n",
      "Epoch 00311: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1037.3870 - val_loss: 1083.8427\n",
      "Epoch 312/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1034.2984\n",
      "Epoch 00312: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 1033.9588 - val_loss: 1083.9141\n",
      "Epoch 313/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1030.6025\n",
      "Epoch 00313: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 1029.2946 - val_loss: 1078.1240\n",
      "Epoch 314/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1033.6031 ETA: 0s - loss:\n",
      "Epoch 00314: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 1033.0503 - val_loss: 1076.5721\n",
      "Epoch 315/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1033.1463\n",
      "Epoch 00315: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1033.0473 - val_loss: 1083.2876\n",
      "Epoch 316/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1033.3486\n",
      "Epoch 00316: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1034.3915 - val_loss: 1086.9178\n",
      "Epoch 317/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1032.2492\n",
      "Epoch 00317: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1034.2261 - val_loss: 1078.2713\n",
      "Epoch 318/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1028.0195\n",
      "Epoch 00318: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1029.0225 - val_loss: 1078.4658\n",
      "Epoch 319/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1031.7302\n",
      "Epoch 00319: val_loss did not improve from 1076.26349\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1031.4025 - val_loss: 1079.0587\n",
      "Epoch 320/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1024.5244\n",
      "Epoch 00320: val_loss improved from 1076.26349 to 1075.51250, saving model to Weights\\Weights-320--1075.51250.hdf5\n",
      "19948/19948 [==============================] - 1s 43us/sample - loss: 1026.9843 - val_loss: 1075.5125\n",
      "Epoch 321/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1030.2350\n",
      "Epoch 00321: val_loss did not improve from 1075.51250\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 1028.4935 - val_loss: 1077.7466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 322/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1028.8286\n",
      "Epoch 00322: val_loss did not improve from 1075.51250\n",
      "19948/19948 [==============================] - 1s 35us/sample - loss: 1028.5905 - val_loss: 1077.7119\n",
      "Epoch 323/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1031.2083\n",
      "Epoch 00323: val_loss did not improve from 1075.51250\n",
      "19948/19948 [==============================] - 1s 66us/sample - loss: 1030.1584 - val_loss: 1077.9043\n",
      "Epoch 324/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1034.9608\n",
      "Epoch 00324: val_loss improved from 1075.51250 to 1074.16997, saving model to Weights\\Weights-324--1074.16997.hdf5\n",
      "19948/19948 [==============================] - 1s 53us/sample - loss: 1033.3310 - val_loss: 1074.1700\n",
      "Epoch 325/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1018.9957\n",
      "Epoch 00325: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 1s 44us/sample - loss: 1021.5860 - val_loss: 1074.7746\n",
      "Epoch 326/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1028.9191\n",
      "Epoch 00326: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1027.4263 - val_loss: 1076.8074\n",
      "Epoch 327/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1025.0028\n",
      "Epoch 00327: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1026.7962 - val_loss: 1076.7174\n",
      "Epoch 328/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1029.9020\n",
      "Epoch 00328: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1030.4717 - val_loss: 1077.7153\n",
      "Epoch 329/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1026.4718\n",
      "Epoch 00329: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1026.0091 - val_loss: 1078.7646\n",
      "Epoch 330/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1024.0009\n",
      "Epoch 00330: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1024.5835 - val_loss: 1079.8026\n",
      "Epoch 331/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1021.2452\n",
      "Epoch 00331: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1020.5816 - val_loss: 1082.3119\n",
      "Epoch 332/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1039.2890\n",
      "Epoch 00332: val_loss did not improve from 1074.16997\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1036.5329 - val_loss: 1077.6930\n",
      "Epoch 333/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1019.5019\n",
      "Epoch 00333: val_loss improved from 1074.16997 to 1074.09289, saving model to Weights\\Weights-333--1074.09289.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1019.5230 - val_loss: 1074.0929\n",
      "Epoch 334/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.3095\n",
      "Epoch 00334: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1023.3331 - val_loss: 1076.9522\n",
      "Epoch 335/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.5512\n",
      "Epoch 00335: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1021.5410 - val_loss: 1074.3885\n",
      "Epoch 336/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1025.5338\n",
      "Epoch 00336: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1029.2919 - val_loss: 1079.4735\n",
      "Epoch 337/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1026.7067\n",
      "Epoch 00337: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1018.7885 - val_loss: 1078.6697\n",
      "Epoch 338/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1026.8202\n",
      "Epoch 00338: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1030.0185 - val_loss: 1075.0212\n",
      "Epoch 339/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.1106\n",
      "Epoch 00339: val_loss did not improve from 1074.09289\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1020.7626 - val_loss: 1077.8784\n",
      "Epoch 340/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1026.9280\n",
      "Epoch 00340: val_loss improved from 1074.09289 to 1074.02019, saving model to Weights\\Weights-340--1074.02019.hdf5\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1026.8461 - val_loss: 1074.0202\n",
      "Epoch 341/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1020.3503\n",
      "Epoch 00341: val_loss did not improve from 1074.02019\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1020.0653 - val_loss: 1074.1264\n",
      "Epoch 342/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1024.1862\n",
      "Epoch 00342: val_loss did not improve from 1074.02019\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 1023.3428 - val_loss: 1074.2767\n",
      "Epoch 343/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1020.9263\n",
      "Epoch 00343: val_loss improved from 1074.02019 to 1071.84300, saving model to Weights\\Weights-343--1071.84300.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1022.9485 - val_loss: 1071.8430\n",
      "Epoch 344/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1019.5498\n",
      "Epoch 00344: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1020.2395 - val_loss: 1076.6929\n",
      "Epoch 345/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1016.7851\n",
      "Epoch 00345: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1019.5276 - val_loss: 1076.8513\n",
      "Epoch 346/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1025.7759\n",
      "Epoch 00346: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1027.4755 - val_loss: 1079.6199\n",
      "Epoch 347/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1024.2469\n",
      "Epoch 00347: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1021.3573 - val_loss: 1073.9054\n",
      "Epoch 348/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1018.1188\n",
      "Epoch 00348: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1020.6049 - val_loss: 1076.8057\n",
      "Epoch 349/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1009.8425\n",
      "Epoch 00349: val_loss did not improve from 1071.84300\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1014.5388 - val_loss: 1072.6687\n",
      "Epoch 350/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1018.6125\n",
      "Epoch 00350: val_loss improved from 1071.84300 to 1070.09350, saving model to Weights\\Weights-350--1070.09350.hdf5\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 1017.9435 - val_loss: 1070.0935\n",
      "Epoch 351/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1020.1973\n",
      "Epoch 00351: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1022.6420 - val_loss: 1076.2450\n",
      "Epoch 352/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1010.2513\n",
      "Epoch 00352: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1014.3134 - val_loss: 1072.9626\n",
      "Epoch 353/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1018.5912\n",
      "Epoch 00353: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1018.6875 - val_loss: 1074.8589\n",
      "Epoch 354/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1025.9827\n",
      "Epoch 00354: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1021.9110 - val_loss: 1072.7394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1024.5324\n",
      "Epoch 00355: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1024.6986 - val_loss: 1073.2758\n",
      "Epoch 356/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1018.7953\n",
      "Epoch 00356: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1019.6574 - val_loss: 1074.6922\n",
      "Epoch 357/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1021.5461\n",
      "Epoch 00357: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1019.6290 - val_loss: 1073.0519\n",
      "Epoch 358/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1015.7233\n",
      "Epoch 00358: val_loss did not improve from 1070.09350\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1012.1404 - val_loss: 1070.9593\n",
      "Epoch 359/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1021.8440\n",
      "Epoch 00359: val_loss improved from 1070.09350 to 1069.36885, saving model to Weights\\Weights-359--1069.36885.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1020.0667 - val_loss: 1069.3688\n",
      "Epoch 360/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1017.1634\n",
      "Epoch 00360: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1014.3391 - val_loss: 1071.4517\n",
      "Epoch 361/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1020.3232\n",
      "Epoch 00361: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1018.4536 - val_loss: 1071.0749\n",
      "Epoch 362/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1020.1436\n",
      "Epoch 00362: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.1246 - val_loss: 1072.4862\n",
      "Epoch 363/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1009.5456\n",
      "Epoch 00363: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1016.4747 - val_loss: 1069.7023\n",
      "Epoch 364/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1021.5031\n",
      "Epoch 00364: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1022.3642 - val_loss: 1071.1458\n",
      "Epoch 365/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1015.9689\n",
      "Epoch 00365: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1015.2839 - val_loss: 1083.7210\n",
      "Epoch 366/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1020.5951\n",
      "Epoch 00366: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1022.4380 - val_loss: 1075.4409\n",
      "Epoch 367/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1023.6591\n",
      "Epoch 00367: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1023.3116 - val_loss: 1071.0175\n",
      "Epoch 368/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1014.9421 ETA: 0s - loss: 101\n",
      "Epoch 00368: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1015.8589 - val_loss: 1073.3615\n",
      "Epoch 369/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1021.7149\n",
      "Epoch 00369: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1019.8724 - val_loss: 1073.9462\n",
      "Epoch 370/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1017.3727\n",
      "Epoch 00370: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1016.1193 - val_loss: 1074.8547\n",
      "Epoch 371/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1010.0580\n",
      "Epoch 00371: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1011.5018 - val_loss: 1069.4848\n",
      "Epoch 372/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1012.76 - ETA: 0s - loss: 1015.2612\n",
      "Epoch 00372: val_loss did not improve from 1069.36885\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1019.1047 - val_loss: 1072.4504\n",
      "Epoch 373/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1020.5102\n",
      "Epoch 00373: val_loss improved from 1069.36885 to 1067.17286, saving model to Weights\\Weights-373--1067.17286.hdf5\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1019.3374 - val_loss: 1067.1729\n",
      "Epoch 374/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1009.1220\n",
      "Epoch 00374: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1011.6156 - val_loss: 1073.9121\n",
      "Epoch 375/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1010.9951\n",
      "Epoch 00375: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1010.9017 - val_loss: 1067.9072\n",
      "Epoch 376/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1022.2154\n",
      "Epoch 00376: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1017.6376 - val_loss: 1070.3757\n",
      "Epoch 377/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1016.9531\n",
      "Epoch 00377: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1016.4929 - val_loss: 1072.5464\n",
      "Epoch 378/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1023.5500\n",
      "Epoch 00378: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1019.9934 - val_loss: 1076.8680\n",
      "Epoch 379/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1005.0111\n",
      "Epoch 00379: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1009.3603 - val_loss: 1067.7225\n",
      "Epoch 380/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1009.6604\n",
      "Epoch 00380: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1009.1314 - val_loss: 1067.8405\n",
      "Epoch 381/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1010.7738\n",
      "Epoch 00381: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1014.3642 - val_loss: 1068.3603\n",
      "Epoch 382/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1009.9274\n",
      "Epoch 00382: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1009.0382 - val_loss: 1074.0852\n",
      "Epoch 383/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1014.4866\n",
      "Epoch 00383: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1012.6367 - val_loss: 1067.7139\n",
      "Epoch 384/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1007.1940\n",
      "Epoch 00384: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1005.0043 - val_loss: 1070.8943\n",
      "Epoch 385/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1011.6568\n",
      "Epoch 00385: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1013.5379 - val_loss: 1068.6104\n",
      "Epoch 386/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1011.9566\n",
      "Epoch 00386: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1007.1526 - val_loss: 1075.2214\n",
      "Epoch 387/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1012.2103\n",
      "Epoch 00387: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1013.6680 - val_loss: 1075.1876\n",
      "Epoch 388/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19456/19948 [============================>.] - ETA: 0s - loss: 1015.4311\n",
      "Epoch 00388: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1015.9287 - val_loss: 1076.2110\n",
      "Epoch 389/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1011.4330\n",
      "Epoch 00389: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1012.9957 - val_loss: 1068.0222\n",
      "Epoch 390/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1009.6128\n",
      "Epoch 00390: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1010.0891 - val_loss: 1069.4872\n",
      "Epoch 391/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1009.8359\n",
      "Epoch 00391: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1009.7798 - val_loss: 1077.3770\n",
      "Epoch 392/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1009.2475\n",
      "Epoch 00392: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1008.4381 - val_loss: 1079.1744\n",
      "Epoch 393/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1006.7744\n",
      "Epoch 00393: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1004.3724 - val_loss: 1070.6973\n",
      "Epoch 394/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1006.7579\n",
      "Epoch 00394: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1008.3248 - val_loss: 1073.4025\n",
      "Epoch 395/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1006.7042\n",
      "Epoch 00395: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1005.4417 - val_loss: 1074.5715\n",
      "Epoch 396/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1009.0539\n",
      "Epoch 00396: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1009.2563 - val_loss: 1069.3690\n",
      "Epoch 397/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1011.3654\n",
      "Epoch 00397: val_loss did not improve from 1067.17286\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1006.9093 - val_loss: 1067.2560\n",
      "Epoch 398/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1011.9045\n",
      "Epoch 00398: val_loss improved from 1067.17286 to 1067.02135, saving model to Weights\\Weights-398--1067.02135.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 1010.5676 - val_loss: 1067.0213\n",
      "Epoch 399/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1014.1409\n",
      "Epoch 00399: val_loss did not improve from 1067.02135\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1009.6306 - val_loss: 1070.3474\n",
      "Epoch 400/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.3785\n",
      "Epoch 00400: val_loss did not improve from 1067.02135\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1003.8796 - val_loss: 1074.3334\n",
      "Epoch 401/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1003.7557\n",
      "Epoch 00401: val_loss improved from 1067.02135 to 1066.50746, saving model to Weights\\Weights-401--1066.50746.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1009.1225 - val_loss: 1066.5075\n",
      "Epoch 402/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 997.4479\n",
      "Epoch 00402: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1001.4760 - val_loss: 1068.6496\n",
      "Epoch 403/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1010.0833\n",
      "Epoch 00403: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1007.1552 - val_loss: 1068.5954\n",
      "Epoch 404/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1006.8956\n",
      "Epoch 00404: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 1006.9788 - val_loss: 1070.9573\n",
      "Epoch 405/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1007.0454\n",
      "Epoch 00405: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1005.0785 - val_loss: 1072.0679\n",
      "Epoch 406/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1005.9388\n",
      "Epoch 00406: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1006.1636 - val_loss: 1071.2436\n",
      "Epoch 407/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1005.6357\n",
      "Epoch 00407: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1003.3820 - val_loss: 1066.9039\n",
      "Epoch 408/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1002.8786\n",
      "Epoch 00408: val_loss did not improve from 1066.50746\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1008.0969 - val_loss: 1068.7754\n",
      "Epoch 409/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1005.4804\n",
      "Epoch 00409: val_loss improved from 1066.50746 to 1065.68430, saving model to Weights\\Weights-409--1065.68430.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 1007.9536 - val_loss: 1065.6843\n",
      "Epoch 410/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1008.1694\n",
      "Epoch 00410: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1011.9641 - val_loss: 1072.0765\n",
      "Epoch 411/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1004.9220\n",
      "Epoch 00411: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1005.9606 - val_loss: 1066.8763\n",
      "Epoch 412/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 1004.1821\n",
      "Epoch 00412: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1000.6247 - val_loss: 1070.8446\n",
      "Epoch 413/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1006.9500\n",
      "Epoch 00413: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1006.6638 - val_loss: 1069.7484\n",
      "Epoch 414/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1011.2165\n",
      "Epoch 00414: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 16us/sample - loss: 1008.0825 - val_loss: 1070.0673\n",
      "Epoch 415/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 991.2991\n",
      "Epoch 00415: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 996.5697 - val_loss: 1074.3524\n",
      "Epoch 416/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1003.1939\n",
      "Epoch 00416: val_loss did not improve from 1065.68430\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1002.5975 - val_loss: 1073.0749\n",
      "Epoch 417/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1000.6479\n",
      "Epoch 00417: val_loss improved from 1065.68430 to 1065.30508, saving model to Weights\\Weights-417--1065.30508.hdf5\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 997.6986 - val_loss: 1065.3051\n",
      "Epoch 418/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1004.2727\n",
      "Epoch 00418: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1004.7965 - val_loss: 1074.8148\n",
      "Epoch 419/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1006.3195\n",
      "Epoch 00419: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 1005.6450 - val_loss: 1067.1705\n",
      "Epoch 420/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1001.0624\n",
      "Epoch 00420: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1001.3169 - val_loss: 1065.5014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1016.8432\n",
      "Epoch 00421: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1015.1630 - val_loss: 1074.5026\n",
      "Epoch 422/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1006.3546\n",
      "Epoch 00422: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1007.7487 - val_loss: 1070.2727\n",
      "Epoch 423/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.6329\n",
      "Epoch 00423: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 1005.8261 - val_loss: 1073.8129\n",
      "Epoch 424/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1006.9275\n",
      "Epoch 00424: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1005.8377 - val_loss: 1070.2098\n",
      "Epoch 425/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 995.3958\n",
      "Epoch 00425: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 997.9935 - val_loss: 1070.3319\n",
      "Epoch 426/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1001.7184\n",
      "Epoch 00426: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 25us/sample - loss: 1001.6610 - val_loss: 1069.7254\n",
      "Epoch 427/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1002.5100\n",
      "Epoch 00427: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1007.3584 - val_loss: 1070.3339\n",
      "Epoch 428/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1008.4838\n",
      "Epoch 00428: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1007.4945 - val_loss: 1070.1629\n",
      "Epoch 429/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.3085\n",
      "Epoch 00429: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1001.4630 - val_loss: 1065.5720\n",
      "Epoch 430/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1002.8629\n",
      "Epoch 00430: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 1001.8497 - val_loss: 1068.6740\n",
      "Epoch 431/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1006.8114\n",
      "Epoch 00431: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 1005.1620 - val_loss: 1071.2514\n",
      "Epoch 432/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 999.4579 \n",
      "Epoch 00432: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 999.3702 - val_loss: 1069.5534\n",
      "Epoch 433/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1006.8540\n",
      "Epoch 00433: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 1006.5188 - val_loss: 1073.2491\n",
      "Epoch 434/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1003.8541\n",
      "Epoch 00434: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1004.9254 - val_loss: 1071.8697\n",
      "Epoch 435/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1001.7966\n",
      "Epoch 00435: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1000.8130 - val_loss: 1074.9609\n",
      "Epoch 436/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 994.7425  ETA: 0s - loss: 1002\n",
      "Epoch 00436: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 996.9751 - val_loss: 1068.4034\n",
      "Epoch 437/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1007.9658\n",
      "Epoch 00437: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1004.9997 - val_loss: 1071.2128\n",
      "Epoch 438/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1000.4839\n",
      "Epoch 00438: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 999.9843 - val_loss: 1068.3303\n",
      "Epoch 439/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1000.4419\n",
      "Epoch 00439: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1001.7509 - val_loss: 1070.4100\n",
      "Epoch 440/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1003.1115\n",
      "Epoch 00440: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 1003.1398 - val_loss: 1068.2390\n",
      "Epoch 441/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1000.1316 ETA: 0s - loss: 9\n",
      "Epoch 00441: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 1002.3859 - val_loss: 1067.6617\n",
      "Epoch 442/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.8861\n",
      "Epoch 00442: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 1003.3143 - val_loss: 1067.1441\n",
      "Epoch 443/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.6153\n",
      "Epoch 00443: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 22us/sample - loss: 998.7932 - val_loss: 1066.1460\n",
      "Epoch 444/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 998.7238\n",
      "Epoch 00444: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 999.6732 - val_loss: 1068.0201\n",
      "Epoch 445/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 1000.4555\n",
      "Epoch 00445: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 999.0355 - val_loss: 1066.1000\n",
      "Epoch 446/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 998.9161 \n",
      "Epoch 00446: val_loss did not improve from 1065.30508\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 998.7568 - val_loss: 1065.3441\n",
      "Epoch 447/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 995.3947\n",
      "Epoch 00447: val_loss improved from 1065.30508 to 1064.89246, saving model to Weights\\Weights-447--1064.89246.hdf5\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 994.8710 - val_loss: 1064.8925\n",
      "Epoch 448/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.4696\n",
      "Epoch 00448: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 17us/sample - loss: 999.9053 - val_loss: 1068.8036\n",
      "Epoch 449/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.6463\n",
      "Epoch 00449: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 997.2519 - val_loss: 1071.8379\n",
      "Epoch 450/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 999.2435\n",
      "Epoch 00450: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 997.5830 - val_loss: 1070.6970\n",
      "Epoch 451/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 997.8071\n",
      "Epoch 00451: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 993.3458 - val_loss: 1066.2075\n",
      "Epoch 452/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.1252\n",
      "Epoch 00452: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 999.0419 - val_loss: 1069.4447\n",
      "Epoch 453/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.3403\n",
      "Epoch 00453: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 996.3563 - val_loss: 1070.1463\n",
      "Epoch 454/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 995.1426\n",
      "Epoch 00454: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 993.3375 - val_loss: 1068.1413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.4192\n",
      "Epoch 00455: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 999.9419 - val_loss: 1071.9366\n",
      "Epoch 456/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 995.4572\n",
      "Epoch 00456: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 995.3234 - val_loss: 1070.9992\n",
      "Epoch 457/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 990.2454\n",
      "Epoch 00457: val_loss did not improve from 1064.89246\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 990.1067 - val_loss: 1065.0477\n",
      "Epoch 458/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 1000.0172\n",
      "Epoch 00458: val_loss improved from 1064.89246 to 1064.46231, saving model to Weights\\Weights-458--1064.46231.hdf5\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 1000.0503 - val_loss: 1064.4623\n",
      "Epoch 459/500\n",
      "16384/19948 [=======================>......] - ETA: 0s - loss: 993.9074\n",
      "Epoch 00459: val_loss improved from 1064.46231 to 1064.22899, saving model to Weights\\Weights-459--1064.22899.hdf5\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 993.2045 - val_loss: 1064.2290\n",
      "Epoch 460/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 999.7264\n",
      "Epoch 00460: val_loss did not improve from 1064.22899\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 999.6537 - val_loss: 1066.0993\n",
      "Epoch 461/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 1002.6655\n",
      "Epoch 00461: val_loss did not improve from 1064.22899\n",
      "19948/19948 [==============================] - 0s 23us/sample - loss: 997.3729 - val_loss: 1069.5160\n",
      "Epoch 462/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 991.0339\n",
      "Epoch 00462: val_loss improved from 1064.22899 to 1063.75500, saving model to Weights\\Weights-462--1063.75500.hdf5\n",
      "19948/19948 [==============================] - 1s 33us/sample - loss: 991.6402 - val_loss: 1063.7550\n",
      "Epoch 463/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 988.0499\n",
      "Epoch 00463: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 0s 19us/sample - loss: 993.5037 - val_loss: 1070.0264\n",
      "Epoch 464/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 989.6226\n",
      "Epoch 00464: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 0s 20us/sample - loss: 989.3809 - val_loss: 1065.4644\n",
      "Epoch 465/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 994.7092\n",
      "Epoch 00465: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 995.5030 - val_loss: 1070.8696\n",
      "Epoch 466/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 991.9912\n",
      "Epoch 00466: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 0s 18us/sample - loss: 989.9023 - val_loss: 1067.8883\n",
      "Epoch 467/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.5764\n",
      "Epoch 00467: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 1s 26us/sample - loss: 996.6027 - val_loss: 1066.0930\n",
      "Epoch 468/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.3975\n",
      "Epoch 00468: val_loss did not improve from 1063.75500\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 994.2137 - val_loss: 1066.5171\n",
      "Epoch 469/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.2904\n",
      "Epoch 00469: val_loss improved from 1063.75500 to 1062.22118, saving model to Weights\\Weights-469--1062.22118.hdf5\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 993.0523 - val_loss: 1062.2212\n",
      "Epoch 470/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 993.6085\n",
      "Epoch 00470: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 0s 21us/sample - loss: 994.4152 - val_loss: 1064.1564\n",
      "Epoch 471/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 992.0627\n",
      "Epoch 00471: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 990.8519 - val_loss: 1064.6189\n",
      "Epoch 472/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 991.9500\n",
      "Epoch 00472: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 992.6415 - val_loss: 1070.6384\n",
      "Epoch 473/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 997.9300 \n",
      "Epoch 00473: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 997.3100 - val_loss: 1067.5519\n",
      "Epoch 474/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 986.0789\n",
      "Epoch 00474: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 987.0475 - val_loss: 1064.5147\n",
      "Epoch 475/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 994.0243\n",
      "Epoch 00475: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 996.8780 - val_loss: 1071.3898\n",
      "Epoch 476/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 987.3366\n",
      "Epoch 00476: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 55us/sample - loss: 987.0729 - val_loss: 1073.3069\n",
      "Epoch 477/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 994.1993\n",
      "Epoch 00477: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 37us/sample - loss: 993.8911 - val_loss: 1065.3144\n",
      "Epoch 478/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 995.4305\n",
      "Epoch 00478: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 994.1771 - val_loss: 1064.1524\n",
      "Epoch 479/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 987.7392\n",
      "Epoch 00479: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 988.0558 - val_loss: 1063.2354\n",
      "Epoch 480/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 994.5887\n",
      "Epoch 00480: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 995.3316 - val_loss: 1065.5368\n",
      "Epoch 481/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 991.4066\n",
      "Epoch 00481: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 46us/sample - loss: 989.7922 - val_loss: 1067.9942\n",
      "Epoch 482/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 993.2604\n",
      "Epoch 00482: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 38us/sample - loss: 993.6475 - val_loss: 1065.2985\n",
      "Epoch 483/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 996.9949\n",
      "Epoch 00483: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 53us/sample - loss: 995.6074 - val_loss: 1067.9860\n",
      "Epoch 484/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 995.2799\n",
      "Epoch 00484: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 994.1384 - val_loss: 1068.0817\n",
      "Epoch 485/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 989.4893\n",
      "Epoch 00485: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 52us/sample - loss: 988.1535 - val_loss: 1068.0404\n",
      "Epoch 486/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 990.6231\n",
      "Epoch 00486: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 988.6844 - val_loss: 1066.2907\n",
      "Epoch 487/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 986.6223\n",
      "Epoch 00487: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 32us/sample - loss: 989.3945 - val_loss: 1067.8690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/500\n",
      "17408/19948 [=========================>....] - ETA: 0s - loss: 982.3995\n",
      "Epoch 00488: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 42us/sample - loss: 985.4887 - val_loss: 1067.6095\n",
      "Epoch 489/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 982.3693\n",
      "Epoch 00489: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 982.6062 - val_loss: 1066.7865\n",
      "Epoch 490/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 992.2404\n",
      "Epoch 00490: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 36us/sample - loss: 990.2624 - val_loss: 1067.5991\n",
      "Epoch 491/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 989.4027\n",
      "Epoch 00491: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 30us/sample - loss: 988.8071 - val_loss: 1067.2337\n",
      "Epoch 492/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 991.0222\n",
      "Epoch 00492: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 45us/sample - loss: 990.2416 - val_loss: 1066.1620\n",
      "Epoch 493/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 988.5368\n",
      "Epoch 00493: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 31us/sample - loss: 986.3556 - val_loss: 1066.3822\n",
      "Epoch 494/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 996.7328 \n",
      "Epoch 00494: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 28us/sample - loss: 997.1353 - val_loss: 1063.2312\n",
      "Epoch 495/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 992.1791\n",
      "Epoch 00495: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 40us/sample - loss: 990.9409 - val_loss: 1065.6380\n",
      "Epoch 496/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 985.6646\n",
      "Epoch 00496: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 985.4560 - val_loss: 1065.8086\n",
      "Epoch 497/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 988.5731\n",
      "Epoch 00497: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 0s 24us/sample - loss: 989.1838 - val_loss: 1067.3701\n",
      "Epoch 498/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 993.9114\n",
      "Epoch 00498: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 27us/sample - loss: 994.5776 - val_loss: 1067.7405\n",
      "Epoch 499/500\n",
      "19456/19948 [============================>.] - ETA: 0s - loss: 978.0301\n",
      "Epoch 00499: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 1s 29us/sample - loss: 979.6276 - val_loss: 1066.3789\n",
      "Epoch 500/500\n",
      "18432/19948 [==========================>...] - ETA: 0s - loss: 991.8679\n",
      "Epoch 00500: val_loss did not improve from 1062.22118\n",
      "19948/19948 [==============================] - 0s 25us/sample - loss: 992.7458 - val_loss: 1065.8779\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.005, decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, warm_start=False, \n",
    "            loss='mean_absolute_error')\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=500, batch_size=1024,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=callbacks_list, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "blue",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1478.978610965659,
          1463.9528442896844,
          1459.4709190673093,
          1433.2028370932394,
          1403.0044870814509,
          1406.8127886166142,
          1389.3272316372177,
          1371.4299656646342,
          1357.500714846883,
          1359.5354559550142,
          1346.5418220919312,
          1359.662910554747,
          1331.157085644355,
          1325.814622290416,
          1308.72667947895,
          1308.0340358565081,
          1305.9603297430742,
          1304.8266061339561,
          1298.1237427271844,
          1301.910909135236,
          1289.2417350296316,
          1283.5300441303639,
          1281.3063629499197,
          1278.2896296728918,
          1278.9222717835905,
          1263.2573212814254,
          1274.4840685880947,
          1276.8268369831876,
          1267.5156427463357,
          1261.0283511544078,
          1258.2430500208745,
          1258.5702925751484,
          1249.5796611962587,
          1258.8403228765885,
          1243.097773718504,
          1244.6019783272445,
          1227.2027010461575,
          1241.699759560404,
          1231.280936856726,
          1233.289822679592,
          1232.924765572128,
          1231.311228676982,
          1225.9194878852986,
          1219.5397464804978,
          1224.636026496038,
          1228.0749021919883,
          1226.4206018656319,
          1212.5227677410487,
          1209.8514868883708,
          1223.2185046599675,
          1210.7726353411447,
          1212.2412649597939,
          1212.0074568879086,
          1208.2433236816112,
          1207.5731027380173,
          1205.8651790064323,
          1194.4082811843998,
          1199.4110648623923,
          1196.036746615419,
          1190.878437183748,
          1192.6251765576828,
          1188.3912025269608,
          1196.646886837418,
          1185.5963205995745,
          1186.5589319829212,
          1186.9709307676521,
          1180.1611429952252,
          1187.387909296593,
          1174.9010685839824,
          1181.7503510592385,
          1176.7388676623675,
          1173.9629390663772,
          1176.3903891573168,
          1168.2845150075743,
          1169.0819265588577,
          1169.224834300627,
          1173.8353325580101,
          1173.5108073961048,
          1163.1926103572414,
          1162.2173257647619,
          1170.6500536404417,
          1171.0108057071432,
          1168.3900603238342,
          1160.290736432302,
          1162.1122489762445,
          1157.2077384108838,
          1164.1265044486759,
          1160.2582576515347,
          1159.3282468010577,
          1155.4014283817614,
          1159.6699708793644,
          1149.1548800318328,
          1151.1957666348521,
          1151.0324064491506,
          1147.4040619379919,
          1145.5731870392317,
          1147.6679392788437,
          1148.9007908109913,
          1144.8804165243696,
          1143.54860461908,
          1144.3305691232752,
          1146.6592727762104,
          1147.7348797057898,
          1137.873627853317,
          1141.7456817902328,
          1143.8003838789446,
          1145.64191421622,
          1135.2810054432541,
          1133.29094951072,
          1142.551569676863,
          1139.568319451864,
          1133.7296299724987,
          1137.5462198787159,
          1128.1185955554754,
          1132.7100447491603,
          1127.1259096159781,
          1127.8760565556383,
          1132.4582751824667,
          1131.7527917801126,
          1128.4277797077089,
          1132.5829241721835,
          1131.7939910613297,
          1120.853395170216,
          1121.818826850201,
          1123.0609906802608,
          1113.8384578223695,
          1128.2253676208534,
          1119.3996953602805,
          1128.5206280469847,
          1110.6954123935122,
          1128.8128935280558,
          1119.4223501612005,
          1121.512345697446,
          1117.2359626728714,
          1119.9042945006627,
          1118.2354541730374,
          1115.1926813181067,
          1110.911230478541,
          1109.5112735984653,
          1114.4020709215054,
          1123.0277811176245,
          1112.003537101738,
          1103.070279528532,
          1115.4569859907244,
          1105.5105531951433,
          1100.891986866047,
          1116.0417397978886,
          1110.7527709251085,
          1103.6790387184415,
          1103.353249699218,
          1103.4634220359653,
          1093.0952137912086,
          1103.9083215726123,
          1103.2162237785037,
          1096.2653777507064,
          1102.390408690525,
          1102.4728769165063,
          1100.9741032984589,
          1098.784941188888,
          1096.6937358420957,
          1096.0703105907392,
          1097.398860181002,
          1096.017968666776,
          1093.3501228682858,
          1093.032387283108,
          1098.7600114790644,
          1088.4067988635686,
          1089.6563057357332,
          1094.7464449561046,
          1097.5022839411768,
          1094.3583925873286,
          1086.7141882370727,
          1091.8834420420828,
          1087.2359769678508,
          1090.4766528227296,
          1098.1791066685491,
          1091.4229889803332,
          1094.1364389208818,
          1089.9327954830999,
          1092.3095142585566,
          1099.0948120141666,
          1083.194840374035,
          1089.744414506036,
          1093.612929897029,
          1079.3110882239134,
          1083.9753032200126,
          1075.820947745388,
          1073.7565267840919,
          1078.4505743497548,
          1084.615698056157,
          1083.6752902272472,
          1078.0690563776366,
          1073.2924506059505,
          1081.7201973421913,
          1083.0740752127406,
          1081.8708236140528,
          1081.7261542118883,
          1070.9375583058923,
          1078.029788558651,
          1073.4319164887302,
          1076.984415265824,
          1077.50596590197,
          1069.3524965447273,
          1070.7034999739558,
          1069.5223344856784,
          1064.324916633834,
          1067.947528491069,
          1069.9650124503396,
          1073.4823883895,
          1063.647115569328,
          1064.8588294531603,
          1071.9989470915395,
          1082.9008086797157,
          1070.841535918192,
          1069.5790749062799,
          1064.5716809260718,
          1070.432841452229,
          1062.4970277702496,
          1062.6370431040627,
          1071.0937782717488,
          1057.870310120767,
          1064.2551050455795,
          1064.6795795288454,
          1057.839932065558,
          1058.2580354918882,
          1055.8518743459558,
          1055.5972368490106,
          1060.4704643817088,
          1066.2836014139693,
          1058.3395168992113,
          1054.6480354414641,
          1058.4865597208013,
          1062.6903441624004,
          1054.3191550227857,
          1056.000089882131,
          1052.005973208565,
          1056.6926377600316,
          1054.0964224268255,
          1062.0602778405496,
          1058.2806699763134,
          1057.1874389770826,
          1065.3733245256026,
          1049.4479200609085,
          1055.943122773508,
          1057.7272726716417,
          1052.1343039999701,
          1050.7508383858578,
          1050.9251949355435,
          1046.2718561178988,
          1057.6944881643444,
          1048.2628501095035,
          1044.5319203464162,
          1053.318634577832,
          1048.0542829936937,
          1051.807669055756,
          1044.637664005516,
          1048.4900441832358,
          1046.3893084177064,
          1055.8172465206412,
          1050.5680445427643,
          1049.3116659466957,
          1046.5897523624901,
          1042.8863226028298,
          1044.8595102270788,
          1051.6620620595972,
          1045.9731867308126,
          1048.81144100883,
          1041.263350849905,
          1048.1477235098366,
          1042.1820105443098,
          1047.9141693941358,
          1041.3768623371939,
          1041.5801395395606,
          1040.9151382828754,
          1041.345634440561,
          1042.244313364161,
          1042.6333771516881,
          1043.7183922583483,
          1042.3182378921524,
          1037.6078066155599,
          1042.4187759512624,
          1032.446487062652,
          1036.3250286927653,
          1036.1736154382254,
          1040.895867451174,
          1042.0888291001918,
          1040.9519515388447,
          1044.550470933901,
          1043.2810416090624,
          1042.5852215760979,
          1032.903266388117,
          1031.0989269855531,
          1033.5056351592266,
          1041.1206212568695,
          1035.925730948317,
          1035.386072979199,
          1028.3908633638866,
          1038.031378899592,
          1030.1669954675124,
          1027.334813312464,
          1037.3870082233339,
          1033.9587517756127,
          1029.2945746342834,
          1033.0502678546252,
          1033.0472786618254,
          1034.3915441622828,
          1034.2260788450362,
          1029.0225142988959,
          1031.40246030059,
          1026.9843496655758,
          1028.493474060389,
          1028.590501037561,
          1030.1584050783013,
          1033.3309862845547,
          1021.585984717492,
          1027.4262992594222,
          1026.7961893794961,
          1030.4716875937986,
          1026.0091361073949,
          1024.583516214423,
          1020.5816058152564,
          1036.5329102982207,
          1019.5230406674541,
          1023.3330788590374,
          1021.5410192844168,
          1029.2918869823259,
          1018.7884564687523,
          1030.018475404215,
          1020.7625589227304,
          1026.8461001533676,
          1020.065343697324,
          1023.3428458792665,
          1022.9485304614262,
          1020.2394530221936,
          1019.5275829950841,
          1027.47553805911,
          1021.3572713615948,
          1020.6049236804755,
          1014.5388451379838,
          1017.9435131072233,
          1022.641982484538,
          1014.3133698397082,
          1018.6874778231997,
          1021.9109692524784,
          1024.6985667373845,
          1019.6574200587543,
          1019.6289717400985,
          1012.1403649855836,
          1020.0667069953167,
          1014.3391461843762,
          1018.4536347114803,
          1023.1246028002886,
          1016.4747473558205,
          1022.3642183666791,
          1015.283851049839,
          1022.4380110455152,
          1023.3115678522948,
          1015.858932198325,
          1019.8723785357555,
          1016.1193328484137,
          1011.5018082169892,
          1019.104739975303,
          1019.3373666674956,
          1011.6155671004377,
          1010.9017299103646,
          1017.6376102891933,
          1016.4929452440604,
          1019.9933587704007,
          1009.3602988864799,
          1009.1314371222601,
          1014.3642276070126,
          1009.0381959770207,
          1012.6366621453083,
          1005.0042914556362,
          1013.5378713047478,
          1007.1526160767017,
          1013.6680431622218,
          1015.9287280963708,
          1012.9957134888601,
          1010.0890705237915,
          1009.7798244062492,
          1008.4380971458409,
          1004.3723654279447,
          1008.3247560478533,
          1005.441678393119,
          1009.256259512036,
          1006.9093069960413,
          1010.5676053148342,
          1009.6306254782943,
          1003.879633372452,
          1009.1224765200067,
          1001.4759787801801,
          1007.1551647075364,
          1006.97875139425,
          1005.0785116742985,
          1006.1635572189842,
          1003.3820482571091,
          1008.096937829372,
          1007.9535831859123,
          1011.9640996277823,
          1005.9605930007864,
          1000.6247459459047,
          1006.6637650591733,
          1008.0824877591895,
          996.5696938994511,
          1002.5975352689453,
          997.6986333779349,
          1004.79652523808,
          1005.6450088344931,
          1001.316936534991,
          1015.1630406170301,
          1007.7486522575884,
          1005.8260748698569,
          1005.8377284602075,
          997.9935047431917,
          1001.6610074988916,
          1007.3584493266477,
          1007.4945406763641,
          1001.4630126708348,
          1001.8496611086285,
          1005.1619855422928,
          999.3702106923313,
          1006.5188154852029,
          1004.9253679929144,
          1000.8129538778363,
          996.975092515932,
          1004.9996656835115,
          999.9843348076091,
          1001.7508625453523,
          1003.139810969458,
          1002.3858721396333,
          1003.3143226588348,
          998.793166994929,
          999.6732331454359,
          999.0354827825785,
          998.7567767504105,
          994.8709967203793,
          999.9053239839599,
          997.2519179382997,
          997.5829656006986,
          993.3457962014714,
          999.0419191949851,
          996.3562820094942,
          993.3375200815088,
          999.9419207468716,
          995.3234252317745,
          990.1067441730335,
          1000.050337126529,
          993.2044907212855,
          999.6537304119618,
          997.3729480095514,
          991.6402327447848,
          993.5037299126166,
          989.3808732049031,
          995.5029991062701,
          989.9022805118503,
          996.6026981186241,
          994.2137105057133,
          993.0522783431257,
          994.4152235631979,
          990.8518928755781,
          992.6415450312414,
          997.3099608665146,
          987.0475168298906,
          996.8779636625538,
          987.0729056190627,
          993.891078902314,
          994.177089852562,
          988.0557524514911,
          995.3316147373797,
          989.792243035016,
          993.6475344318079,
          995.6074136749689,
          994.1384252376491,
          988.153458330037,
          988.684396753336,
          989.394455613893,
          985.4886655866777,
          982.6061853592779,
          990.2623807005637,
          988.807097183145,
          990.2415567952262,
          986.3555813841574,
          997.135306765853,
          990.9408715345445,
          985.4559703740467,
          989.183769952756,
          994.577573835527,
          979.6276284159126,
          992.745770081313
         ]
        },
        {
         "marker": {
          "color": "red",
          "opacity": 0.5,
          "size": 5
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          3084.509010585726,
          2715.4950776315295,
          2335.5779227162498,
          1942.2547453212337,
          1560.2305680315696,
          1533.344025447609,
          1391.9270684100927,
          1391.9061119947019,
          1283.8164903064373,
          1257.5554474348148,
          1343.492591528991,
          1249.1565963839969,
          1273.931771115643,
          1238.5280704930476,
          1249.1369806369607,
          1222.9302471582364,
          1250.940627198098,
          1259.9327399921435,
          1222.0851111816505,
          1220.0682521381764,
          1211.8369431175354,
          1205.204886758306,
          1206.0696130251154,
          1198.6068092225332,
          1213.547128074987,
          1226.6462083154483,
          1200.5660728391865,
          1206.3319354442644,
          1193.6816748693086,
          1191.05509409915,
          1185.6145979302046,
          1185.1049158720878,
          1195.6122691703508,
          1178.8923863177072,
          1190.6616004835228,
          1188.8721666077897,
          1185.9436894323678,
          1180.6641418077627,
          1176.8585817999847,
          1170.0766341119725,
          1182.5095795454902,
          1187.198525663796,
          1193.485152069208,
          1182.7332619498195,
          1169.1954041934812,
          1201.307268331255,
          1163.29106499653,
          1171.4303922865465,
          1167.9388673735305,
          1167.4191385884549,
          1172.5245584320014,
          1163.2916225007284,
          1169.0115744539319,
          1167.075092677485,
          1169.4974389712079,
          1160.095866342334,
          1154.335890943406,
          1177.3461461714521,
          1163.2173219951953,
          1171.3176595329496,
          1149.066680865367,
          1165.8868981226187,
          1152.0677210209435,
          1151.3972082786338,
          1161.8523864929678,
          1162.7438438328325,
          1162.2467412342403,
          1148.9362301211665,
          1156.4446289356233,
          1167.4867800813365,
          1145.2876482271877,
          1144.1296021021844,
          1136.3747351757147,
          1158.4674257561849,
          1147.3641992618307,
          1141.2627891110637,
          1140.881246000343,
          1144.491007723597,
          1139.0002606630756,
          1161.8137982729706,
          1168.6337769215586,
          1164.2797444498274,
          1149.706319291486,
          1142.1767012690025,
          1146.4462056180053,
          1142.164296874021,
          1152.8610615138423,
          1150.0568497625075,
          1137.7877770288687,
          1131.8489132780385,
          1130.4961731801511,
          1139.8263482613388,
          1140.8904682937357,
          1127.8227976234302,
          1127.213094255024,
          1131.6640804911128,
          1140.8941201469524,
          1132.3219289096965,
          1141.0794216084867,
          1127.842263787214,
          1120.7621218243605,
          1136.3702708340513,
          1128.6568766537137,
          1131.4713001362136,
          1133.4344721813825,
          1122.5901398078363,
          1120.274556473785,
          1126.3801115076935,
          1127.865220128976,
          1125.2781789537564,
          1128.2893589494795,
          1127.7512316691366,
          1118.395898256365,
          1124.1052776770384,
          1117.430914004341,
          1122.8788331106189,
          1126.812787147952,
          1119.8262186518934,
          1113.3938662886787,
          1122.3735050731511,
          1115.8287717009537,
          1132.729375453327,
          1119.4132002239025,
          1120.5983823321494,
          1117.8889842349874,
          1117.26573600639,
          1126.2949605987285,
          1123.6535698137423,
          1116.763428713483,
          1115.2601944499056,
          1117.329515602872,
          1128.9984995656675,
          1115.6549778936956,
          1126.229164210737,
          1121.2937095187722,
          1115.7381127944375,
          1106.4674388762344,
          1118.55574094697,
          1109.4610059641395,
          1109.484224804805,
          1119.927251968399,
          1108.162983620695,
          1113.811538589199,
          1112.4910066221003,
          1110.2630645586728,
          1119.2769027841528,
          1112.80539896896,
          1110.8753983012082,
          1113.6598005644755,
          1115.3612704310503,
          1116.292395726937,
          1111.654417819338,
          1110.115182359878,
          1107.5438399849374,
          1115.9569946558317,
          1107.720194625166,
          1108.5711581802138,
          1112.1013578908678,
          1103.0247622088916,
          1102.6687175229815,
          1105.7975677435159,
          1105.7134755687052,
          1113.5483993000944,
          1106.5694716908492,
          1105.1958261891073,
          1103.888299985235,
          1105.5377683392837,
          1107.6491687714229,
          1098.861665060603,
          1103.1242935979249,
          1104.7098363704617,
          1105.4615470682759,
          1109.4101152253672,
          1104.3016077152158,
          1101.491683895707,
          1102.2332384267454,
          1099.3022535349721,
          1100.9726837384621,
          1096.9292813414488,
          1107.5920679655776,
          1097.567139430684,
          1099.4797086927965,
          1104.1624014429706,
          1112.2470333511662,
          1101.093185739955,
          1091.894198132919,
          1103.031963451654,
          1094.0532236843135,
          1097.5223628308602,
          1102.4404994979134,
          1094.7854237668328,
          1097.318209424582,
          1098.1009566278574,
          1094.631847123811,
          1104.6267381862765,
          1104.704122197205,
          1093.1215624490865,
          1097.7741915112103,
          1092.4145905819976,
          1095.8059469513312,
          1092.0615952306073,
          1094.9603147382413,
          1094.8989924340394,
          1098.5851433208766,
          1092.451617736388,
          1093.5775856337807,
          1092.9432019588821,
          1091.3804880237446,
          1088.0713742693895,
          1090.1024615416097,
          1087.995716964694,
          1087.4072754199983,
          1094.560474886071,
          1090.1291115934632,
          1092.1251352637953,
          1095.7357040169675,
          1098.7898559203147,
          1095.7518046921998,
          1095.5318606697726,
          1093.8511852496179,
          1093.8005112119636,
          1089.3400277978603,
          1088.6692040663147,
          1094.1112826943804,
          1086.8247748344916,
          1085.2787359194454,
          1087.9098622972463,
          1086.7035935766978,
          1086.0743785404552,
          1092.078207587775,
          1092.454446110669,
          1091.6721458213228,
          1089.6632662157938,
          1086.8883016741966,
          1090.5558375849473,
          1086.5544343026243,
          1096.605608248438,
          1086.5940258882079,
          1087.9998772198342,
          1084.9909301292737,
          1095.1411498019852,
          1091.8622597709123,
          1090.0910535360688,
          1091.8977003784057,
          1089.2881120552108,
          1084.9588757184695,
          1092.6249580207368,
          1084.2971097411864,
          1085.2392393073553,
          1084.122387519073,
          1085.6389316568782,
          1083.1842352616804,
          1086.778229696038,
          1083.0276907459395,
          1088.3630967125855,
          1083.9044343417886,
          1081.1313220036157,
          1080.7187980252563,
          1080.2196308057198,
          1080.2485874406268,
          1084.138625244973,
          1082.4466789678554,
          1077.142098876023,
          1081.4599129856767,
          1094.7565273715568,
          1087.8108361035568,
          1080.259301356378,
          1079.9757152287352,
          1080.7570982161042,
          1082.652444402321,
          1080.3788569519031,
          1085.4915484850455,
          1082.8609687922974,
          1094.0773516269253,
          1084.2106409036542,
          1086.9138231814827,
          1080.2742934608498,
          1081.0294408039026,
          1083.46684923281,
          1082.9427370783176,
          1077.0470726819417,
          1084.0303134340693,
          1076.2634875334072,
          1081.8667485413246,
          1078.7212806030054,
          1080.1345416540034,
          1086.744241693442,
          1082.0119364791817,
          1079.5431463357463,
          1084.0113723904808,
          1080.4677631323864,
          1078.5865848618519,
          1082.2470938110964,
          1082.6228975854801,
          1079.7532726690958,
          1085.7582941477922,
          1085.8655798533218,
          1082.5277754877525,
          1077.457909558994,
          1082.5375756507935,
          1083.842667742772,
          1083.9141365940116,
          1078.124023510933,
          1076.5721067157422,
          1083.2875559217637,
          1086.9178081273028,
          1078.2712637819268,
          1078.4657629612382,
          1079.0587034101163,
          1075.512499050265,
          1077.746557431157,
          1077.7118988863233,
          1077.9042783698553,
          1074.169972911014,
          1074.774638082452,
          1076.8074160786796,
          1076.7174311168021,
          1077.7152884364582,
          1078.7646489025765,
          1079.802576469328,
          1082.3119245046887,
          1077.6930064309784,
          1074.0928865000171,
          1076.9522480862352,
          1074.3885433915289,
          1079.473522540441,
          1078.669732662344,
          1075.0211609755677,
          1077.8783987047966,
          1074.020194253105,
          1074.126448761898,
          1074.2766724734308,
          1071.8429950096813,
          1076.6929416384944,
          1076.851301690058,
          1079.6198884188734,
          1073.905384615234,
          1076.8057499792428,
          1072.6686830094181,
          1070.0934981978535,
          1076.2450101709758,
          1072.9625720721533,
          1074.8588861557628,
          1072.739444136787,
          1073.2757568848929,
          1074.6922107097594,
          1073.0519198059485,
          1070.959314848489,
          1069.3688493892714,
          1071.4517275188928,
          1071.0749032200517,
          1072.486165005577,
          1069.7023064900382,
          1071.1457501172972,
          1083.7209636656871,
          1075.4409145418713,
          1071.0175275785402,
          1073.3614816246852,
          1073.9462431912816,
          1074.8546648532279,
          1069.4847772421185,
          1072.4503547357897,
          1067.1728627977664,
          1073.9120890585052,
          1067.9071568744753,
          1070.3757421395237,
          1072.546436335057,
          1076.8679518408017,
          1067.7225085025755,
          1067.8404699386374,
          1068.3603021175368,
          1074.0851817019172,
          1067.7139138664825,
          1070.8942858365344,
          1068.6103663470335,
          1075.2213537668642,
          1075.1875785489538,
          1076.211011055502,
          1068.0222468799739,
          1069.487178333589,
          1077.376964458177,
          1079.1744277063724,
          1070.697255099587,
          1073.402465281803,
          1074.571537511201,
          1069.3689646303044,
          1067.2560049194312,
          1067.0213471529885,
          1070.3474490951864,
          1074.3334460368442,
          1066.507463398978,
          1068.6496290257012,
          1068.5953747076383,
          1070.9573204785568,
          1072.0679320432344,
          1071.243588799629,
          1066.9038833241818,
          1068.7754133892652,
          1065.6843000296865,
          1072.0765263366393,
          1066.8763127637717,
          1070.8446131817725,
          1069.7484073826324,
          1070.0673462672871,
          1074.3523726630647,
          1073.0748865996904,
          1065.305082912839,
          1074.8147616909432,
          1067.1705360206067,
          1065.5014101605582,
          1074.502627314416,
          1070.2727441445413,
          1073.8129200618876,
          1070.2098411191364,
          1070.3319425917541,
          1069.7253573157398,
          1070.333909815912,
          1070.1629283500383,
          1065.5720171518212,
          1068.6740335663742,
          1071.2513521973538,
          1069.5533601475163,
          1073.2491174563631,
          1071.869742238022,
          1074.960876109917,
          1068.4034191534708,
          1071.2127814544378,
          1068.3302604838987,
          1070.4100325886368,
          1068.2390213333965,
          1067.661727792064,
          1067.1440929767004,
          1066.145971150969,
          1068.020106182324,
          1066.100043217835,
          1065.3441354259355,
          1064.8924600935088,
          1068.8036382142398,
          1071.837903304451,
          1070.6969788931692,
          1066.2075291265103,
          1069.4446947561707,
          1070.1462984178786,
          1068.141347973481,
          1071.9365645600435,
          1070.9991518230602,
          1065.0476904747266,
          1064.4623077618996,
          1064.2289883384299,
          1066.0992669319137,
          1069.51599779544,
          1063.754995801584,
          1070.026415824699,
          1065.4643872652882,
          1070.8696260668362,
          1067.8883344253654,
          1066.0930237709842,
          1066.5170916306224,
          1062.2211774050031,
          1064.1564273654471,
          1064.618944705453,
          1070.6383799538958,
          1067.5518767007109,
          1064.5146736005802,
          1071.389802279873,
          1073.3068775202244,
          1065.3143601342006,
          1064.152400660663,
          1063.2354141275127,
          1065.5368413196577,
          1067.9942082568975,
          1065.2984988753963,
          1067.9860199996554,
          1068.0816502300513,
          1068.0404436300005,
          1066.290710828623,
          1067.8689947134033,
          1067.6095343743343,
          1066.7864614501611,
          1067.5990863354095,
          1067.233650680245,
          1066.1620021381764,
          1066.3821835943374,
          1063.2312233486753,
          1065.6380008921633,
          1065.8086029046615,
          1067.3700896256048,
          1067.7404652976645,
          1066.3789118309166,
          1065.8779298343661
         ]
        },
        {
         "marker": {
          "color": "grey",
          "size": 5
         },
         "mode": "lines",
         "name": "Lowest error from previous models",
         "type": "scatter",
         "x": [
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312,
          1074.8323216045312
         ]
        }
       ],
       "layout": {
        "legend": {
         "font": {
          "color": "black",
          "family": "sans-serif",
          "size": 12
         },
         "traceorder": "normal",
         "x": 0.57,
         "y": 1
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Learning curve"
        },
        "xaxis": {
         "range": [
          10,
          499
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "range": [
          900,
          1500
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"affd4ca9-3eb4-4c18-bdc1-a0e9de85af64\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"affd4ca9-3eb4-4c18-bdc1-a0e9de85af64\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'affd4ca9-3eb4-4c18-bdc1-a0e9de85af64',\n",
       "                        [{\"marker\": {\"color\": \"blue\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Training Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [1478.978610965659, 1463.9528442896844, 1459.4709190673093, 1433.2028370932394, 1403.0044870814509, 1406.8127886166142, 1389.3272316372177, 1371.4299656646342, 1357.500714846883, 1359.5354559550142, 1346.5418220919312, 1359.662910554747, 1331.157085644355, 1325.814622290416, 1308.72667947895, 1308.0340358565081, 1305.9603297430742, 1304.8266061339561, 1298.1237427271844, 1301.910909135236, 1289.2417350296316, 1283.5300441303639, 1281.3063629499197, 1278.2896296728918, 1278.9222717835905, 1263.2573212814254, 1274.4840685880947, 1276.8268369831876, 1267.5156427463357, 1261.0283511544078, 1258.2430500208745, 1258.5702925751484, 1249.5796611962587, 1258.8403228765885, 1243.097773718504, 1244.6019783272445, 1227.2027010461575, 1241.699759560404, 1231.280936856726, 1233.289822679592, 1232.924765572128, 1231.311228676982, 1225.9194878852986, 1219.5397464804978, 1224.636026496038, 1228.0749021919883, 1226.4206018656319, 1212.5227677410487, 1209.8514868883708, 1223.2185046599675, 1210.7726353411447, 1212.2412649597939, 1212.0074568879086, 1208.2433236816112, 1207.5731027380173, 1205.8651790064323, 1194.4082811843998, 1199.4110648623923, 1196.036746615419, 1190.878437183748, 1192.6251765576828, 1188.3912025269608, 1196.646886837418, 1185.5963205995745, 1186.5589319829212, 1186.9709307676521, 1180.1611429952252, 1187.387909296593, 1174.9010685839824, 1181.7503510592385, 1176.7388676623675, 1173.9629390663772, 1176.3903891573168, 1168.2845150075743, 1169.0819265588577, 1169.224834300627, 1173.8353325580101, 1173.5108073961048, 1163.1926103572414, 1162.2173257647619, 1170.6500536404417, 1171.0108057071432, 1168.3900603238342, 1160.290736432302, 1162.1122489762445, 1157.2077384108838, 1164.1265044486759, 1160.2582576515347, 1159.3282468010577, 1155.4014283817614, 1159.6699708793644, 1149.1548800318328, 1151.1957666348521, 1151.0324064491506, 1147.4040619379919, 1145.5731870392317, 1147.6679392788437, 1148.9007908109913, 1144.8804165243696, 1143.54860461908, 1144.3305691232752, 1146.6592727762104, 1147.7348797057898, 1137.873627853317, 1141.7456817902328, 1143.8003838789446, 1145.64191421622, 1135.2810054432541, 1133.29094951072, 1142.551569676863, 1139.568319451864, 1133.7296299724987, 1137.5462198787159, 1128.1185955554754, 1132.7100447491603, 1127.1259096159781, 1127.8760565556383, 1132.4582751824667, 1131.7527917801126, 1128.4277797077089, 1132.5829241721835, 1131.7939910613297, 1120.853395170216, 1121.818826850201, 1123.0609906802608, 1113.8384578223695, 1128.2253676208534, 1119.3996953602805, 1128.5206280469847, 1110.6954123935122, 1128.8128935280558, 1119.4223501612005, 1121.512345697446, 1117.2359626728714, 1119.9042945006627, 1118.2354541730374, 1115.1926813181067, 1110.911230478541, 1109.5112735984653, 1114.4020709215054, 1123.0277811176245, 1112.003537101738, 1103.070279528532, 1115.4569859907244, 1105.5105531951433, 1100.891986866047, 1116.0417397978886, 1110.7527709251085, 1103.6790387184415, 1103.353249699218, 1103.4634220359653, 1093.0952137912086, 1103.9083215726123, 1103.2162237785037, 1096.2653777507064, 1102.390408690525, 1102.4728769165063, 1100.9741032984589, 1098.784941188888, 1096.6937358420957, 1096.0703105907392, 1097.398860181002, 1096.017968666776, 1093.3501228682858, 1093.032387283108, 1098.7600114790644, 1088.4067988635686, 1089.6563057357332, 1094.7464449561046, 1097.5022839411768, 1094.3583925873286, 1086.7141882370727, 1091.8834420420828, 1087.2359769678508, 1090.4766528227296, 1098.1791066685491, 1091.4229889803332, 1094.1364389208818, 1089.9327954830999, 1092.3095142585566, 1099.0948120141666, 1083.194840374035, 1089.744414506036, 1093.612929897029, 1079.3110882239134, 1083.9753032200126, 1075.820947745388, 1073.7565267840919, 1078.4505743497548, 1084.615698056157, 1083.6752902272472, 1078.0690563776366, 1073.2924506059505, 1081.7201973421913, 1083.0740752127406, 1081.8708236140528, 1081.7261542118883, 1070.9375583058923, 1078.029788558651, 1073.4319164887302, 1076.984415265824, 1077.50596590197, 1069.3524965447273, 1070.7034999739558, 1069.5223344856784, 1064.324916633834, 1067.947528491069, 1069.9650124503396, 1073.4823883895, 1063.647115569328, 1064.8588294531603, 1071.9989470915395, 1082.9008086797157, 1070.841535918192, 1069.5790749062799, 1064.5716809260718, 1070.432841452229, 1062.4970277702496, 1062.6370431040627, 1071.0937782717488, 1057.870310120767, 1064.2551050455795, 1064.6795795288454, 1057.839932065558, 1058.2580354918882, 1055.8518743459558, 1055.5972368490106, 1060.4704643817088, 1066.2836014139693, 1058.3395168992113, 1054.6480354414641, 1058.4865597208013, 1062.6903441624004, 1054.3191550227857, 1056.000089882131, 1052.005973208565, 1056.6926377600316, 1054.0964224268255, 1062.0602778405496, 1058.2806699763134, 1057.1874389770826, 1065.3733245256026, 1049.4479200609085, 1055.943122773508, 1057.7272726716417, 1052.1343039999701, 1050.7508383858578, 1050.9251949355435, 1046.2718561178988, 1057.6944881643444, 1048.2628501095035, 1044.5319203464162, 1053.318634577832, 1048.0542829936937, 1051.807669055756, 1044.637664005516, 1048.4900441832358, 1046.3893084177064, 1055.8172465206412, 1050.5680445427643, 1049.3116659466957, 1046.5897523624901, 1042.8863226028298, 1044.8595102270788, 1051.6620620595972, 1045.9731867308126, 1048.81144100883, 1041.263350849905, 1048.1477235098366, 1042.1820105443098, 1047.9141693941358, 1041.3768623371939, 1041.5801395395606, 1040.9151382828754, 1041.345634440561, 1042.244313364161, 1042.6333771516881, 1043.7183922583483, 1042.3182378921524, 1037.6078066155599, 1042.4187759512624, 1032.446487062652, 1036.3250286927653, 1036.1736154382254, 1040.895867451174, 1042.0888291001918, 1040.9519515388447, 1044.550470933901, 1043.2810416090624, 1042.5852215760979, 1032.903266388117, 1031.0989269855531, 1033.5056351592266, 1041.1206212568695, 1035.925730948317, 1035.386072979199, 1028.3908633638866, 1038.031378899592, 1030.1669954675124, 1027.334813312464, 1037.3870082233339, 1033.9587517756127, 1029.2945746342834, 1033.0502678546252, 1033.0472786618254, 1034.3915441622828, 1034.2260788450362, 1029.0225142988959, 1031.40246030059, 1026.9843496655758, 1028.493474060389, 1028.590501037561, 1030.1584050783013, 1033.3309862845547, 1021.585984717492, 1027.4262992594222, 1026.7961893794961, 1030.4716875937986, 1026.0091361073949, 1024.583516214423, 1020.5816058152564, 1036.5329102982207, 1019.5230406674541, 1023.3330788590374, 1021.5410192844168, 1029.2918869823259, 1018.7884564687523, 1030.018475404215, 1020.7625589227304, 1026.8461001533676, 1020.065343697324, 1023.3428458792665, 1022.9485304614262, 1020.2394530221936, 1019.5275829950841, 1027.47553805911, 1021.3572713615948, 1020.6049236804755, 1014.5388451379838, 1017.9435131072233, 1022.641982484538, 1014.3133698397082, 1018.6874778231997, 1021.9109692524784, 1024.6985667373845, 1019.6574200587543, 1019.6289717400985, 1012.1403649855836, 1020.0667069953167, 1014.3391461843762, 1018.4536347114803, 1023.1246028002886, 1016.4747473558205, 1022.3642183666791, 1015.283851049839, 1022.4380110455152, 1023.3115678522948, 1015.858932198325, 1019.8723785357555, 1016.1193328484137, 1011.5018082169892, 1019.104739975303, 1019.3373666674956, 1011.6155671004377, 1010.9017299103646, 1017.6376102891933, 1016.4929452440604, 1019.9933587704007, 1009.3602988864799, 1009.1314371222601, 1014.3642276070126, 1009.0381959770207, 1012.6366621453083, 1005.0042914556362, 1013.5378713047478, 1007.1526160767017, 1013.6680431622218, 1015.9287280963708, 1012.9957134888601, 1010.0890705237915, 1009.7798244062492, 1008.4380971458409, 1004.3723654279447, 1008.3247560478533, 1005.441678393119, 1009.256259512036, 1006.9093069960413, 1010.5676053148342, 1009.6306254782943, 1003.879633372452, 1009.1224765200067, 1001.4759787801801, 1007.1551647075364, 1006.97875139425, 1005.0785116742985, 1006.1635572189842, 1003.3820482571091, 1008.096937829372, 1007.9535831859123, 1011.9640996277823, 1005.9605930007864, 1000.6247459459047, 1006.6637650591733, 1008.0824877591895, 996.5696938994511, 1002.5975352689453, 997.6986333779349, 1004.79652523808, 1005.6450088344931, 1001.316936534991, 1015.1630406170301, 1007.7486522575884, 1005.8260748698569, 1005.8377284602075, 997.9935047431917, 1001.6610074988916, 1007.3584493266477, 1007.4945406763641, 1001.4630126708348, 1001.8496611086285, 1005.1619855422928, 999.3702106923313, 1006.5188154852029, 1004.9253679929144, 1000.8129538778363, 996.975092515932, 1004.9996656835115, 999.9843348076091, 1001.7508625453523, 1003.139810969458, 1002.3858721396333, 1003.3143226588348, 998.793166994929, 999.6732331454359, 999.0354827825785, 998.7567767504105, 994.8709967203793, 999.9053239839599, 997.2519179382997, 997.5829656006986, 993.3457962014714, 999.0419191949851, 996.3562820094942, 993.3375200815088, 999.9419207468716, 995.3234252317745, 990.1067441730335, 1000.050337126529, 993.2044907212855, 999.6537304119618, 997.3729480095514, 991.6402327447848, 993.5037299126166, 989.3808732049031, 995.5029991062701, 989.9022805118503, 996.6026981186241, 994.2137105057133, 993.0522783431257, 994.4152235631979, 990.8518928755781, 992.6415450312414, 997.3099608665146, 987.0475168298906, 996.8779636625538, 987.0729056190627, 993.891078902314, 994.177089852562, 988.0557524514911, 995.3316147373797, 989.792243035016, 993.6475344318079, 995.6074136749689, 994.1384252376491, 988.153458330037, 988.684396753336, 989.394455613893, 985.4886655866777, 982.6061853592779, 990.2623807005637, 988.807097183145, 990.2415567952262, 986.3555813841574, 997.135306765853, 990.9408715345445, 985.4559703740467, 989.183769952756, 994.577573835527, 979.6276284159126, 992.745770081313]}, {\"marker\": {\"color\": \"red\", \"opacity\": 0.5, \"size\": 5}, \"mode\": \"lines\", \"name\": \"Validation Loss\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [3084.509010585726, 2715.4950776315295, 2335.5779227162498, 1942.2547453212337, 1560.2305680315696, 1533.344025447609, 1391.9270684100927, 1391.9061119947019, 1283.8164903064373, 1257.5554474348148, 1343.492591528991, 1249.1565963839969, 1273.931771115643, 1238.5280704930476, 1249.1369806369607, 1222.9302471582364, 1250.940627198098, 1259.9327399921435, 1222.0851111816505, 1220.0682521381764, 1211.8369431175354, 1205.204886758306, 1206.0696130251154, 1198.6068092225332, 1213.547128074987, 1226.6462083154483, 1200.5660728391865, 1206.3319354442644, 1193.6816748693086, 1191.05509409915, 1185.6145979302046, 1185.1049158720878, 1195.6122691703508, 1178.8923863177072, 1190.6616004835228, 1188.8721666077897, 1185.9436894323678, 1180.6641418077627, 1176.8585817999847, 1170.0766341119725, 1182.5095795454902, 1187.198525663796, 1193.485152069208, 1182.7332619498195, 1169.1954041934812, 1201.307268331255, 1163.29106499653, 1171.4303922865465, 1167.9388673735305, 1167.4191385884549, 1172.5245584320014, 1163.2916225007284, 1169.0115744539319, 1167.075092677485, 1169.4974389712079, 1160.095866342334, 1154.335890943406, 1177.3461461714521, 1163.2173219951953, 1171.3176595329496, 1149.066680865367, 1165.8868981226187, 1152.0677210209435, 1151.3972082786338, 1161.8523864929678, 1162.7438438328325, 1162.2467412342403, 1148.9362301211665, 1156.4446289356233, 1167.4867800813365, 1145.2876482271877, 1144.1296021021844, 1136.3747351757147, 1158.4674257561849, 1147.3641992618307, 1141.2627891110637, 1140.881246000343, 1144.491007723597, 1139.0002606630756, 1161.8137982729706, 1168.6337769215586, 1164.2797444498274, 1149.706319291486, 1142.1767012690025, 1146.4462056180053, 1142.164296874021, 1152.8610615138423, 1150.0568497625075, 1137.7877770288687, 1131.8489132780385, 1130.4961731801511, 1139.8263482613388, 1140.8904682937357, 1127.8227976234302, 1127.213094255024, 1131.6640804911128, 1140.8941201469524, 1132.3219289096965, 1141.0794216084867, 1127.842263787214, 1120.7621218243605, 1136.3702708340513, 1128.6568766537137, 1131.4713001362136, 1133.4344721813825, 1122.5901398078363, 1120.274556473785, 1126.3801115076935, 1127.865220128976, 1125.2781789537564, 1128.2893589494795, 1127.7512316691366, 1118.395898256365, 1124.1052776770384, 1117.430914004341, 1122.8788331106189, 1126.812787147952, 1119.8262186518934, 1113.3938662886787, 1122.3735050731511, 1115.8287717009537, 1132.729375453327, 1119.4132002239025, 1120.5983823321494, 1117.8889842349874, 1117.26573600639, 1126.2949605987285, 1123.6535698137423, 1116.763428713483, 1115.2601944499056, 1117.329515602872, 1128.9984995656675, 1115.6549778936956, 1126.229164210737, 1121.2937095187722, 1115.7381127944375, 1106.4674388762344, 1118.55574094697, 1109.4610059641395, 1109.484224804805, 1119.927251968399, 1108.162983620695, 1113.811538589199, 1112.4910066221003, 1110.2630645586728, 1119.2769027841528, 1112.80539896896, 1110.8753983012082, 1113.6598005644755, 1115.3612704310503, 1116.292395726937, 1111.654417819338, 1110.115182359878, 1107.5438399849374, 1115.9569946558317, 1107.720194625166, 1108.5711581802138, 1112.1013578908678, 1103.0247622088916, 1102.6687175229815, 1105.7975677435159, 1105.7134755687052, 1113.5483993000944, 1106.5694716908492, 1105.1958261891073, 1103.888299985235, 1105.5377683392837, 1107.6491687714229, 1098.861665060603, 1103.1242935979249, 1104.7098363704617, 1105.4615470682759, 1109.4101152253672, 1104.3016077152158, 1101.491683895707, 1102.2332384267454, 1099.3022535349721, 1100.9726837384621, 1096.9292813414488, 1107.5920679655776, 1097.567139430684, 1099.4797086927965, 1104.1624014429706, 1112.2470333511662, 1101.093185739955, 1091.894198132919, 1103.031963451654, 1094.0532236843135, 1097.5223628308602, 1102.4404994979134, 1094.7854237668328, 1097.318209424582, 1098.1009566278574, 1094.631847123811, 1104.6267381862765, 1104.704122197205, 1093.1215624490865, 1097.7741915112103, 1092.4145905819976, 1095.8059469513312, 1092.0615952306073, 1094.9603147382413, 1094.8989924340394, 1098.5851433208766, 1092.451617736388, 1093.5775856337807, 1092.9432019588821, 1091.3804880237446, 1088.0713742693895, 1090.1024615416097, 1087.995716964694, 1087.4072754199983, 1094.560474886071, 1090.1291115934632, 1092.1251352637953, 1095.7357040169675, 1098.7898559203147, 1095.7518046921998, 1095.5318606697726, 1093.8511852496179, 1093.8005112119636, 1089.3400277978603, 1088.6692040663147, 1094.1112826943804, 1086.8247748344916, 1085.2787359194454, 1087.9098622972463, 1086.7035935766978, 1086.0743785404552, 1092.078207587775, 1092.454446110669, 1091.6721458213228, 1089.6632662157938, 1086.8883016741966, 1090.5558375849473, 1086.5544343026243, 1096.605608248438, 1086.5940258882079, 1087.9998772198342, 1084.9909301292737, 1095.1411498019852, 1091.8622597709123, 1090.0910535360688, 1091.8977003784057, 1089.2881120552108, 1084.9588757184695, 1092.6249580207368, 1084.2971097411864, 1085.2392393073553, 1084.122387519073, 1085.6389316568782, 1083.1842352616804, 1086.778229696038, 1083.0276907459395, 1088.3630967125855, 1083.9044343417886, 1081.1313220036157, 1080.7187980252563, 1080.2196308057198, 1080.2485874406268, 1084.138625244973, 1082.4466789678554, 1077.142098876023, 1081.4599129856767, 1094.7565273715568, 1087.8108361035568, 1080.259301356378, 1079.9757152287352, 1080.7570982161042, 1082.652444402321, 1080.3788569519031, 1085.4915484850455, 1082.8609687922974, 1094.0773516269253, 1084.2106409036542, 1086.9138231814827, 1080.2742934608498, 1081.0294408039026, 1083.46684923281, 1082.9427370783176, 1077.0470726819417, 1084.0303134340693, 1076.2634875334072, 1081.8667485413246, 1078.7212806030054, 1080.1345416540034, 1086.744241693442, 1082.0119364791817, 1079.5431463357463, 1084.0113723904808, 1080.4677631323864, 1078.5865848618519, 1082.2470938110964, 1082.6228975854801, 1079.7532726690958, 1085.7582941477922, 1085.8655798533218, 1082.5277754877525, 1077.457909558994, 1082.5375756507935, 1083.842667742772, 1083.9141365940116, 1078.124023510933, 1076.5721067157422, 1083.2875559217637, 1086.9178081273028, 1078.2712637819268, 1078.4657629612382, 1079.0587034101163, 1075.512499050265, 1077.746557431157, 1077.7118988863233, 1077.9042783698553, 1074.169972911014, 1074.774638082452, 1076.8074160786796, 1076.7174311168021, 1077.7152884364582, 1078.7646489025765, 1079.802576469328, 1082.3119245046887, 1077.6930064309784, 1074.0928865000171, 1076.9522480862352, 1074.3885433915289, 1079.473522540441, 1078.669732662344, 1075.0211609755677, 1077.8783987047966, 1074.020194253105, 1074.126448761898, 1074.2766724734308, 1071.8429950096813, 1076.6929416384944, 1076.851301690058, 1079.6198884188734, 1073.905384615234, 1076.8057499792428, 1072.6686830094181, 1070.0934981978535, 1076.2450101709758, 1072.9625720721533, 1074.8588861557628, 1072.739444136787, 1073.2757568848929, 1074.6922107097594, 1073.0519198059485, 1070.959314848489, 1069.3688493892714, 1071.4517275188928, 1071.0749032200517, 1072.486165005577, 1069.7023064900382, 1071.1457501172972, 1083.7209636656871, 1075.4409145418713, 1071.0175275785402, 1073.3614816246852, 1073.9462431912816, 1074.8546648532279, 1069.4847772421185, 1072.4503547357897, 1067.1728627977664, 1073.9120890585052, 1067.9071568744753, 1070.3757421395237, 1072.546436335057, 1076.8679518408017, 1067.7225085025755, 1067.8404699386374, 1068.3603021175368, 1074.0851817019172, 1067.7139138664825, 1070.8942858365344, 1068.6103663470335, 1075.2213537668642, 1075.1875785489538, 1076.211011055502, 1068.0222468799739, 1069.487178333589, 1077.376964458177, 1079.1744277063724, 1070.697255099587, 1073.402465281803, 1074.571537511201, 1069.3689646303044, 1067.2560049194312, 1067.0213471529885, 1070.3474490951864, 1074.3334460368442, 1066.507463398978, 1068.6496290257012, 1068.5953747076383, 1070.9573204785568, 1072.0679320432344, 1071.243588799629, 1066.9038833241818, 1068.7754133892652, 1065.6843000296865, 1072.0765263366393, 1066.8763127637717, 1070.8446131817725, 1069.7484073826324, 1070.0673462672871, 1074.3523726630647, 1073.0748865996904, 1065.305082912839, 1074.8147616909432, 1067.1705360206067, 1065.5014101605582, 1074.502627314416, 1070.2727441445413, 1073.8129200618876, 1070.2098411191364, 1070.3319425917541, 1069.7253573157398, 1070.333909815912, 1070.1629283500383, 1065.5720171518212, 1068.6740335663742, 1071.2513521973538, 1069.5533601475163, 1073.2491174563631, 1071.869742238022, 1074.960876109917, 1068.4034191534708, 1071.2127814544378, 1068.3302604838987, 1070.4100325886368, 1068.2390213333965, 1067.661727792064, 1067.1440929767004, 1066.145971150969, 1068.020106182324, 1066.100043217835, 1065.3441354259355, 1064.8924600935088, 1068.8036382142398, 1071.837903304451, 1070.6969788931692, 1066.2075291265103, 1069.4446947561707, 1070.1462984178786, 1068.141347973481, 1071.9365645600435, 1070.9991518230602, 1065.0476904747266, 1064.4623077618996, 1064.2289883384299, 1066.0992669319137, 1069.51599779544, 1063.754995801584, 1070.026415824699, 1065.4643872652882, 1070.8696260668362, 1067.8883344253654, 1066.0930237709842, 1066.5170916306224, 1062.2211774050031, 1064.1564273654471, 1064.618944705453, 1070.6383799538958, 1067.5518767007109, 1064.5146736005802, 1071.389802279873, 1073.3068775202244, 1065.3143601342006, 1064.152400660663, 1063.2354141275127, 1065.5368413196577, 1067.9942082568975, 1065.2984988753963, 1067.9860199996554, 1068.0816502300513, 1068.0404436300005, 1066.290710828623, 1067.8689947134033, 1067.6095343743343, 1066.7864614501611, 1067.5990863354095, 1067.233650680245, 1066.1620021381764, 1066.3821835943374, 1063.2312233486753, 1065.6380008921633, 1065.8086029046615, 1067.3700896256048, 1067.7404652976645, 1066.3789118309166, 1065.8779298343661]}, {\"marker\": {\"color\": \"grey\", \"size\": 5}, \"mode\": \"lines\", \"name\": \"Lowest error from previous models\", \"type\": \"scatter\", \"x\": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499], \"y\": [1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312, 1074.8323216045312]}],\n",
       "                        {\"legend\": {\"font\": {\"color\": \"black\", \"family\": \"sans-serif\", \"size\": 12}, \"traceorder\": \"normal\", \"x\": 0.57, \"y\": 1}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Learning curve\"}, \"xaxis\": {\"range\": [10, 499], \"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [900, 1500], \"title\": {\"text\": \"Loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('affd4ca9-3eb4-4c18-bdc1-a0e9de85af64');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DNN_plot_loss(history,starting_epoch=10, previous_val_loss=previous_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "list_of_files = glob.glob('Weights/*') # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file =  latest_file # choose the best checkpoint \n",
    "model.load_weights(weights_file) # load it\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.33% : Share of forecasts within 5% absolute error\n",
      "\n",
      "1062   : Mean absolute error \n",
      "\n",
      "9.12% : Mean absolute percentage error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_val_loss.append(np.asarray(history.history[\"val_loss\"]).min())\n",
    "performance_summary(model, X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1200.5697104463793,\n",
       " 1152.6631266439226,\n",
       " 1092.2156825275013,\n",
       " 1099.1921439590592,\n",
       " 1097.4319312732637,\n",
       " 1074.8323216045312,\n",
       " 1062.2211774050031]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\"basic_model\",\"dropout\",\"batch_norm\",\"leakyRELU\",\"1024_layer\",\"lr_decay\",\"callbacks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison=pd.DataFrame(previous_val_loss,columns=[\"val_loss\"])\n",
    "df_comparison[\"model\"]=models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverlabel": {
          "namelength": 0
         },
         "hovertemplate": "model=%{x}<br>val_loss=%{y}",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "basic_model",
          "dropout",
          "batch_norm",
          "leakyRELU",
          "1024_layer",
          "lr_decay",
          "callbacks"
         ],
         "xaxis": "x",
         "y": [
          1200.5697104463793,
          1152.6631266439226,
          1092.2156825275013,
          1099.1921439590592,
          1097.4319312732637,
          1074.8323216045312,
          1062.2211774050031
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "model"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "val_loss"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"47a46e71-3afd-4815-baf4-4f01b6b1e91b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"47a46e71-3afd-4815-baf4-4f01b6b1e91b\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '47a46e71-3afd-4815-baf4-4f01b6b1e91b',\n",
       "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"model=%{x}<br>val_loss=%{y}\", \"legendgroup\": \"\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [\"basic_model\", \"dropout\", \"batch_norm\", \"leakyRELU\", \"1024_layer\", \"lr_decay\", \"callbacks\"], \"xaxis\": \"x\", \"y\": [1200.5697104463793, 1152.6631266439226, 1092.2156825275013, 1099.1921439590592, 1097.4319312732637, 1074.8323216045312, 1062.2211774050031], \"yaxis\": \"y\"}],\n",
       "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"model\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"val_loss\"}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('47a46e71-3afd-4815-baf4-4f01b6b1e91b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_comparison, x=\"model\", y=\"val_loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "528px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
